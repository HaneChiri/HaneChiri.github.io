<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[MFC习题|RGB颜色模型演示程序]]></title>
    <url>%2Fpost%2FMFC_RGB_demonstration%2F</url>
    <content type="text"><![CDATA[习题来源：《计算机图形学基础教程》孔令德（第二版） 用mfc基于对话框的编程，实现下图的RGB颜色模型演示程序。点击颜色按钮能将“颜色及代码”这个组框中的静态文本框变成对应的颜色，调色板按钮可以调出自带的颜色选择对话框。滚动条和旁边的编辑框都可以调整颜色。 设计对话框过程不详述，直接开始代码和思路介绍。参考链接见文末。 改变演示块颜色我在这里将用于演示颜色的静态文本框称为演示块，对应的ID为IDC_COLOR_BOX。 查找了很久关于“如何修改控件颜色”的资料。 改变控件颜色需要在对话框类的OnCltColor()成员函数里面写对应代码。要生成这个方法，需要添加WM_CTLCOLOR这个消息的响应函数，在Class View的对话框类上右键可以找到Add Windows Message Handler，在这里添加就可以了。 生成的代码如下： 123456789HBRUSH CComputerGraphcisExercise2Dlg::OnCtlColor(CDC* pDC, CWnd* pWnd, UINT nCtlColor) &#123; HBRUSH hbr = CDialog::OnCtlColor(pDC, pWnd, nCtlColor); // TODO: Change any attributes of the DC here // TODO: Return a different brush if the default is not desired return hbr;&#125; 这个函数会在每个控件被重绘时调用，所以将改变控件颜色的代码放在这里就行了。 参数pWnd可以用来识别现在是哪个控件正在被重绘。 它的返回值是用于填充控件的画刷。 可以先判断是哪个控件正在被重绘，当演示块被重绘时，将它的颜色调整为自己设置的颜色。 12345678910111213HBRUSH CComputerGraphcisExercise2Dlg::OnCtlColor(CDC* pDC, CWnd* pWnd, UINT nCtlColor) &#123; HBRUSH hbr = CDialog::OnCtlColor(pDC, pWnd, nCtlColor); // TODO: Change any attributes of the DC here if(pWnd-&gt;GetDlgCtrlID() == IDC_COLOR_BOX)//判断控件 &#123; hbr=CreateSolidBrush(m_color);//调整颜色 &#125; // TODO: Return a different brush if the default is not desired return hbr;&#125; 上面代码的m_color是一个对话框类的protected变量，我把它和自动生成的m_hIcon放在了一起。 注意：如果这个变量被设置为public，就会在运行时产生错误，原因未知。 此变量在对话框类的初始化函数OnInitDialog()内初始化。 有了以上代码之后，想要改变演示块的颜色，只需要改变m_color的值并刷新对话框（例如使用Invalidate()）就可以了。 显示颜色代码在演示块下面有一个静态文本框用于显示当前颜色的十六进制代码，例如“#ffffff”。 由于颜色每次都是在对话框刷新的时候被改变的，可以将这个功能写在OnPaint()内。获取方式也不难，看代码基本能看懂，不赘述。 12345678910void CComputerGraphcisExercise2Dlg::OnPaint() &#123; //其他代码 //获取颜色代码 CStatic *color_code = (CStatic*)GetDlgItem(IDC_COLOR_CODE); CString color; color.Format("#%02x%02x%02x",GetRValue(m_color),GetGValue(m_color),GetBValue(m_color)); color_code -&gt;SetWindowText(color);&#125; 实现颜色按钮双击每个颜色按钮，添加它们的响应事件，例如： 123456void CComputerGraphcisExercise2Dlg::OnButtonRed() &#123; // TODO: Add your control notification handler code here m_color = RGB(255,0,0); Invalidate();&#125; 调整它们的颜色，并进行刷新重绘。 至于调色板按钮，需要使用mfc内置的颜色对话框CColorDlg: 123456789101112void CComputerGraphcisExercise2Dlg::OnButtonPalette() &#123; // TODO: Add your control notification handler code here CColorDialog palette; int nResponse = palette.DoModal(); if(nResponse == IDOK) &#123; m_color = palette.GetColor();//获取调色板的颜色 &#125; Invalidate(); &#125; 实现滚动条初始化滚动条首先需要在对话框的OnInitDialog()方法内，新增初始化滚动条范围值的代码。 123456789101112131415161718BOOL CComputerGraphcisExercise2Dlg::OnInitDialog()&#123; //其它代码 //... CScrollBar *scroll=(CScrollBar*)GetDlgItem(IDC_SCROLLBAR_R); scroll-&gt;SetScrollRange(0,255); scroll = (CScrollBar*)GetDlgItem(IDC_SCROLLBAR_G); scroll-&gt;SetScrollRange(0,255); scroll = (CScrollBar*)GetDlgItem(IDC_SCROLLBAR_B); scroll-&gt;SetScrollRange(0,255); return TRUE; // return TRUE unless you set the focus to a control&#125; 这段代码初始化了三个滚动条控件，首先用GetDlgItem()来获取ID对应的控件对象的指针，然后调用SetScrollRange()来设定其范围为0~255。 响应滚动条事件滚动条的响应事件不像按钮一样是每个按钮分开的，而是分为水平滚动条事件响应函数，和垂直滚动条响应函数。 在Class View里对对话框类右键，在右键菜单中找到Add Windows Message Handler，添加WM_HSCROLL消息的响应函数（如果是垂直滚动条，应该是WM_VSCROLL消息）。 生成的响应函数是这样的： 12345void CComputerGraphcisExercise2Dlg::OnHScroll(UINT nSBCode, UINT nPos, CScrollBar* pScrollBar) &#123; // TODO: Add your message handler code here and/or call default CDialog::OnHScroll(nSBCode, nPos, pScrollBar);&#125; 没看文档，不过参数大概意思可能是： nSBCode：滚动条响应的消息类型 nPos：滚动条改变状态之后的值 pScrollBar：指向被改变状态的滚动条控件的指针 滚动条拖动的代码需要自己写，在实现功能之前，你即使用鼠标拖动滑块，滑块也会回到原来的位置。 在这个响应函数里面，我只让滚动条改变对应的编辑框对应的数值。 1234567891011121314151617181920212223242526272829303132void CComputerGraphcisExercise2Dlg::OnHScroll(UINT nSBCode, UINT nPos, CScrollBar* pScrollBar) &#123; // TODO: Add your message handler code here and/or call default CDialog::OnHScroll(nSBCode, nPos, pScrollBar); int pos = pScrollBar-&gt;GetScrollPos();//获取当前位置 switch(nSBCode) &#123; case SB_THUMBPOSITION://被拖动 pos = nPos; break; //其实这里还可以写别的事件响应，丰富功能 &#125; pScrollBar-&gt;SetScrollPos(pos); //设置与滚动条对应的编辑框的数值 switch(pScrollBar-&gt;GetDlgCtrlID()) &#123; case IDC_SCROLLBAR_R: SetDlgItemInt(IDC_EDIT_R,pos); break; case IDC_SCROLLBAR_G: SetDlgItemInt(IDC_EDIT_G,pos); break; case IDC_SCROLLBAR_B: SetDlgItemInt(IDC_EDIT_B,pos); break; &#125; &#125; 响应编辑框变化事件现在已经可以滑动滚动条来修改编辑框内的值了，但演示块的颜色还不会改变，我把这个功能写在编辑框里面了，这样，可以顺便实现“在编辑框内修改值来修改颜色”的功能。 这是其中一个编辑框的响应函数代码，其他两个类似，要说的内容都写在注释里面了。 12345678910111213141516171819202122232425void CComputerGraphcisExercise2Dlg::OnChangeEditR() &#123; // TODO: If this is a RICHEDIT control, the control will not // send this notification unless you override the CDialog::OnInitDialog() // function and call CRichEditCtrl().SetEventMask() // with the ENM_CHANGE flag ORed into the mask. // TODO: Add your control notification handler code here UpdateData();//更新数据，将数据从控件上同步到绑定的变量 int pos = atoi(m_R_value.GetBuffer(0)); ((CScrollBar*)GetDlgItem(IDC_SCROLLBAR_R))-&gt;SetScrollPos(pos); //根据滚动条位置设置当前颜色值 int R=0,G=0,B=0; R=((CScrollBar*)GetDlgItem(IDC_SCROLLBAR_R))-&gt;GetScrollPos(); G=((CScrollBar*)GetDlgItem(IDC_SCROLLBAR_G))-&gt;GetScrollPos(); B=((CScrollBar*)GetDlgItem(IDC_SCROLLBAR_B))-&gt;GetScrollPos(); m_color = RGB(R,G,B); //为了防止整个对话框闪烁，只刷新演示块 CRect rect; ((CStatic*)GetDlgItem(IDC_COLOR_BOX))-&gt;GetWindowRect(&amp;rect); ScreenToClient(&amp;rect);//转换为对话框上的客户坐标 InvalidateRect(rect);//只刷新控件位置&#125; 完成这一步之后，已经能够实现使用滚动条或者编辑框来改变颜色了，但是当你在点击颜色按钮时，虽然颜色改变了，但是滚动条的位置和编辑框的值不会随之改变。 因此还需要一步： 滚动条随颜色而变化位置这个对话框内只有颜色按钮能够改变颜色，所以简单地在所有颜色按钮的代码内添加改变位置的代码即可。 而改变滚动条的位置只需要改变对应的编辑框的数值就可以了。 于是颜色按钮代码变成了这样 12345678910111213141516void CComputerGraphcisExercise2Dlg::OnButtonRed() &#123; // TODO: Add your control notification handler code here m_color = RGB(255,0,0); //调整滚动条位置 int R=0,G=0,B=0; R=GetRValue(m_color); G=GetGValue(m_color); B=GetBValue(m_color); SetDlgItemInt(IDC_EDIT_R,R); SetDlgItemInt(IDC_EDIT_G,G); SetDlgItemInt(IDC_EDIT_B,B); Invalidate();&#125; 部分用到的MFC函数或宏的简介详细的自己百度 GetRValue()，GetGValue()，GetBValue()，分别用于获取颜色值的RGB三个通道的值 SetDlgItemInt()，可以将值送入ID对应的控件 GetDlgItem()，通过ID来获取指向控件的指针，记得转换指针类型 Invalidate()，使客户区无效化，引起重绘 参考链接 VS2010/MFC编程入门之二十六（常用控件：滚动条控件Scroll Bar）-鸡啄米]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>mfc</tag>
        <tag>framework</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MFC用对话框获取输入]]></title>
    <url>%2Fpost%2FMFC_get_input_by_Dialog%2F</url>
    <content type="text"><![CDATA[在MFC调用对话框读入数据，并在客户区输出。 这是《计算机图形学基础教程》的一个习题： 使用MFC设计一个长方形类CRectangle，调用对话框读入长方形的长度和宽度，在客户区输出长方形的周长和面积。 这个书上并没有教怎么用对话框读取输入，我在这之前也完全没接触过MFC的对话框。弄了两小时，终于把这道题做出来了。以此文记录一下 参考链接 MFC调用输入对话框并返回输入信息 MFC对话框和常用教程 设计对话框找了一下，MFC似乎没有像python那样的input()或者像是VB里面的inputBox()之类的函数，所以得自己先设计对话框。 首先打开Resource View，在Dialog处右键菜单插入新的对话框。 接着就是放控件以及给控件命名了。这个比较简单，就不详细说了。 我设计的对话框有两个Edit控件，一个是IDC_LENGTH，用于输入长方形的长，一个是IDC_WIDTH，用于输入长方形的宽。 新建对话框类在设计好的对话框上右键菜单打开类向导，也就是classWizard，会弹出一个对话框如下图： 大致意思是：检测到有个新建的对话框资源，你可能想要为它创建一个类，要创建吗？ 点确定创建一个对应的类。 如果没有弹出这个对话框，你也可以在类向导右上角的Add Class按钮来创建一个MFC里面的类，把基类调整成CDialog，Dialog ID设置成你刚刚设计的对话框ID就可以了。 （其实命名最好在后面加个Dlg后缀以表示这是对话框，但是我懒得改了） 添加关联变量在类向导里面选择第二个选项卡，也就是Member Bariables成员变量选项卡。 这里面列出了对话框上控件的ID，这些ID可以在设计对话框的时候指定。 选中用于输入数据的控件，然后点击Add Variable添加对话框类的成员变量。改变量名字，其他选项默认即可。 这个操作与你直接在类代码中添加的区别是，这个操作会建立起控件和这个成员变量的关联关系。这个关联关系体现在自定义对话框类的DoDataExchange()这个成员函数内： 12345678void CInputRectangle::DoDataExchange(CDataExchange* pDX)&#123; CDialog::DoDataExchange(pDX); //&#123;&#123;AFX_DATA_MAP(CInputRectangle) DDX_Text(pDX, IDC_LENGTH, m_edLength);//添加关联变量之前，这里是没有这两行的 DDX_Text(pDX, IDC_WIDTH, m_edWidth); //&#125;&#125;AFX_DATA_MAP&#125; 调用对话框如图，我打算使用菜单来调用对话框输入矩形长和宽。 添加菜单的过程不详细说。 直接跳到菜单的响应函数： 1234567891011void CComputerGraphicsExerciseView::OnHomework2_2() &#123; // TODO: Add your command handler code here CInputRectangleDlg inputDlg; int nResponse = inputDlg.DoModal(); if(nResponse==IDOK) &#123; //这里获取输入并在客户区输出 &#125;&#125; 在文件开头include对话框类的头文件，声明对象，并调用对话框对象的DoModal()方法。 这个方法在对话框关闭之后，才会返回一个值，对应关闭对话框的动作，这里我用nResponse这个int变量接收返回值。 接着判断返回值，如果是点击确定按钮关闭对话框，那么获取对话框的输入，并且在客户区输出。 获取输入绑定对话框上两个编辑框的变量分别为：m_edWidth和m_edLength。默认情况下，它们是CString类型的，因此需要进行类型转换。 12int width=atoi(inputDlg.m_edWidth.GetBuffer(0));int height=atoi(inputDlg.m_edLength.GetBuffer(0)); 对上面两行代码的说明： 两个关联变量是public的，因此可以直接访问。 CString的GetBuffer()成员函数返回对应的字符数组类型的字符串 atoi（ASCII to integer）把字符串转换成整型数 进行输出获取设备上下文，并调整坐标系： 1234567891011CDC *pDC=GetDC();//获取设备上下文CRect rect;GetClientRect(&amp;rect);pDC-&gt;SetMapMode(MM_ANISOTROPIC);pDC-&gt;SetWindowExt(rect.Width(),rect.Height());pDC-&gt;SetViewportExt(rect.Width(),-rect.Height());pDC-&gt;SetViewportOrg(rect.Width()/2,rect.Height()/2);rect.OffsetRect(-rect.Width()/2,-rect.Height()/2);pDC-&gt;Rectangle(rect);//清空屏幕 输出数据，并释放设备上下文： 12345678910CRectangle crect(width,height); CString perimeter_text,area_text;perimeter_text.Format("长方形的周长为：%.2f",crect.perimeter());area_text.Format("长方形的面积为：%.2f",crect.area());pDC-&gt;TextOut(0,0,perimeter_text);pDC-&gt;TextOut(0,20,area_text);ReleaseDC(pDC);//释放设备上下文 这样就完成了 菜单代码概览1234567891011121314151617181920212223242526272829303132333435363738void CComputerGraphicsExerciseView::OnHomework2_2() &#123; // TODO: Add your command handler code here CInputRectangleDlg inputDlg; int nResponse = inputDlg.DoModal(); if(nResponse==IDOK) &#123; CDC *pDC=GetDC(); CRect rect; GetClientRect(&amp;rect); pDC-&gt;SetMapMode(MM_ANISOTROPIC); pDC-&gt;SetWindowExt(rect.Width(),rect.Height()); pDC-&gt;SetViewportExt(rect.Width(),-rect.Height()); pDC-&gt;SetViewportOrg(rect.Width()/2,rect.Height()/2); rect.OffsetRect(-rect.Width()/2,-rect.Height()/2); pDC-&gt;Rectangle(rect);//清空屏幕 int width=atoi(inputDlg.m_edWidth.GetBuffer(0)); int height=atoi(inputDlg.m_edLength.GetBuffer(0)); CRectangle crect(width,height); CString perimeter_text,area_text; perimeter_text.Format("长方形的周长为：%.2f",crect.perimeter()); area_text.Format("长方形的面积为：%.2f",crect.area()); pDC-&gt;TextOut(0,0,perimeter_text); pDC-&gt;TextOut(0,20,area_text); ReleaseDC(pDC); &#125;&#125; 错误思路一开始我以为需要编写对话框的ok按钮的响应事件，写成了下面这样，试了一下不行，不知道为什么： 1234567891011121314151617181920212223242526272829303132333435void CInputRectangleDlg::OnOK() &#123; // TODO: Add extra validation here CDialog::OnOK(); UpdateData();//用于将数据从对话框同步到成员变量中 int width=atoi( m_edWidth.GetBuffer(0)); int height=atoi( m_edLength.GetBuffer(0)); CRectangle crect(width,height); CDC *pDC=GetDC(); CRect rect; GetClientRect(&amp;rect); pDC-&gt;SetMapMode(MM_ANISOTROPIC); pDC-&gt;SetWindowExt(rect.Width(),rect.Height()); pDC-&gt;SetViewportExt(rect.Width(),-rect.Height()); pDC-&gt;SetViewportOrg(rect.Width()/2,rect.Height()/2); rect.OffsetRect(-rect.Width()/2,-rect.Height()/2); pDC-&gt;Rectangle(rect);//清空屏幕 CString perimeter_text,area_text; perimeter_text.Format("长方形的周长为：%.2f",crect.perimeter()); area_text.Format("长方形的面积为：%.2f",crect.area()); pDC-&gt;TextOut(100,100,perimeter_text); pDC-&gt;TextOut(100,300,area_text); ReleaseDC(pDC);&#125;]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>mfc</tag>
        <tag>framework</tag>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫解析库BeautifulSoup速查]]></title>
    <url>%2Fpost%2Fpython_spider_parser_beautifulsoup%2F</url>
    <content type="text"><![CDATA[为了方便使用，将BeautifulSoup库常用的接口进行总结。 总结内容来源：《python3网络爬虫开发实战》崔庆才 导入与解析12from bs4 import Beatsoup = BeautifulSoup(response.text,'lxml') 节点选择器提取属性获取到的是第一个标签 soup.title.string:获取title标签的文本内容 soup.title.name:获取节点名称“title” soup.p.attrs:获取节点属性字典 soup.p.attrs[&#39;class&#39;]或者soup.p[&#39;class&#39;]:获取节点属性 关联选择子孙 soup.p.contents:获取直接子节点列表 soup.p.children：获取直接子节点生成器 soup.p.descendants：获取所有子孙节点生成器 祖先 soup.p.parent：获取直接父节点 soup.p.parents：获取所有祖先节点生成器 兄弟 soup.a.next_sibling：获取下一个兄弟节点 soup.a.previous_sibling：获取上一个兄弟节点 soup.a.next_siblings：获取后面所有兄弟节点列表 soup.a.previous_siblings：获取前面所有兄弟节点列表 方法选择器find_all()1find_all(self, name=None, attrs=&#123;&#125;, recursive=True, text=None,limit=None, **kwargs) 用法： soup.find_all(name=&#39;ul&#39;)：获取所有ul节点组成的列表 soup.find_all(attrs={&#39;id&#39;:&#39;list&#39;})：获取id为list的节点 常用参数如id和class可以直接传入，如：soup.find_all(id = &#39;list&#39;)或soup.find_all(class_=&#39;element&#39;) soup.find_all(text=re.compile(&#39;link&#39;))可以匹配文本，也可以用正则表达式对象 find()返回第一个匹配的元素，和find_all用法差不多 其他 find_parents(),find_parent() find_next_siblings(),find_next_sibling() find_previous_siblings(),find_previous_sibling() find_all_next(),find_next()返回节点后符合条件的节点 find_all_previous(),find_previous() CSS选择器soup.select(&#39;CSS选择器&#39;)：返回列表]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年暑假总结]]></title>
    <url>%2Fpost%2F2019_summer_holidays_summary%2F</url>
    <content type="text"><![CDATA[这个暑假大致是7月7日（到家时间）至8月24日，已经过了6周左右。 如果是以前的假期，肯定是不记得自己做过什么了，但是这次每周做了一次周总结，因此可以对照着周总结来进行假期总结。应该是头一次有参考地记录下自己的整个假期了。 做了什么事情科目二最主要做的事情是考驾照的科目二，每天都去练一个上午或者一个下午。 认识了几个同一个高中的同学，并且和陌生人打交道没有那么困难了。 bilibili代码分享群里面有个人问我要不要参与b站的暑假爆肝活动，在7月28日之前投稿4个视频。我想要奖励里面的一个月大会员，于是就参加了。 7月13日投了一个:“如何像项目一样整理和管理你的个人电脑文件”，分享了一下我自己不久前开始使用的整理电脑文件的方式，而且特地利用“网易见外”这个网站来试着加上字幕。比起分享代码思路，这种视频受众更加广一些。 通过自动上字幕，我发现自己讲解的时候语气词和停顿还蛮多的，需要多锻炼表达能力。 学习python，利用python写了一个脚本，用来自动帮我的明日方舟的关卡点“开始游戏”按钮，灵感来自一个用机械装置做“物理外挂”的视频。录制了一个演示视频：“明日方舟代理指挥“代理指挥”的代理指挥” 后来没什么想做的视频主题，就没有继续做了。 到了八月，买了一个板绘用的数位板，开始学习板绘作为平时的兴趣。八月录制了三个临摹过程的视频，感觉不错。 个人博客 &amp; githubpython爬虫使我开始更加积极地更新博客的事件是，我终于在7月15日解决了信息门户的登录密码加密问题。 将以前存放在有道云笔记里面的分析过程重新整理了一下写成博文：“学校信息门户模拟登录之密码加密” 再利用这个加密模块，很快写出了模拟登录信息门户的python自定义包：“学校信息门户模拟登录” 没想到这个自定义包竟然获得了封掣大佬的pull request（也就是他参与了这个项目，贡献了一部分代码）！这可是我第一次获得pull request！兴奋之情可想而知，我当时就想继续写点什么出来。 不过这个项目发展空间并没有太多，最多输出cookies文件给别的程序或者脚本用。在写好了文档，增加了成绩查询，学习了如何将其打包成exe文件，又学习了如何release一个版本之后，就没有什么可以扩展的地方了。 在这个过程中，除了上面说到的那些，我还开始使用github的issue来记录待办事项和bug，等到有空的时候去修复。 在github上面终于有了一个像样的项目了，这让我很高兴。 我又开始把之前写的信息门户爬虫给整理了一下，整理到了一个github仓库当中：“chd_spider” 在这个成绩的激励下，我开始尝试做之前领取的任务：爬取微信公众号，确实有成功的部分，不过目前卡关了 博客主题优化给博客增加了几个功能： RSS简易信息聚合 valine评论 hexo的Next主题挺方便的，将所有东西都准备好了，增加这些功能挺容易的。 RSS简易信息聚合在设计自我管理系统中的信息输入子系统时，了解到RSS这个概念。 RSS(Really Simple Syndication)是一种描述和同步网站内容的格式，是使用最广泛的XML应用。RSS搭建了信息迅速传播的一个技术平台，使得每个人都成为潜在的信息提供者。发布一个RSS文件后，这个RSS Feed中包含的信息就能直接被其他站点调用，而且由于这些数据都是标准的XML格式，所以也能在其他的终端和服务中使用，是一种描述和同步网站内容的格式。 就本质而言，RSS和Atom是一种信息聚合的技术，都是为了提供一种更为方便、高效的互联网信息的发布和共享，用更少的时间分享更多的信息。同时RSS和Atom又是实现信息聚合的两种不同规范 （来自百度百科） 以我的理解，RSS的Feed其实就是一个将网站的内容格式化的XML文件，也就是一个“地图”，按照一定的标准标注了特定内容，RSS阅读器其实就是一个爬虫软件。将RSS Feed给RSS阅读器之后，阅读器爬虫按照这份“地图”，解析出需要爬取的链接，然后获取文章内容，再展现给用户。 弄懂了原理之后，我帮我的博客也加了一个RSS插件，在部署博客的同时生成RSS Feed（我博客的RSS Feed），这样别人使用RSS阅读器就可以“订阅”我的博客，在我的博客更新的时候可以第一时间看到。 在电脑上我下了一个RSS阅读器irreader，订阅了几个朋友的博客。除此之外，这个阅读器还能订阅没有Feed的链接，我猜原理是根据你选择的几个链接来自动生成一个Feed来进行订阅，甚至能订阅B站up的视频以及贴吧的帖子，还是挺好用的。 valine评论和阅读量计数用了同一个leancloud应用，效果挺不错： 但遗憾的是，leancloud在经过上个月的域名封禁事件之后，又是实名注册又是绑定备案域名的，十月一号之后就得找别的办法来弄评论和点击数了。 半途而废的事情 暑假开始的运动目标没有完成，意志力随着身体素质的变差越来越弱 跟着廖雪峰python教程写代码的目标没有完成 利用time meter记录时间 每天在anki录入30个单词 晚上十一点睡]]></content>
      <categories>
        <category>自我管理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一封邀请函]]></title>
    <url>%2Fpost%2Finvitation%2F</url>
    <content type="text"><![CDATA[如果你是从外部链接来到这个页面，那么你也许是获得了进入一个自我管理群的邀请函。 你可以阅读下面的介绍，以确定要不要加入这个群。 建议在电脑上查看，有侧边栏目录方便跳转。 无论是否选择加入此群，都请不要将此链接随意传播。 群简介这是一个QQ群，于2019年4月26日创建。 是为了营造一个良好的学习氛围，提供一个外在监督环境建立的。 在建立之初，我并没有想好这是一个什么样的群，只是想聚集一些小伙伴一起学习。后来，这个群慢慢地发展起来，变成了一个有着打卡系统和反思系统的群。 打卡系统：本群的打卡系统利用了QQ群的群相册。如果你有想要打卡的项目，可以在登记了信息之后创建群相册用于打卡，由管理员以及全体群成员监督，超过一定天数未打卡，就会删除对应的相册，并扣除一定的积分。而达到一定天数可以将相册归档，获得与持续天数正相关的积分。 反思系统：作为本群成员，需要每周周日总结一下本周的收获，并以文字的形式发到群内。如果没有收获，也需要在群里说明（例如说：“本周无总结”）。没有声明本周没有总结且未总结的，扣除一定的积分，在下一周周日结算之前补回，则取消扣分。正常总结会获得一定量的积分。 积分系统：进群之后初始积分为0，若积分为负数且在下一周周日结算之前仍然为负数，则会强制离开群聊。 邀请系统：本群采用邀请制，每个月最多邀请一个愿意遵守群规则的人入群。如果没有这样的人选，这个月就不邀请，宁缺毋滥。 文件系统：为了方便学习交流，如果需要上传文件，请将文件上传到对应的文件夹，并使得看文件名就知道这个文件的作用，多版本文件请用6位数日期+修改次数的后缀 群活动：群内会不定期地进行一些活动，自愿参与。 简单来说，想要留在这个群里面，最简单的方式是只需要在每周日发一条“本周无总结”，维持积分不为负。 而在这个基础上，你可以选择群里面提供的规则来进行自我提升、自我管理，也可以向管理员提出自己的规则提案、活动提案。 本群提供的是一种氛围，一种监督环境，至于能否从中获得提升，还需要看你自己。 详细内容见下： 打卡系统由一个.xlsx文件（即电子表格文件）来实现。下图为示意图，点击可放大。 创建打卡相册创建打卡相册需要以下步骤： 在群文件内下载“打卡相册登记表xxxxxx-x.xlsx”文件 在其中根据工作表“打卡相册字段描述”内的说明，在工作表“打卡相册登记表”内填写好对应的信息 将文件名修改为“打卡相册登记表+6位数日期+任意分隔符+这一天第几次修改”，如：“打卡相册登记表190617_1“代表2019年6月17日的第一次修改。并将文件上传 在群相册创建自己的打卡相册，开始打卡 打卡相册规则下面只列出比较重要的几个规则，具体的积分计算规则见这里 相册状态：正在进行、放弃、失败、归档 如果连续三天未打卡，管理员就删除相册，并在登记表内将相册状态设置为“失败”。 不创建打卡不扣分，创建打卡而未坚持下来会扣分。没有请假制度，创建打卡前请考虑好 对于有期限的相册，比如打卡目标是“两周读完《xxx》”，那么在结束日期时，可以将其状态设置为“归档”。相册资源回收（删除或改作他用），避免资源闲置。若持续时间大于等于一百天，则可以选择保留相册。（可以给其他群员作榜样） 对于没有期限的相册，比如“每天背单词”，那么在创建时间满三十天后就可以选择“归档”（三十天应该够养成一个小习惯了），删除规则同上一条。 相册删除后，相册记录还会保留在登记表里面，是公开的哦。 反思系统其登记表与打卡相册登记表使用同一个工作簿。 作用 在群文件内复习自己学习内容 看到别人学习了自己却没有而产生激励效果。 保持群内一定的活跃度，去除不活跃成员 作为群内一个基本的群活动，强化学习氛围 规则 每周日，每个人在群聊天发一个周总结，内容是自己这周学习了什么，没有限制，只是给大家一个自我反省的机会。 如果没有可以写的东西，那么也在群里面报备，方式为在群聊天中说：“本周无总结”或者别的能表明这一事实的话。别有压力，只是回复一句话的功夫。 无论总结多少，只要总结了，都会获得一定量的积分。 如果周日那一天状态不好或者很忙，可以在群里@管理员，告知推迟时间（不可超过下周周日），只要在报备的时间之前补了总结，也可以获得总结积分。 如果没有报备也没有在截止之前发周总结，可以在下一周总结之前补。如果没有补，则会扣除一定的积分。 每周所有成员的周总结将会被管理员整理到一个文件中，发到群内，即周总结是公开的，方便你随时查看自己的周总结以及自己下周的目标。 尽量使用markdown语法，方便管理员整理。如果你不了解什么是markdown，那么就只需要在你的总结前面加上一行“## 你的昵称”即可。如果你想要学习markdown，可以参考我在b站发的这个视频。本篇文章就是使用markdown语法来书写的。 总结示例内容没有限制，想写什么都可以，不限字数，但是最起码的格式是，在总结前面加上“## 你的昵称” 以下内容仅供参考，可以根据自己的喜好来增加或删除模块。 简易总结示例123## 憧憬少本周没做什么事情 详细总结示例摘自“周总结week5”，使用了markdown语法，可以在群文件的“周报”文件夹中找到它，看一下markdown的渲染效果。 123456789101112131415161718192021222324252627282930## 憧憬少### 本周做了什么- 驾校练完了右侧倒车入库，开始学习左侧- （周一）完成信息门户密码加密模块并上传github和[编写博客](https://hanechiri.github.io/post/portal_login_encrypt/#more)- （周三）完成信息门户模拟登录模块并上传[github](https://github.com/HaneChiri/CHD_portal_login)和[编写博客](https://hanechiri.github.io/post/portal_login/#more)- 驾校排队练车的时候无聊，开始使用墨者写作APP来重新开始以前放弃的小说并在群里连载- （周六）写了一个python脚本用于自动启动明日方舟的代理指挥，学习了`pyautogui`库。- （周日）发布上述脚本的[介绍视频](https://www.bilibili.com/video/av60038926/)以及上传脚本和打包的exe到[github](https://github.com/HaneChiri/arknights_assist)### 本周的目标有没有达到【目标链编号，每完成一个目标，生成下一个目标，编号增加，未完成则归零】- [x] 【1】一周四次运动- [x] 【0】一周三次，每天写代码半小时### 下周的目标- [ ] 【2】一周四次运动，包括不限于跑步，散步等- [ ] 【1】一周三次，每天写代码半小时- [ ] 【0】每天利用time meter记录时间开销### 概括这一周分数：85%- 在驾校遇到了同一个高中的同学（虽然不是一个班不认识）- 做了挺多事情 群活动可以在群内向管理员提出群活动的建议。 目前已经举行过的群活动： #1学期总结【活动】学期总结【编号】#1（也就是第一次活动）【时间】2019-6-21~2019-7-10【内容】本学期已经告一段落，学科的内容是否考完试就忘得差不多了呢？为了避免这一学期白学，各位学研都市居民可以在活动时间内在群文件的群活动作品提交文件夹内提交自己的学期总结。形式不限，可以是手写总结拍照，可以是知识框架思维导图，可以是笔记文件，可以是录音讲解等。【存档】活动结束之后，会将群文件中提交的总结统一打包，保存到群活动文件夹中，群活动提交文件夹会被清空。【排名】活动结束之后，会进行作品投票，票数最多的参与者可以获得奖励【奖励】目前我能想到的奖励就只有30天自定义专属头衔了]]></content>
      <categories>
        <category>自我管理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[爬取微信公众号文章2获取页面失败]]></title>
    <url>%2Fpost%2Fwechat_offical_account_spider_2_fail_to_get_html%2F</url>
    <content type="text"><![CDATA[虽然获取到了微信公众号文章的链接，但没法获取到包含文章内容的html。 花了一个小时来研究怎么获取页面，最后还是失败了。 requests首先按照一般思路，使用requests库来获取页面，但是获取到的却是不含有文章内容的一堆js代码和css代码，以及少量的没有内容的html。 去查看Ajax请求，有4个请求，其中三个都是没有文章内容的json，而第一个请求也是最可疑的一个，无法预览。 第一个请求的接口： 1https://mp.weixin.qq.com/mp/appmsgreport?action=page_time&amp;__biz=MzAwNjA3Nzg0MA==&amp;uin=&amp;key=&amp;pass_ticket=&amp;wxtoken=777&amp;devicetype=&amp;clientversion=&amp;appmsg_token=&amp;x5=0&amp;f=json 这是以POST方式访问的接口，下面一大堆的Form data，这其中甚至还有文章的标题！ 也就是说在访问这个接口之前，就已经得知了文章的内容了吗？ selenium我觉得模拟请求太过于复杂，于是尝试使用selenium来获取。 但是得到的内容和上文说的一样，并没有什么不同。非常奇怪。加了60秒的延时让它充分渲染也没用，问题不在这里。 教训后来输出到文件才发现，内容并没有少，确确实实地获取到了文章内容，但是由于print出来的字符数有限制，无法在控制台显示完，才导致我以为获取失败。当个教训吧。]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>selenium</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年年中总结]]></title>
    <url>%2Fpost%2F2019_semi-annual_summary%2F</url>
    <content type="text"><![CDATA[要一个人在家一周，有些孤独，有些茫然，想起来写总结。 不太清楚总结怎么写，就和以前一样，把过去的东西列出来看看吧，帮助自己回顾一番。 这是2019上半年的总结，也是大二第二学期的学期总结。 过去做了什么事情翻了一下qq空间动态以及别的一些痕迹，大概了解了一下我自己在2019年上半年干了些什么事情。 输出平台上半年我主要在三个平台上输出一些东西： 个人博客 bilibili github 个人博客2019年2月6日，我在github上部署了我的个人博客，并记录下了过程（不算教程啦，更像是日记） 到今天（2019年8月10日）为止，已经有29篇博文了。标签中占的比例比较大的是“python”和“spider”，2月份之后除了5月没有python文之外，其他月份都有至少2篇的博文关于python，因为觉得python写起来很舒服，比起c++和java的严格语法，python更容易写出东西来。自从2月份的寒假开始学习python爬虫之后，就时不时地更新相关的博文。 有了个人博客之后，我就彻底抛弃了原本的微信公众号。 在个人博客里面写东西有一种更加快乐的感觉。一步一步了解Hexo博客搭建的原理，一步一步改进博客功能，添加右下角的live2D人物，安装图片插件，添加基于leancloud的评论和阅读次数统计……这些让我有种掌控感。 Hexo日记本搭建了Hexo个人博客之后，又新建了一个博客作为日记本，把以前写的电子版日记存在里面，不部署到服务器，只留存在我电脑上，主要是想利用它好看的渲染效果和各种插件来丰富日记本。 github下图是我目前的github资料页： 从contributions日历可以看到，2018年还只是零星的几个commits，2019年的commits就增加了好多。 主要有两个原因。 第一个原因，是我部署了个人博客之后，写博客以及频繁调试博客提供了commits数。不过这只是次要原因。 第二个原因，是我真正开始把git当成生产工具了，这才是主要原因。 我大概是在2018年4月15日创建了github账号，2018年下半年的时间并没有去管github账号，因为那时候还不知道怎么用github，觉得得把git命令行给用熟了才能去玩这个网站。但是这就形成了一个不算太高但是我不想跨的门槛，而且我觉得平时我也用不到这么高级的东西，于是就没去学了。 一次偶然的机会，我看到了社团里面一个大佬——封掣是如何使用github-desktop（github官方的GUI桌面软件）来管理自己的代码，终于明白，管它那么多高大上的命令干什么，一个工具，能够解决人的问题就行。于是我开始用起有GUI的git软件了（现在用的就是github-desktop），把它当成存档软件，写课设的时候，写完一个功能就存一个档，写错了就回档。 bilibili从创作中心的日历来看，今天是我成为up主的464天。2018年5月4日，我投稿了第一个视频。 到今天，已经陆陆续续投稿了26个视频。主要的主题是分享代码思路。 然而总是有人只要代码不要思路，这让我很是苦恼，详情可以看这个 现在前往代码分享群的通道基本已经关闭，只有少数几个视频下面的群号我没有删除，避免伸手党直接进来。一到期末，需要课程设计的时候，我的视频播放量就会增加，我算是明白怎么回事了。 想要解决“代码分享和代码抄袭的矛盾”，我有一个初步的想法，就是在视频里面先介绍思路，声明视频主要分享思路，不提供源代码，如果是伸手党，估计就直接走了找别的代码，同时也能够让真正想学的人学到思路。 不过还有一个问题，我做的视频的主题注定我每个视频的观众很少有重叠。我并不像教程up主一样，针对某个固定主题来出视频，而是更加注重于巩固自己学到的知识，分享只是顺带的。这也不是太大的问题。 组织团队网络安全协会虽然加了这个社团，但是感觉自己很难进入网络安全这个领域。 易班工作站大概2019年3月的时候，偶然看到易班工作站的群里面发了一个通知，说是辅导员有个技术讲座。当时我只是一个普普通通的社团成员，想去就去，那时候正好没什么事情，就去了。没想到会对我影响这么大。 我当时只是过去凑个热闹，没想到这是工作站的技术组的又一次纳新。 以前的技术组纳新我也有去，那时候是大一，啥也不懂，连html都不会，然后就让我们自学来制作一个留言板。 自学诶！ 刚开始自学html的时候，感觉这个难度我能hold住，没问题，可以进去！ 但是学到后面我连需要用什么东西，需要学什么都不知道，很长一段时间没有进展，想放弃了。可能是因为很多同学也是如此，辅导员无奈之下，给大家简单演示了一下怎么用easyPHP来写，然后又放着我们去自学。 其实有问题确实可以问他，但是我对和辅导员打交道这件事情有些畏惧。一个自闭的人。 然后越学越自闭，最后放弃。 和我一起参加纳新的老朱则坚持了下来，加入了技术组。挺羡慕的，也明白自己不够强大，不够努力。 这次我是大二，在课堂上和课外学了更多的东西，以前没写成功的留言板也写了一下，以及一个注册登录系统，录了个视频发b站（传送门）。 成功完成写一个爬虫的挑战，也因此入了python的坑。 打卡学习群2019年4月26日，一时心血来潮，建了一个学习群。 我给这个群制定了一些规则，比如：每月最多邀请一人进群，每周做个周总结，可以申请群相册来打卡等。 现在过去差不多四个月了，从刚开始的三个人到现在的五个人。还是有按照我的预期来发展的。 我认为能顺利发展的主要因素是邀请制进群，人多了就不好管了，人少就容易遵守规则。 运动在2018年6月30日，也就是大一的暑假快要开始的时候，我在QQ空间建了一个打卡相册，名字叫做《到6月30日要有300张》，一年365天，运动300天。 到2019年6月30日结算时，共有218张截图打卡，虽然没有达成目标，但是进度也差不多三分之二，比较满意。 开学之后打算开始新的打卡。 现状兴趣爱好小说在我建的打卡学习群里，有人建了一个写小说的打卡相册，这让我又开始想要写小说了。 我曾经写过网络小说，在起点中文网上面发过，但是写的很烂，挤出二十几章就没了。 不过他把我的兴趣又勾起来了。不要像以前那样，一打算写就把全部的精力都耗费在上面，而是合理分配时间精力 板绘2019年7月29日，我网购的数位板到了。这是我第一块数位板。 上一次想要买数位板时，我对自己说，得把手绘的习惯稳定下来之后才准买，不能买了不用。 我以前有一定的手绘基础，一般是临摹，偶尔会画几幅原创的画（比较丑啦）。很久没画，没时间，也没有那份兴趣。 打卡群里面唯一的妹子会板绘，有一次在上课之前我坐她旁边看到她在用数位板画画，是个大触。她在群里也新建了一个打卡相册，是一个画画相册。 这一次是快要过20岁生日了，我想买一个自己真正喜欢的生日礼物。犹豫了很久，终于下定决心买了。在知乎上查推荐品牌型号，在b站上看推荐视频，买到了一块两百多的数位板。 买完之后我还是很担心自己会不会把它扔在一边吃灰，安慰自己说老弟的高达和假面骑士模型比我这个贵多了，我买个这个来玩没啥的。 没想到板绘真的挺好玩的，到货当天我就临摹了一个menhera酱表情图，录了一个视频发在b站，挺有成就感的。 今天（2019年8月11日）晚上还打算试试直播临摹，昨天探过绘画直播间了，也是有人看这种的。 人际交往这个暑假在驾校学科目二，多了很多和别人交流的机会，和陌生人聊天也没有那么困难了。 健康状况健康状况不容乐观。 现在喝任何一种饮料都和喝咖啡一样兴奋，抵抗力越来越差，肚子也越来越胖了，成为了一个肥宅。 所适应的刺激水平越来越高，控制不住自己玩手机，玩电脑（主要是看b站）。]]></content>
      <categories>
        <category>自我管理</category>
      </categories>
      <tags>
        <tag>diary</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬取微信公众号文章1获取文章链接]]></title>
    <url>%2Fpost%2Fwechat_offical_account_spider_1_get_article_urls%2F</url>
    <content type="text"><![CDATA[爬取微信公众号的文章，之前一直觉得应该很难，我搞不定，但是尝试了一下发现，其实这和之前爬取的网站没有太大的区别。 本文记录了2019年8月7日爬取某一特定微信公众号的所有文章链接的方式，读者请注意时效性。 前言 需要一个可登录的微信公众号。本文采用的方法是使用微信公众号内部的搜索来搜索文章。 由于登录部分很复杂，我还没搞懂，本文直接手动获取cookies来登录。 参考链接记一次微信公众号爬虫的经历-CSDN 如何手动获取在登录后的微信公众平台的【素材管理】页面，点击【新建图文素材】，在新出现的编辑页面内，找到用于插入别的文章引用的，【超链接】图标。接着就会出现下图的窗口，输入需要获取的公众号查找即可。 分析打开F12开发者工具，搜索到公众号之后，查看Ajax请求 换页的时候会再次发出Ajax请求，多换几页，查看它们的参数的规律。 通过观察，知道了接口是： 1https://mp.weixin.qq.com/cgi-bin/appmsg 在访问这个接口时，需要在后面带上参数： 12345678910111213data = &#123; 'token':'271444813', #在同一次登录不变，在首页源代码里面可以获取 'lang':'zh_CN', # 不变 'f':'json',# 不变 'ajax':'1',# 不变 'random': str(random.random()),# 随机数 'action':'list_ex',# 不变 'begin':'0',# 代表页数，每翻一页就会+5，但是每一页的文章数不一定为5篇 'count':'5', # 应该是每一次获取的文章篇数 'query':'',# 不变 'fakeid':'MzAwNjA3Nzg0MA==',# 文章所在的公众号的id 'type':'9',# 不变&#125; 获取步骤 登录微信公众平台 手动在开发者工具中获取cookies字符串 带好参数访问Ajax接口，获取到所需要的json数据 示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243import requestsimport randomdef cookiejar_from_str(cookies_str): ''' 将cookies字符串转换为cookiejar ''' cookies=dict([item.split('=',1) for item in cookies_str.split(';')]) print(cookies) cookies=requests.utils.cookiejar_from_dict(cookies) return cookiesif __name__ == '__main__': # cookies字符串 cookies_str='''openid2ticket_okSCe0vbk_v5067L-AuViT1wrkEg=ARU37unMfUwam3yNHXFcw5CMFvTHMvmBnjjS8A8= '''.strip() # 这里我手动截去了大部分cookies字符串，明白意思即可 # headers headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3722.400 QQBrowser/10.5.3738.400' &#125; cookies = cookiejar_from_str(cookies_str) data = &#123; 'token':'271444813', #在同一次登录不变，在首页源代码里面可以获取 'lang':'zh_CN', # 不变 'f':'json',# 不变 'ajax':'1',# 不变 'random': str(random.random()),# 随机数 'action':'list_ex',# 不变 'begin':'0',# 代表页数，每翻一页就会+5，但是每一页的文章数不一定为5篇 'count':'5', # 应该是每一次获取的文章篇数 'query':'',# 不变 'fakeid':'MzAwNjA3Nzg0MA==',# 文章所在的公众号的id 'type':'9',# 不变 &#125; url='https://mp.weixin.qq.com/cgi-bin/appmsg' response_json = requests.get(url, cookies=cookies,headers=headers, params=data).json() for item in response_json["app_msg_list"]: # 获取url print(item['link'])]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>ajax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简易密码生成器]]></title>
    <url>%2Fpost%2Fsimple_password_generater%2F</url>
    <content type="text"><![CDATA[为了管理自己平时各种各样的账号密码，我使用了一个加密了的xlsx文件来记录，同时使用了密码生成规则。为了方便生成密码，使用python写了一个小工具。 由于代码比较简单，因此不做过多说明，仅做记录。 密码生成规则对于一些比较重要的账号，比如QQ，密码采用随机字符串，再记住，这样的字符串是没有规律的。 对于一些不太重要的账号，就使用对应的网站变量进行偏移。 代码导入模块12import randomimport clipboard 生成随机密码1234567891011def generate_random(length, alphabeta=None): ''' 生成指定长度的随机密码 ''' length = int(length) if alphabeta == None: alphabeta = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' # 字母表 password = '' for i in range(0, length): password += random.choice(alphabeta) return password 生成偏移密码12345678910111213def generate_offset(raw_password, offset, alphabeta=None): ''' 将原始密码进行偏移 ''' offset = int(offset) if alphabeta == None: alphabeta = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ' # 字母表 password = '' for character in raw_password: index = alphabeta.index(character) # 获取原本的索引 new_index = (index+offset) % len(alphabeta) # 获取偏移后的索引 password += alphabeta[new_index] return password 主函数1234567891011121314151617181920212223242526if __name__ == '__main__': print('q退出') print('random 生成随机密码') print('offset 生成偏移密码') cmd = input('&gt;') while cmd != 'q' and cmd != 'Q': password = '' if cmd == 'random': length = input('请输入密码长度：') password = generate_random(length) clipboard.copy(password) print('密码已复制到剪切板:\n',password) elif cmd == 'offset': raw_password = input('请输入原始密码：') offset = input('请输入偏移量：') password = generate_offset(raw_password, offset) clipboard.copy(password) print('密码已复制到剪切板:\n',password) else: print('请输入正确的指令') print('q退出') print('random 生成随机密码') print('offset 生成偏移密码') cmd = input('&gt;')]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学校信息门户模拟登录]]></title>
    <url>%2Fpost%2Fportal_login%2F</url>
    <content type="text"><![CDATA[将登陆学校信息门户的部分专门封装成一个模块，需要的时候导入。 相关链接本文代码的github链接 获取登录所需表单数据 从图中看到的，和在登录页面源代码中查找的，需要的表单数据如下： username：用户名，也就是信息门户账号 password：是经过加密之后的密码 lt：是一个每次请求都会变化的表单隐藏域值 dllt：固定表单隐藏域值 execution：固定表单隐藏域值 _eventId：固定表单隐藏域值 rmShown：固定表单隐藏域值 除了需要表单数据之外，还需要在登录页面源代码中获取密钥，详情见：学校信息门户模拟登录之密码加密 使用BeautifulSoup来获取这些数据。 123456789101112131415161718192021222324252627282930313233343536373839404142from portal_login.encrypt import *import requestsfrom bs4 import BeautifulSoupdef get_login_data(login_url,headers): ''' 长安大学登录表单数据解析 :param login_url: 登录页面的url :return (登录信息字典,获取时得到的cookies) ''' username=input('input username:') password=input('input password:') username.strip() password.strip()#去除头尾空格 #获取登录所需表单数据 response=requests.get(login_url,headers=headers) html=response.text soup=BeautifulSoup(html,'lxml') #获取密钥来加密密码 pattern = re.compile('var\s*?pwdDefaultEncryptSalt\s*?=\s*?"(.*?)"') key = pattern.findall(html)[0] password=encrypt_aes(password,key) lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': username, 'password': password, 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return (login_data,response.cookies) 要注意的是，获取完数据之后，需要将response的cookies留下来，因为不同cookies对应的登录数据也不一样（比如说每次打开页面都不一样的密钥和lt） 登录登录过程分析在登录页面输入账号密码，F12打开开发者工具，Network勾选Preserve log，点击登录，然后就会出现下图场景： 找到从登陆页面出去的第一个响应，可以发现这个响应的状态码是302，代表“重定向”。在Response Headers里面可以找到Location这个键，它指示的是重定向的地址。 这个响应的含义大概是“服务器告诉浏览器带着给它的cookies去访问Location指示的url” 在刷出来的一大堆响应中继续寻找，找到下一个地址： 从图中可以看到，目的地址已经是门户的主页url了，继续跳转： 随便打开一个登陆才能查看的页面，查看它的cookie，发现浏览器带着这几个cookies来访问这个页面，也就是说，我们需要获取到这几个cookies，才能登录成功： 处理跳转默认情况下，requests的post()方法是得到跳转后最终页面的响应，也就是说，登录成功就返回门户主页的响应，登录失败就返回跳转之后回到的登录页面的响应。 需要设置一个参数，来阻止它进行跳转： 1response=requests.post(login_url,headers=headers,data=data,cookies=cookies,allow_redirects=False) 也就是： 1allow_redirects=False 不允许跳转，第一次请求得到什么响应就返回什么响应。 每一次跳转，我们需要做的工作如下： 将现有的cookies与新获取的cookies合并 找到下一个重定向地址，带上cookies，再一次请求 实现代码如下： 123456789response=requests.post(login_url,headers=headers,data=data,cookies=cookies,allow_redirects=False) while response.status_code == 302:#如果响应状态是“重定向” #合并新获取到的cookies cookies=join_cookies(cookies,response.cookies) #获取下一个需要跳转的url next_station=response.headers['Location'] response=requests.post(next_station,headers=headers,cookies=cookies,allow_redirects=False)cookies=join_cookies(cookies,response.cookies) 其中join_cookies()的实现如下： 1234567def join_cookies(cookies1,cookies2): ''' 将cookies1和cookies2合并 ''' cookies=dict(cookies1,**cookies2) cookies=requests.utils.cookiejar_from_dict(cookies) return cookies 登录函数总览123456789101112131415161718192021222324252627def login(login_url,headers,check_url=None): ''' 登录到CHD信息门户 :param login_url: 登录页面的url :param headers: 使用的headers :param check_url: 用于检查的url，尝试请求此页面并核对是否能请求到 :return: 已登录的cookies ''' data,cookies=get_login_data(login_url,headers)#获取登录数据 response=requests.post(login_url,headers=headers,data=data,cookies=cookies,allow_redirects=False) while response.status_code == 302:#如果响应状态是“重定向” #合并新获取到的cookies cookies=join_cookies(cookies,response.cookies) #获取下一个需要跳转的url next_station=response.headers['Location'] response=requests.post(next_station,headers=headers,cookies=cookies,allow_redirects=False) cookies=join_cookies(cookies,response.cookies) #登录检查 if check_url != None: response = requests.get(check_url,headers=headers,cookies=cookies) if response.url==check_url: print("登录成功") else: print("登录失败") return cookies]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学校信息门户模拟登录之密码加密]]></title>
    <url>%2Fpost%2Fportal_login_encrypt%2F</url>
    <content type="text"><![CDATA[以前写的爬虫无法登录到学校的信息门户上去了，因为门户的新JS代码将表单的密码先加密了一次，再将其与别的表单数据POST过去。使用的是AES加密的CBC模式。 本文前半部分是我的python组长雁横给组员们讲解的信息门户的密码加密思路，然后由我总结成文，后半部分是我自己写的加密代码实现，使用python的PyCryptodome库来进行加密。 参考链接 浏览器开发者工具基本使用教程-博客园 Python运行js代码 python加密与解密（大致介绍了加密解密算法）-博客园 常见加密方式与python实现-简书 PyCryptodome库的官方文档 python encode方法-菜鸟教程 本文代码的github链接 问题描述登录之后查看原本提交表单的部分可以发现，密码由明文传输改成密文传输了。于是原本只需要POST账号和密码的明文就行，现在需要多经过一步——加密。 起码咱们学校还是有考虑安全问题嘛！OVO 分析加密过程因为登录到主页的时候已经是加密好的密码，所以加密工作应该是在登录页面就进行的。 所以回到登录页面刷新一下，筛选javascript文件（因为js文件是用于动作的） 在这几个js文件中找找有没有线索，然后在其中一个js文件中找到了一个密码加密函数。 encryptPassword()传入密码，返回加密后的密码。 12345678function encryptPassword(pwd0) &#123; try&#123; var pwd1 = encryptAES(pwd0,pwdDefaultEncryptSalt); $("#casLoginForm").find("#passwordEncrypt").val(pwd1); &#125;catch(e)&#123; $("#casLoginForm").find("#passwordEncrypt").val(pwd0); &#125;&#125; 核心逻辑就一句。 1var pwd1 = encryptAES(pwd0,pwdDefaultEncryptSalt); encryptPassword()调用了一个名为encryptAES()的函数，参数pwd0可能是未加密的密码，pwdDefaultEncryptSalt可能是加密用的密钥。try-catch不用说了，就是错误处理。 encrypt是加密的意思，而AES是一种加密的方式。 而刚刚的js文件里面有一个文件就带着encrypt这个单词，点进去看，找到了下一个函数： encryptAES()传入密码明文和AES密钥，返回密文。 1234567function encryptAES(data,aesKey)&#123; //加密 if(!aesKey)&#123; return data; &#125; var encrypted =getAesString(randomString(64)+data,aesKey,randomString(16)); //密文 return encrypted;&#125; 代码逻辑： 如果没有给出密钥，那么就不加密直接返回明文； 如果给出了密钥，那么就调用getAesString()函数来获取密文 返回密文 其中randomString()函数代码如下： 123456789var $aes_chars = 'ABCDEFGHJKMNPQRSTWXYZabcdefhijkmnprstwxyz2345678';var aes_chars_len = $aes_chars.length;function randomString(len) &#123; var retStr = ''; for (i = 0; i &lt; len; i++) &#123; retStr += $aes_chars.charAt(Math.floor(Math.random() * aes_chars_len)); &#125; return retStr;&#125; 从上图可以看到，getAesString()就在这个函数上方。 getAesString()传入明文、密钥、偏移量，返回密文。 12345678910111213//AES-128-CBC加密模式，key需要为16位，key和iv可以一样function getAesString(data,key0,iv0)&#123;//加密 key0=key0.replace(/(^\s+)|(\s+$)/g, "");//去除开头和结尾的空白 var key = CryptoJS.enc.Utf8.parse(key0); var iv = CryptoJS.enc.Utf8.parse(iv0); var encrypted =CryptoJS.AES.encrypt(data,key, &#123; iv:iv, mode:CryptoJS.mode.CBC, padding:CryptoJS.pad.Pkcs7 &#125;); return encrypted.toString(); //返回的是base64格式的密文&#125; 在这个函数中调用了aes加密算法的函数来加密 密钥还差密钥pwdDefaultEncryptSalt，去js文件里面搜索： 图中可以看到，密钥的来源是pwdEncryptArr[1]变量，但是在js文件里面却找不到这个从哪里来的了。 不过去搜索登录页面源代码的时候发现它就写在页面里面。 得到了密钥：1var pwdDefaultEncryptSalt = "QgkxfHXdbwRHcvDI"; 后来发现，这个密钥同样每次都会变化，可以在获取表单变化的隐藏域值的时候顺便获取了。 总结信息门户加密算法是：AES-128-CBC加密模式，key需要为16位，key和iv可以一样（从注释里面得到的） 密文data是长度为64的随机字符串与登录密码连接。 密钥key就放在登录页面源码内，每次都会变化，需要动态获取。 偏移量iv是长度为16的随机字符串。 现在知道了它的加密算法以及密钥，我们在模拟登录的时候把我们的密码用同样的方式加密，向以前那样发送就可以登录了。 有两种解决方案： 直接在python里面运行复制来的js代码。参考：Python运行js代码 使用python进行加密 加密python代码实现AES简介AES（Advanced Encryption Standard）（高级加密标准），用于代替原本的DES（Data Encryption Standard） 2006年，高级加密标准已然成为对称密钥加密中最流行的算法之一。——搜狗百科 AES算法将明文分为长度相等的若干组，每次加密一组数据。 分组长度固定为128位（16字节），密钥长度则可以是128，192或256比特（16、24和32字节）。 我遇到的加密问题需要的是128位的密钥。 PyCryptodome库这个库是PyCrypto库（已经停止更新）的延续。 PyCryptodome库的官方文档 安装方式（windows）： 1pip install pycryptodomex 导入方式： 1import Cryptodome 获取密钥在页面源码里面密钥的格式是： 1var pwdDefaultEncryptSalt = "QgkxfHXdbwRHcvDI"; 使用正则表达式来解析： 12345678910def get_encrypt_salt(login_url): ''' 获取密钥 :param login_url:登录页面的url :return: (密钥,密钥对应的cookies) ''' response=requests.get(login_url) pattern = re.compile('var\s*?pwdDefaultEncryptSalt\s*?=\s*?"(.*?)"') pwdDefaultEncryptSalt = pattern.findall(response.text)[0] return (pwdDefaultEncryptSalt,response.cookies) 获取初始化向量iv是初始化向量，也称作偏移量。 在上面的分析中，传给加密函数的iv是一个随机字符串： 1var encrypted =getAesString(randomString(64)+data,aesKey,randomString(16)); //密文 现在用python来实现这个randomString() randomString()的python实现12345678910def random_string(length): ''' 获取随机字符串 :param length:随机字符串长度 ''' ret_string='' aes_chars = 'ABCDEFGHJKMNPQRSTWXYZabcdefhijkmnprstwxyz2345678' for i in range(length): ret_string+=random.choice(aes_chars) return ret_string 那一串用于随机的字符串是我从js文件的注释里面复制下来的，这个串并没有覆盖全部的字母和数字，为了防止意外，直接使用它的。 getAesString()的python实现12345678910111213141516171819202122232425from Cryptodome.Cipher import AESfrom Cryptodome.Util.Padding import pad#用于对齐import base64def get_aes_string(data,key,iv): ''' 用AES-CBC方式加密字符串 :param data: 需要加密的字符串 :param key: 密钥 :param iv: 偏移量 :return: base64格式的加密字符串 ''' #预处理字符串 data=str.encode(data) data=pad(data, AES.block_size)#将明文对齐 #预处理密钥和偏移量 key=str.encode(key) iv=str.encode(iv) cipher = AES.new(key, AES.MODE_CBC, iv)#初始化加密器 cipher_text=cipher.encrypt(data)#加密 #返回的是base64格式的密文 cipher_b64=str(base64.b64encode(cipher_text), encoding='utf-8') return cipher_b64 对齐首先，先将明文对齐，因为AES加密是分组加密，所以明文的长度需要是组长度的倍数。 有两种方式 Cryptodome.Util.Padding中的pad函数就可以实现对齐，就是我采用的办法。 组长雁横的代码是这样实现对齐的： 1234def add_to_16(value): while len(value) % 16 != 0: value += '\0' return str.encode(value) # 返回bytes 预处理js代码里面在加密之前，对数据做了编码处理： 12var key = CryptoJS.enc.Utf8.parse(key0);var iv = CryptoJS.enc.Utf8.parse(iv0); 因此也顺便处理一下。 python encode方法 描述Python encode() 方法以 encoding 指定的编码格式编码字符串。errors参数可以指定不同的错误处理方案。 语法encode()方法语法： 12&gt; str.encode(encoding='UTF-8',errors='strict')&gt; encryptAES()的python实现1234567891011121314def encrypt_aes(data,key=None): ''' 进行AES加密 :param data: 需要加密的字符串 :param key: 密钥 :return: 如果key存在，则返回密文，否则返回明文 ''' if(not key): return data else: data=random_string(64)+data iv=random_string(16)#偏移量 encrypted =get_aes_string(data,key,iv) return encrypted encryptPassword()的python实现1234567891011def encrypt_password(password,login_url): ''' 加密密码 :param password: 需要加密的密码 :param login_url:登录页面的url :return: (加密后的密码,对应的cookies) ''' key,cookies=get_encrypt_salt(login_url) password.strip()#去除头尾空格 encrypted=encrypt_aes(password,key) return (encrypted,cookies) 这就完成了 测试代码123456if __name__ == '__main__': login_url='http://ids.chd.edu.cn/authserver/login?service=http%3A%2F%2Fportal.chd.edu.cn%2F' password=input('password:') password,cookies=encrypt_password(password,login_url) print('encrypted password:',password) 有效性检验可以使用浏览器开发者工具的控制台，调用js函数，传入同样的参数，看是否得到相同的结果。 测试结果如图：]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python读取ini文件失败的原因]]></title>
    <url>%2Fpost%2Fpython_read_ini_No_section%2F</url>
    <content type="text"><![CDATA[尝试使用python的configparser来读取ini配置文件，但是遇到了No Section的错误。 最终发现其实是路径出了问题。 问题描述初始代码简化之后是： 1234567from configparser import ConfigParserif __name__ == '__main__': config=ConfigParser()#创建配置对象 config.read('test.ini')#读取配置文件 result=config.get(section='test',option='name')#读取test下的name print(result) 同目录的test.ini的内容如下： 12[test]name = tom 但是运行出现了configparser.NoSectionError: No section: &#39;test&#39;的错误 原因探索经过单步调试后发现并没有读取到文件的内容，猜测可能是没有找到文件。 以前在import自定义模块的时候遇到过类似的问题，当时的解决方法是把当前工作路径设置为正在执行的文件所在的路径。 解决方案 使用绝对路径 将当前工作路径改为当前文件路径，再使用相对路径 第二种方法的代码如下： 12345678910from configparser import ConfigParserimport osif __name__ == '__main__': curpath=os.path.dirname(os.path.realpath(__file__)) filename=os.path.join(curpath,"test.ini") config=ConfigParser()#创建配置对象 config.read(filename)#读取配置文件 result=config.get(section='test',option='name')#读取test下的name print(result)]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[excel实现周总结签到积分制]]></title>
    <url>%2Fpost%2Fexcel_weekly_sign%2F</url>
    <content type="text"><![CDATA[我在自己一个学习群里设定了一个周总结制度，这篇博客记录一下如何使用excel函数来实现计算打卡相册的积分。这里其实我用的是wps表格，但是函数一样，所以我就分类在excel里面。 简介每周日，每个人在群聊天发一个周总结，内容是自己这周学习了什么，没有限制，只是给大家一个自我反省的机会。 如果没有可以写的东西，那么也在群里面报备，方式为在群聊天中说：“本周无总结”或者别的能表明这一事实的话。别有压力，只是回复一句话的功夫。 如果没有报备也没有在截止之前发周总结，将会被艾特提醒。可以在下一周总结之前补。 为了方便描述，下文把发送周总结称为“签到” 积分规则 如果本周签到了，积分=原本积分+正调整参数 如果未签到，积分=原本积分+负调整参数 如果补签到，积分=原本积分 签到登记表样例 成员ID 昵称 正常签到次数 周总结积分 week1 week2 week3 week4 week5 week6 1 憧憬少 1 1 1 2 听星缘 1 1 1 3 简白 1 1 1 4 HUST 1 1 1 5 咸鱼米 1 1 1 1表示已签到，-1表示未签到，补签改为0 编写公式正常签到次数即计算1出现的次数（补签不算），如果用之前的SUMIF函数就是： 1=SUMIF(对应成员的签到区域,1) 但是我又查到一个更适合的函数：COUNTIF 参数和SUMIF差不多含义，写成公式也是一样 1=COUNTIF(对应成员的签到区域,1) 但是前者只能计算1出现的次数，如果计算-1出现的次数就不行了。 周总结积分比较简单，不赘述了。 1=COUNTIF(E2:ZZ2,1)*积分规则!$C$3+COUNTIF(E2:ZZ2,-1)*积分规则!$D$3*(-1)]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[excel实现打卡相册积分制]]></title>
    <url>%2Fpost%2Fexcel_clock_in_album%2F</url>
    <content type="text"><![CDATA[我在自己一个学习群里设定了一个打卡相册制度，这篇博客记录一下如何使用sumif函数来实现计算打卡相册的积分。这里其实我用的是wps表格，但是函数一样，所以我就分类在excel里面。 规则说明群员可以申请建立打卡相册，需要自己下载群文件中的登记表填写相关信息，然后就可以创建群相册，在群相册描述里面写上打卡内容。 相册状态 相册状态：正在进行、放弃、失败、归档 如果连续三天未打卡，管理员就删除相册，并在登记表内将相册状态设置为“失败”。 对于有期限的相册，比如打卡目标是“两周读完《xxx》”，那么在结束日期时，可以将其状态设置为“归档”。相册资源回收（删除或改作他用），避免资源闲置。若持续时间大于等于一百天，则可以选择保留相册。（可以给其他群员作榜样） 对于没有期限的相册，比如“每天背单词”，那么在创建时间满三十天后就可以选择“归档”（三十天应该够养成一个小习惯了） 相册删除后，相册记录还会保留在登记表里面 积分计算 创建相册不需要积分，但是“放弃”或“失败”每个会扣除5积分 一个成员的打卡相册总积分=他所有相册的积分之和 单个相册的积分： 若相册状态是“正在进行”，则积分=持续天数1*正调整参数=正调整参数*（当前日期-创建日期） 若相册状态是“归档”，则积分=正调整参数*持续天数2=正调整参数*（结束日期-创建日期）,目前参数为0.5 若相册状态是“放弃”或“失败”，则积分=负调整参数，也就是扣除积分，目前参数为-5，即扣除5积分 相册字段 字段 描述 相册ID 这个手动赋值为：最大的相册ID+1 申请人昵称 可以写真名或者自己的群名片昵称，只要大家能通过这个知道是谁即可 相册名称 无特别要求，不过最好写明昵称和目的，例如：憧憬少的英语流利说APP打卡 相册目标描述 描述你要通过这个打卡相册达到的目标，例如：每天读口语10分钟 如何判断目标完成 上传到打卡相册的图片应当满足怎样的要求，例如：每天在相册内上传一张可以表明读了10分钟的截图 相册状态 目前用到的状态：正在进行，放弃，失败，归档（仅留表中记录，相册本身删除，若打卡满100天可选择保留） 创建日期 用于计算持续天数的字段 结束日期 归档日期，或有期相册结束日期。 持续天数 除了正在进行状态，其他状态都停止增加持续天数 相册类型 目前的类型：无期（未规定期限，满30天可以选择归档），有期（规定了完成期限，若期限内完成则归档，未完成则为失败） 打卡相册积分 利用表格的自动填充功能复制上一个相册的公式 编写公式计算相册持续天数相册持续天数有两种情况，一种是“正在进行”，一种是其他状态，只有“正在进行”的打卡相册会继续计算天数。 也就是说： “正在进行”的相册的持续天数=今天日期-创建日期 其他状态相册的持续天数=结束日期-创建日期 因此需要一个IF判断。 IF函数的语法是： 1IF(条件，条件为真时的返回值，条件为假时的返回值) 公式如下（中文处替换为对应的单元格） 1=IF(相册状态=&quot;正在进行&quot;,TODAY()-创建日期,结束日期-创建日期) 计算打卡相册积分根据上述规则，我们需要用IF函数判断一下相册状态。 这里还用到了一个函数OR excel里面的与或非不是用逻辑运算符的，而是用函数。 公式如下： 1=IF(相册状态=&quot;正在进行&quot;,1,0)*(积分规则!$C$2)*持续天数+IF(相册状态=&quot;归档&quot;,1,0)*(积分规则!$C$2)*持续天数+IF(OR(相册状态=&quot;失败&quot;,相册状态=&quot;放弃&quot;),1,0)*(积分规则!$D$2)*(-1) 其中，积分规则!$C$2代表的是我在另一个名为“积分规则”的表中的C2格中设置的一个正调整参数。积分规则!$D$2同理。 计算个人总积分一个成员可以有多个相册，因此需要将他所有的相册的积分相加。 相加可以使用SUM函数，来将已知区域求和。 例如现在的情况是这样的： 申请人昵称 相册名称 打卡相册积分 憧憬少 憧憬少的英语流利说打卡 18 简白 简白的英语打卡 12 咸鱼米 米米的啃书打卡 12 咸鱼米 米米的每日练习 5 H.U.S.T. H.U.S.T.的小说练笔 3 米米有两个相册，她的积分就是12+5=17，相加的格子不确定，要如何用公式计算她的积分呢？ 我查到了SUMIF这个函数，也就是“条件相加”，格式如下： 1SUMIF(条件区域,求和条件,[实际求和区域]) 它的官方文档链接 条件区域：也就是要按条件计算的单元格区域。不太好理解，我的理解是，这个函数对于“条件区域”内符合条件的单元格进行求和。 求和条件：定义进行求和的单元格需要满足的条件。例如：32、”&gt;32”、B5、”32”、”苹果” 或 TODAY ()。任何文本条件或任何含有逻辑或数学符号的条件都必须使用双引号 (“) 括起来。 如果条件为数字，则无需使用双引号。 实际求和区域：如果省略，则将条件区域当作实际求和区域。 在这里，条件区域是“申请人昵称”，实际求和区域是“打卡相册积分”，求和条件是要计算积分的成员昵称。这样我们就可以将某个成员的所有相册数据所在的那几行给筛选出来，再将这几行的打卡相册积分相加，得到这个成员的总积分了。 某成员打卡相册总积分计算公式： 1=SUMIF(打卡相册登记表!B:B,某成员昵称,打卡相册登记表!K:K) 这里的B:B和K:K就分别对应了“申请人昵称”和“打卡相册积分”这两列。]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WIN10共享文件夹]]></title>
    <url>%2Fpost%2Fwin10_share_folder%2F</url>
    <content type="text"><![CDATA[这是我以前写的第一篇博客的补档，由于图片太多于是就发在了CSDN，现在不愁图片的问题了，于是就在整理电脑文件时把这篇博客在个人博客这边发一下。 这是在CSDN的第一篇博客，也是我第一篇正式的博客。 我们的linux老师上课时用到了共享文件夹，于是我就百度学习了一下。 来写一下刚刚学到的共享文件夹的方法。 共享方法 首先右键你想要共享的文件夹，【共享】-&gt;【特定用户】 2.在选择框里面选择Everyone，接着点击旁边的【添加】 3.调整权限后，点击【共享】即可 4.共享完成 别人进入共享文件夹的方法1.你可以复制系统给你的链接给局域网内（我只试过局域网）的别人，让他复制到文件资源管理器地址栏 2.或者找到资源管理器最左下角的【网络】，让他点进去就是了。 点进去之后的效果是这样： 然后你就可以用这个文件夹和局域网里的各位来分享文件了。]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个好用的图床管理工具PicGo]]></title>
    <url>%2Fpost%2FPicGo_imgur%2F</url>
    <content type="text"><![CDATA[先前给hexo博客插图片都是把图片commit到github上再手动构造链接，比较麻烦，又不想把图片直接放在博客所在的库。 这次找到了一个好东西：PicGo 测试一下图片： 很方便的一个工具，简单地截图然后上传剪切板图片，它就自动帮我上传到github上我准备好的库里面，然后把markdown格式的图片引用复制到我的剪切板里面。 具体如何下载安装和使用，它的官方文档肯定比我写的详细，不赘述。 我使用的是github图床（当然，它还支持别的图床），提一下与它相关的一个比较重要的插件。 PicGo插件：github-plus它的github库链接 它的作用是，让本地的PicGo相册和github库的内容同步。 PicGo本体只负责上传，不负责删除。我在发现上传错图片，在PicGo相册中删除了图片之后，发现github上面并没有删除这些图片。这是个比较严重的问题。而手动删除的时候很麻烦，要clone到本地，删除之后再提交。 好在找到了这个插件。 这个插件的功能： 将删除操作同步到github 从github上把图片同步到本地相册，从而可以复制链接]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python相对路径是相对于哪里]]></title>
    <url>%2Fpost%2Fpython_relative_path%2F</url>
    <content type="text"><![CDATA[在学习scrapy时，保存数据到文件的时候，发现一直出现“找不到这样的文件或文件夹”的错误，最后发现是因为python的相对路径。 问题描述学习scrapy时，编写pipeline来将数据保存到文件当中，代码如下： 123456class NovelPipeline(): def process_item(self, item, spider): with open('Novel/'+str(item['title'][0])+'.txt','w') as f: for p in item['content']: f.write(p+'\n') return item 看着爬取时调试信息飞快闪过（爬取的东西有点多），却没有发现我准备好的Novel文件夹里面多出文件，连忙把爬虫停下来。发现出现了“找不到这样的文件或文件夹”的错误。 分析过程查看日志信息，发现文件名是对的，但是为什么不行呢？ 于是我在pipelines.py里面写了测试代码： 12with open('Novel/'+'文件名'+'.txt','w') as f: print(1) 发现同样的错误。 我把前面的文件夹去掉，也就是： 12with open('文件名'+'.txt','w') as f: print(1) 发现文件生成在了我的工作目录下！ 这个时候我才注意到相对路径的问题。 当前目录是这样的（略去无关文件）： learn_scrapy 文件名.txt practice practice pipeline.py 我本来以为这个相对路径会使得文件生成在pipelines.py的同级目录下，但是却生成在了我的VScode的工作文件夹？ 我回忆起java课时老师写错相对路径导致无法显示图片的问题。那时也是需要相对当前项目的根目录来写相对路径的。我认为这是eclipse的特性。 会不会这个也是vscode的特性？ 于是我搜索“python 相对路径”，找到了和我遇到类似问题的朋友：vscode中使用python相对路径问题?-知乎 我的工作目录是/Work 我在工作目录中创建了文件/Work/Program/main.py 并且运行main.py 生成了 file.txt文件 123&gt; with open('file.txt','w') as f:&gt; f.write('HelloWorld')&gt; 我以为file.txt在/Work/Program路径下，和创建它的main.py在一个路径中 结果file.txt这个文件却在/Work路径下面（/Work/file.txt），而不是我所期望的/Python/Program路径下面 所以应该怎么配置，或者安装什么插件，能让py创建的文件在自己的相对路径下，而不是直接跑到了工作路径那里？ 这个问题怎么解决啊，困扰了我好久，而我又比较喜欢vscode的界面不想放弃它。求解答！ 看了回答之后我继续搜索，终于解决了困惑。 解决方案参考链接： Python里使用相对路径的坑-简书 Python里写这种相对路径, 是相对于终端的当前目录的. 解决办法是, 获取脚本所在目录, 构造绝对路径]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[调整博客分类]]></title>
    <url>%2Fpost%2Fadjust_categories%2F</url>
    <content type="text"><![CDATA[目前个人博客内的分类不太合理，于是重新调整分类 调整前分类 c++ java python 工具 日志 流程 算法 标签 awt c++ git hexo java mysql notepad++ python scrapy stl 信息检索 小游戏 日志 爬虫 算法 项目 题目 分析c++、java、python等大类很明显和标签重叠了。 我目前对二者的理解： 分类表明一个事物是什么； 标签表明一个事物有什么。 按照文章区别于其他类型文章的特征来分类。 新建一个标签前，要考虑这个标签的可重用性，比如c++、java这类标签肯定会经常用到，但是notepad++这类基本只用一次了，所以将它归到IDE这个标签内。stl和awt这类标签不常用，可以删去。 调整后分类 分类 内容 工具使用 工具的获取（下载安装）、使用，类似教程 编程语言 记录遇到的一些语法问题 项目总结 主要记录过程及遇到的问题与方案，包括一些感想和体会 算法模型 记录一些算法相关的题目以及概念 过程记录 记录解决方案和过程，记录经验总结，简化版的项目总结，侧重过程 调整之后，分类比之前清晰多了，我写新的博文，就知道应该归类到哪里，找的时候也知道应该到哪里去找了。 标签减少了一些标签 IDE ：分得比较宽泛，连notepad++都算进去 c++ git hexo java mysql python scrapy 框架 爬虫]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo日记本]]></title>
    <url>%2Fpost%2Fhexo_diary%2F</url>
    <content type="text"><![CDATA[打算从纸质日记转到电子日记。 之前是一个月的日记放在一个markdown文件里面，每天一个一级标题。昨天突发奇想，为啥不用Hexo来搭建日记本呢？它本来就是用来写博客（blog网络日志）的呀。 于是今天就来搭建hexo日记本 前言本文只分享设计思路以及步骤，不提供详细教程，详细教程可以看这个：【持续更新】最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析-遇见西门 优点利用hexo搭建日记本有很多优点： 好看，并且可以随时换主题 比我之前的方式更加地将日记格式化，便于以后编写脚本来管理 可以在scaffolds里面设置日记模板 可以设置分类与标签 有的主题甚至能搜索文章 需求 不联网：这个日记本和我部署到github上面的博客有些不一样，因为这个是比较隐私的，我不打算放在网上，仅利用移动硬盘备份。并且看日记都在本地，不使用外链图片，以免断网的时候无法查看 功能少：并且不需要评论，阅读计数等功能，起到的只是一个阅读器的作用。 重美观：需要能够方便地切换主题。 无需侧边目录：因为我打算一篇只记录一天的内容，写不了太多，标题层级不会太多。 写日记要便捷 步骤参考链接： hexo官方中文文档 初始化hexo刚开始的时候我不是很清楚hexo在一台电脑上是否可以搭多个博客，后来发现，hexo的每个博客其实就是一个“项目”，那些命令得在已经搭建博客的文件夹里面才能使用，而不是我之前想的“全局命令”。 首先初始化： 123hexo init &lt;folder&gt;cd &lt;folder&gt;npm install 然后就可以通过以下命令查看本地内容了： 1hexo server 或简写为 1hexo s 全局设置在博客根目录下的_config.yml内配置 标题相关1234567title: 日记subtitle:description:keywords:author: 憧憬少language: zh-CNtimezone: permalink这个设置会决定你的文件最后渲染之后放在哪里。 利用hexo g来渲染markdown文件，它会将渲染好的html文件放在public目录下，部署到github时，上传的就是这个文件夹里面的内容。 比如最开始的设置： 1permalink: :year/:month/:day/:title/ 则会将最开始的hello-world.md示例文章给生成在public\2019\06\26\hello-world这个文件夹当中。 我觉得一天的内容单独放一个文件夹有点不太合适，而一年的内容全部放在一个文件夹的话，三百多个文件也不好管理，所以按照一个月的内容放在一个文件夹内的规则，将这个设置改成： 1permalink: :year/:month/:title/ new_post_name新建文章的文件名，因为日记按照时间管理比较方便，因此在文件名中加入日期 1new_post_name: :year-:month-:day-:title.md # File name of new posts 图片问题我之前一直是将图片上传到github的一个repo上面然后使用链接的，看了文档之后才发现原来还有更简便的方法！ 方法一 外链首先开启仓库的github page这个设置。 比如用户名是HaneChiri，创建的仓库名叫blog_images，那么在这个仓库根目录下的图片avatar.jpg的链接就是 1https://hanechiri.github.io/blog_images/avatar.jpg 而不是 1https://github.com/HaneChiri/blog_images/avatar.jpg 后者是浏览编辑这个图片的链接，而不是图片本身。 上传之后无法访问这个链接也不要急，等几分钟就可以了。 日记本不能使用这个，因为我需要在不联网的时候也能看。 方法二 资源文件夹来自资源文件夹-hexo官方文档 资源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 source/images 文件夹中。然后通过类似于 ![](/images/image.jpg) 的方法访问它们。 早知道认真看文档了，插图片就简单多了。 这个是将图片放在source/images中，而我将Typora设置成将图片自动保存在同目录下的images中，编辑之后只要将这个文件夹内图片给复制到前者所述文件夹，就可以在编辑以及渲染时都看到图片了。 方法三 下载插件Hexo文章中插入图片的方法-CSDN 我不需要每个文章的图片分开管理，这样会导致source\_posts\内有太多没用的空文件夹，因此我使用方法二，读者可以选择适合自己的方法。 主题为了防止和联网的博客弄混（毕竟一旦将日记上传上去，repo里面就会留下痕迹，哪怕删掉也看得到，除非删repo），我打算换个别的主题。 找到了几个心仪的： Gal ：galgame。和我第一次用的夏娜 shana主题是同类型的 Sakura ：樱。贼好看，功能蛮多的样子 One ：单页面。每个文章都可以配图，上面的几个也是 但是考虑到个人的一些因素，还是先用着Next吧，反正可以换。 外观Next主题有四种外观（scheme），在配置文件（themes\next\_config.yml）中可以找到并修改： 12345# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini 侧边栏社交链接最右边的||后面跟着的是文字边上显示的图标 12345678910111213social: GitHub: https://github.com/HaneChiri || github #E-Mail: mailto:yourname@gmail.com || envelope #Weibo: https://weibo.com/yourname || weibo #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype Bilibili: https://space.bilibili.com/13290087 头像在对应的位置放上头像图片 123456789101112# Sidebar Avataravatar: # In theme directory (source/images): /images/avatar.gif # In site directory (source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: false # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false 左右为了防止和博客混淆而误将日记上传，而将侧边栏调整到相反方向 1234sidebar: # Sidebar Position, available values: left | right (only for Pisces | Gemini). #position: left position: right 回到顶部这么好用的小功能当然要开着呀！ 123456back2top: enable: true # Back to top in sidebar. sidebar: true # Scroll percent label in b2t button. scrollpercent: true 菜单首先在解开“分类”(categories)和“标签”(tags)的注释 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 但是这个还只是在侧边的菜单栏处显示了“分类”和“标签”两项，还没有功能。 需要在根目录下使用指令来生成这两个页面： 12hexo new page categorieshexo new page tags 这下能显示了，但是仍然不够，因为Next还没有识别出这两个页面就是分类和标签页面。 打开source\categories\index.md，里面是： 1234---title: categoriesdate: 2019-06-26 15:44:09--- 在里面加上一句，变成： 123title: categoriesdate: 2019-06-26 15:44:09type: "categories" 这样就能识别出这是分类页面了，能够使用了。标签页面同理。 本地搜索第一步 修改主题设置找到这个设置： 1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # If auto, trigger search by changing input. # If manual, trigger search by pressing enter key or search button. trigger: auto # Show top n results per article, show all results by setting to -1 top_n_per_article: 1 # Unescape html strings to the readable one. unescape: false 将enable改为true之后就会在菜单显示一个”搜索“，但是还无法使用。 照着注释里面那个github项目内的说明 第二步 下载插件：1npm install hexo-generator-searchdb --save 第三步 添加全局设置在根目录下的_config.yml加上如下设置： 123456search: path: search.xml field: post format: html limit: 10000 content: true 我把帮助中的注释复制过来就是下面这样： 12345678910111213141516171819# see https://github.com/theme-next/hexo-generator-searchdbsearch: # file path. By default is search.xml . If the file extension is .json, the output format will be JSON. Otherwise XML format file will be exported. path: search.xml # the search scope you want to search, you can chose: # post (Default) - will only covers all the posts of your blog. # page - will only covers all the pages of your blog. # all - will covers all the posts and pages of your blog. field: post # the form of the page contents, works with xml mode, options are: # html (Default) - original html string being minified. # raw - markdown text of each posts or pages. # excerpt - only collect excerpt. # more - act as you think. format: html #define the maximum number of posts being indexed, always prefer the newest. limit: 10000 # whether contains the whole content of each article. If false, the generated results only cover title and other meta info without mainbody. By default is true. content: true 意外地轻松便捷呢。 编写快捷打开的脚本虽然弄好了，但是每次想写还得打开命令行输入命令，再进入文件夹用Typora打开文件，太麻烦了。 于是写一下bat批处理脚本。 这东西其实就是把在命令行执行的命令放在一个文本文件然后把后缀名改成.bat而已。 不过我不是很熟命令，弄了很久。 快速打开本地预览首先是快速查看我的日记。目标是双击一下脚本文件就可以在浏览器中看到我的日记。 一般情况下的步骤： 在根目录打开命令行 输入hexo s 打开浏览器 在地址栏输入localhost:4000 我写出来的.bat文件是这样的： 12set browser="C:\Program Files (x86)\Tencent\QQBrowser\QQBrowser.exe"%browser% localhost:4000 &amp;&amp; hexo s 只有两行，第一行是设置用于打开日记本的浏览器所在的位置，当然，如果设置了环境变量，这里可以直接写浏览器的名字。 第二行是利用这个浏览器打开localhost:4000，打开成功才执行hexo s来启动hexo。 新建日记一般情况下的步骤： 在根目录打开命令行 输入hexo new &lt;title&gt; 打开source\_posts\ 找到并打开新建的日记 获取标题1set /p title=请输入标题: /p表示动态输入 创建文件1hexo new "%title%" %变量名%表示引用已经赋值的变量。 打开文件由于我设置的文件名不只是标题，因此还需要获取日期来组成文件名。 12345678set year=%date:~0,4%set month=%date:~5,2%set day=%date:~8,2%rem 在这里设置你的文件名格式set new_post_name=%year%-%month%-%day%-%title%"S:\Program Files\Typora\Typora.exe" source\_posts\%new_post_name%.md 其中： %date%是系统变量，用于获取系统时间，返回的值的格式是2019/06/26 周三 %date:~x,y%代表从第x个字符开始，获取y个字符 刚开始的脚本代码是这样的： 12345678910set /p title=请输入标题:hexo new "%title%"set year=%date:~0,4%set month=%date:~5,2%set day=%date:~8,2%rem 在这里设置你的文件名格式set new_post_name=%year%-%month%-%day%-%title%"S:\Program Files\Typora\Typora.exe" source\_posts\%new_post_name%.md 但是我发现在执行完hexo new &quot;%title%&quot;之后，命令行直接退出，加pause都没用。 猜测是因为，hexo创建文件需要时间，还没创建好就打开，于是出错了。 后来改成： 1hexo new "%title%" &amp;&amp; call z_open_editor.bat 在创建完之后，才会执行后面的内容，后面的代码都放在z_open_editor.bat里面 最终代码： 1234rem z_new_diary.bat@echo offset /p title=请输入标题:hexo new "%title%" &amp;&amp; call z_open_editor.bat 1234567891011121314151617rem z_open_editor.batrem 本文件只支持打开默认布局的文件@echo offset year=%date:~0,4%set month=%date:~5,2%set day=%date:~8,2%dir source\_postsif not defined title set /p title=请输入标题:rem 在这里设置你的文件名格式set new_post_name=%year%-%month%-%day%-%title%echo source\_posts\%new_post_name%.md"S:\Program Files\Typora\Typora.exe" source\_posts\%new_post_name%.md]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[练习利用Scrapy爬取b站排行榜]]></title>
    <url>%2Fpost%2FScrapy_spider_bilibiliRank%2F</url>
    <content type="text"><![CDATA[开始学python的Scrapy框架了，参考书是崔庆才的《python3网络爬虫开发实战》 跟着示例敲完之后，又试着按照一样的逻辑去爬取了B站排行榜的数据。 通过这个小项目学习使用Scrapy框架。 步骤新建项目首先新建一个名为practice的项目 1$scrapy startproject practice 这个项目的目录结构（省略init文件）： practice practice items.py middlewares.py pipelines.py settings.py scrapy.cfg 这一个项目里面的代码是整个项目的爬虫通用的。 新建Spider新建一个爬虫bilibiliRank 12$cd practice$scrapy genspider bilibiliRank 然后与在此目录下出现了一个spider文件夹，用于存放这个新的爬虫 spider bilibiliRank.py bilibiliRank.py中： 123456789import scrapyclass BilibilirankSpider(scrapy.Spider): name = 'bilibiliRank'#爬虫名字 allowed_domains = ['bilibili.com']#允许爬取的域名 start_urls = ['https://www.bilibili.com/ranking/']#初始url def parse(self, response): pass spider文件夹里面是用于爬取不同网站的爬虫，它继承自scrapy.Spider，scrapy的引擎Engine就是利用你写的爬虫里面的parse()方法来解析页面获取数据，可以在这个方法里面将数据以item的形式返回出去，给ItemPipeline继续处理。 创建Itemitems.py里面定义了不同的item，这些item都继承自scrapy.Item，文件生成的内容如下（无关注释已删去）： 123456import scrapyclass PracticeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() pass 在这里你可以照着它的模板新建一个类，也可以直接修改，总之只要符合要求就可以： 1234567import scrapyclass RankItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() num=scrapy.Field() title=scrapy.Field() 在这个Item子类当中，我新建了两个域，也可以说是字段。按照注释给出的格式来就可以了。 解析response适当简化的流程大概是：引擎利用爬虫的start_url发起请求，然后将得到的响应response作为参数传入爬虫的parse()方法中。parse()将解析出的数据装入Item并返回给引擎。 需要解析的html页面内容（只展示其中一个项的结构）： 12345678910111213141516171819202122232425&lt;ul class="rank-list"&gt; &lt;li class="rank-item"&gt; &lt;div class="num"&gt;1&lt;/div&gt; &lt;div class="content"&gt; &lt;div class="img"&gt; &lt;a href="//www.bilibili.com/video/av56121331/" target="_blank"&gt; &lt;div class="lazy-img cover"&gt; &lt;img alt="视频标题" src="图片url"&gt; &lt;/div&gt; &lt;/a&gt; &lt;div class="watch-later-trigger w-later"&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="info"&gt; &lt;a href="视频url" target="_blank" class="title"&gt;视频标题&lt;/a&gt;&lt;!----&gt; &lt;div class="detail"&gt;&lt;span class="data-box"&gt; &lt;i class="b-icon play"&gt;&lt;/i&gt;366.8万&lt;/span&gt; &lt;span class="data-box"&gt;&lt;i class="b-icon view"&gt;&lt;/i&gt;3.8万&lt;/span&gt; &lt;a target="_blank" href="视频url"&gt; &lt;span class="data-box"&gt; &lt;i class="b-icon author"&gt;&lt;/i&gt;作者名&lt;/span&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="pts"&gt; &lt;div&gt;3798978&lt;/div&gt; 综合得分 &lt;/div&gt;&lt;/div&gt;&lt;!----&gt;&lt;/div&gt; &lt;/li&gt;&lt;/ul&gt; 爬虫文件： 1234567891011121314151617import scrapyfrom practice.items import RankItem#这是之前自定义的itemclass BilibilirankSpider(scrapy.Spider): name = 'bilibiliRank' allowed_domains = ['bilibili.com'] start_urls = ['https://www.bilibili.com/ranking/'] def parse(self, response): #获取所有的项目 rank_items=response.css('.rank-list .rank-item') #获取每一项中的数据 for rank_item in rank_items: item=RankItem() item['num']=rank_item.css('.num::text').extract_first() item['title']=rank_item.css('.content .info .title::text').extract_first() yield item#每次调用就会返回一个item 遇到的问题： 注意获取的所有项目得是一个节点，不能用extract()读取其中的数据，第一次写时，写成了： 123rank_list=response.css('.rank-item').extract()for rank_item in rank_list: #…… 爬取1$scrapy crawl bilibiliRank -o bilibiliRank.json 利用名为bilibiliRank爬虫进行爬取，并将得到的结果保存在bilibiliRank.json文件中 参考链接 scrapy官方中文文档]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>framework</tag>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基于AWT的对战小游戏]]></title>
    <url>%2Fpost%2Fjava_game_FightFieldFrame%2F</url>
    <content type="text"><![CDATA[这学期的java课设弄完了，写个博客总结一下。 哔哩哔哩对应视频的传送门 课设目的与要求根据讲义中策略模式的案例，设计和实现一个基于策略模式的角色扮演游戏。其中包括主要有角色类及其子类、相关的行为类集合和测试类等。 通过本次实验，能够在掌握面向对象程序设计的基本思想基础上；深化理解 Java 面向对象程序设计中消息、继承、多态、接口、抽象类和抽象方法等概念和实现方式；并进一步掌握 Java 程序设计中的基本语法和 Java程序运行方法等；理解和应用包（package）。 内容一个游戏中有多种角色(Character)，例如：国王（King）、皇后（Queen）、骑士（Knight）、老怪（Troll）。角色之间可能要发生战斗(fight)，每场战斗都是一个角色与另一角色之间的一对一战 斗。 每个角色都有自己的生命值 (hitPoint) 、 魔法值（magicPoint）、攻击力值(damage)和防御力值(defense)。 每种角色都有一种武器进行攻击（fight）；在程序运行中，可以动态修改角色的武器(setWeaponBehavior)。 每种角色都有一种魔法对自己或者其他角色施法（performMagic）；可以动态改变拥有的魔法（setMagicBehavior）。 首先设计和实现抽象类 Characters。 设计和实现 Character 类的几个子类：King、Queen、Knight、Troll。位 设计接口 WeaponBehavior 和 MagicBehavior。 接 口 WeaponBehavior 的 实 现 类 ： KnifeBehavior （ 用 刀 ） BowAndArrowBehavior （ 用 弓 箭 ） AxeBehavior （ 用 斧 ） SwordBehavior（用剑） 接口MagicBehavior的实现类： HealBehavior（治疗） InvisibleBehavior（隐身）。 实现接口中的抽象方法，可以只在屏幕输出简单信息，也可以结合生命值(hitPoint)、攻击力值(damage)和防御力值(defense)计算。 编写测试代码，对以上设计的系统进行测试。要求在程序运行期间，能动态改变角色拥有的武器或者魔法。 自己添加一种角色、或者添加一种武器及魔法，设计相应的类，并编写测试代码进行测试。 按照 Java 的规范，添加详细的文档注释，并用 Javadoc 生成标准的帮助文档。 将上述编译、运行、生成帮助文档的命令，填写至实验报告相应位置。 填写实验报告。并将程序代码及生成的帮助文档打包上交。 涉及的主要内容 单例模式。游戏窗口只能有一个对象，因此使用了单例模式。 策略模式。在角色类中有两个抽象策略（武器策略和魔法策略），具体策略在类中实现。 双缓冲技术。在绘制游戏画面的时候使用了双缓冲技术，防止画面闪烁。 多线程。在两处使用了多线程，一处是为了解决按键冲突的问题，另一处是为了实现游戏周期性判定的功能。 awt。 基本逻辑流程 抽象角色类由具体子类实现，子类主要实现了抽象方法getAppearance，用于获取角色的外貌（即图片），外貌会根据角色状态的不同而改变，比如角色死亡时外貌是墓碑； 根据角色的坐标以及属性（例如是否隐身，当前武器是什么）来绘制角色以及属性条、武器栏和魔法栏。 游戏时钟周期线程用于周期性地执行一些操作，例如每秒钟恢复一定的HP和MP，对于隐身状态的角色，每秒钟扣除一定量的MP等。 游戏说明 玩家1操作：键盘上A键D键分别对应左右移动，J键使用武器攻击，K键使用魔法，L键切换武器，O键切换魔法； 玩家2操作：键盘上←键→键分别对应左右移动，小键盘上，1键使用武器攻击，2键使用魔法，3键切换武器，6键切换魔法； 每把武器有自己的攻击威力和攻击距离，只有在两个角色的距离在武器的攻击范围内时，才能够攻击成功； 伤害计算公式为：被攻击者受到的最终伤害=攻击者攻击力+攻击者武器威力-被攻击者的防御力。若伤害小于等于0，则不予扣除； 每秒钟会恢复一定量的HP和MP； 一方死亡（HP降为0及以下）则游戏结束。 设计与实现主要框架12345678910111213141516171819202122public class FightFieldFrame extends Frame&#123; //一些游戏常量以及窗口 public static final Dimension SCREEN_DIMENSION=Toolkit.getDefaultToolkit().getScreenSize(); public static final int FFF_X=0; public static final int FFF_Y=0; public static final int FFF_HEIGHT=SCREEN_DIMENSION.height; public static final int FFF_WIDTH=SCREEN_DIMENSION.width;//……省略其他成员函数，下面会列举来说明/*******************main函数**************************/ public static void main(String args[]) &#123; FightFieldFrame f=getInstance("战斗领域"); f.initFrame(); //初始化角色 f.initCharacter(); //添加事件监听者 f.addWindowListener(new MyWindowListener()); f.addKeyListener(new GamePad(player1,player2,f)); //新建时钟线程，用于游戏中的周期性属性检查 Thread clockThread=new Thread(new ClockThread(player1, player2, fff)); clockThread.start(); &#125;&#125; 单例模式FightFieldFrame 类中： 1234567891011//只能有一个窗体对象，使用单例模式private static FightFieldFrame fff;//单例模式使用的对象private FightFieldFrame(String title) &#123; super(title);&#125;public static FightFieldFrame getInstance(String title) &#123; if(fff==null) &#123; fff=new FightFieldFrame(title); &#125; return fff;&#125; 双缓冲双缓冲因为有两个绘图对象而得名，先在一个image对象上绘图然后再将此对象绘制到Frame上，用于减少重绘时的闪烁。 123456789101112/** * 初始化框架的位置和大小，以及缓冲对象 */public void initFrame() &#123; //这里准备一些对象构造完成之后才能做的事情 fff.setVisible(true); setBounds(FFF_X, FFF_Y, FFF_WIDTH, FFF_HEIGHT); Dimension d=getSize(); imgBuffer=createImage(d.width, d.height); gBuffer=imgBuffer.getGraphics();&#125; 创建好缓冲对象后，在缓冲对象上绘制： 123456789101112131415161718192021222324public void paint(Graphics g) &#123; //全都先绘制在缓冲区 //绘制背景 Image background=getToolkit().getImage("image\\background.jpg"); if(background!=null) &#123; gBuffer.drawImage(background, FFF_X, FFF_Y, FFF_WIDTH, FFF_HEIGHT, this); &#125; //绘制人物 if(player1!=null) &#123; drawCharacter(gBuffer,player1); drawStrand(gBuffer, player1); &#125; if(player2!=null) &#123; drawCharacter(gBuffer,player2); drawStrand(gBuffer, player2); &#125; drawSlot(gBuffer); //drawStrand(gBuffer);//绘制绝对位置的属性条，由于没有什么技术含量就只做了一个示例 //由于使用了背景图片，所以不必特地清空背景 g.drawImage(imgBuffer, 0, 0, this);&#125; 但是，即便如此仍然会闪烁，这是因为重绘时调用的update函数会将Frame用背景色填充一次 再绘制。所以应该覆盖掉原本的方法，让它只绘制，不清空： 123456//======================//public void update(Graphics g) &#123; //覆盖原本的方法 paint(g);&#125;//======================/*/ 玩家操纵使用GamePad类作为键盘监听者，监听Frame的按键，调用角色对应的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 游戏手柄类 * 用于将键位与角色的动作对应起来 */public class GamePad implements KeyListener&#123; private Characters player1;//玩家1 private Characters player2;//玩家2 private FightFieldFrame fff; public GamePad(Characters p1, Characters p2, FightFieldFrame f) &#123; // TODO Auto-generated constructor stub player1=p1; player2=p2; fff=f; &#125; @Override public void keyPressed(KeyEvent e) &#123; int code=e.getKeyCode(); switch (code) &#123; case KeyEvent.VK_J://玩家1攻击 player1.fight(player2); player2.display(); break; case KeyEvent.VK_K://玩家1使用魔法 player1.performMagic(player2); break; case KeyEvent.VK_A://玩家1左 player1.setMoveLeftFlag(true); player1.setDirection(true);//false为朝右，true为朝左 break; case KeyEvent.VK_D://玩家1右 player1.setMoveRightFlag(true); player1.setDirection(false); break; case KeyEvent.VK_L://玩家1切换武器 player1.changeWeapon(); break; case KeyEvent.VK_O://玩家1切换魔法 player1.changeMagic(); break; /****************************************************************/ case KeyEvent.VK_NUMPAD1://玩家2攻击 player2.fight(player1); player1.display(); break; case KeyEvent.VK_NUMPAD2://玩家2使用魔法 player2.performMagic(player1); break; case KeyEvent.VK_LEFT://玩家2左 player2.setMoveLeftFlag(true); player2.setDirection(true);//false为朝右，true为朝左 break; case KeyEvent.VK_RIGHT://玩家2右 player2.setMoveRightFlag(true); player2.setDirection(false); break; case KeyEvent.VK_NUMPAD3://玩家2切换武器 player2.changeWeapon(); break; case KeyEvent.VK_NUMPAD6://玩家2切换魔法 player2.changeMagic(); break; default: break; &#125; fff.repaint();//重绘 &#125; @Override public void keyReleased(KeyEvent e) &#123; int code=e.getKeyCode(); switch (code) &#123; case KeyEvent.VK_J: break; case KeyEvent.VK_A://左 player1.setMoveLeftFlag(false); break; case KeyEvent.VK_D://右 player1.setMoveRightFlag(false); break; case KeyEvent.VK_K: break; case KeyEvent.VK_NUMPAD1: break; case KeyEvent.VK_LEFT: player2.setMoveLeftFlag(false); break; case KeyEvent.VK_RIGHT: player2.setMoveRightFlag(false); break; default: break; &#125; fff.repaint();//重绘 &#125; @Override public void keyTyped(KeyEvent e) &#123;&#125;&#125; 注意，这里控制角色左右移动并不是直接调用角色的移动方法，而是更改角色移动的标志变量，利用线程来调用角色的移动方法。这样可以解决角色的按键冲突问题。 角色移动线程移动线程只负责发送消息给角色，而角色移动的具体判定由角色自身完成，从而更好地实现面向对象的思想。 1234567891011121314public class MoveThread implements Runnable&#123; private Characters character; public MoveThread(Characters c) &#123; character=c; &#125; public void run() &#123; while(true) &#123; //线程只负责发送消息，让角色自己判断移动 character.moveRight(); character.moveLeft(); &#125; &#125;&#125; 下面这是Characters类中的角色移动函数，添加了延时以免在按下移动按键的一瞬间，角色移动太快出了屏幕外面。 12345678910111213141516171819202122232425262728293031/** * 向左移动&lt;br/&gt; * 由于两个线程各自操作自己的角色，所以此函数不需要同步 */public void moveLeft() &#123; if(!isAliveFlag) return; if(moveLeftFlag) &#123; x-=1; try &#123; Thread.sleep(1);//防止跑得太快 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;/** * 向右移动&lt;br/&gt; * 由于两个线程各自操作自己的角色，所以此函数不需要同步 */public void moveRight() &#123; if(!isAliveFlag) return; if(moveRightFlag) &#123; x+=1; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 武器攻击实现机制在Characters类中，使用武器进行攻击的方法如下，它的主要逻辑是调用useWeapon方法： 1234567891011121314151617181920212223/** * 攻击某个角色 * @param c 要攻击的角色 * @return 造成的真实伤害 */public int fight(Characters c) &#123; //由于武器有不同的特性，所以伤害的逻辑让武器实现 //比如后期编写高级玩法时，弓需要计算射程 if(!isAliveFlag) return 0;//如果已死亡，直接返回，下同 if(weapon==null) &#123; System.out.println(name+"没有武器，无法攻击"); return 0; &#125; int attackRange=weapon.getAttackRange(); if(attackRange&gt;distance(c)) &#123; //攻击距离大于角色之间的距离才可攻击 return weapon.useWeapon(this,c);//此角色攻击角色c &#125; else &#123; return 0; &#125; &#125; 角色类的两个属性，武器和魔法，使用的都是对应接口的引用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152protected WeaponBehavior weapon;//武器protected MagicBehavior magic;//魔法以下是武器接口：public interface WeaponBehavior &#123; /** * 使用武器 * @param attacker 武器持有者 * @param victim 被攻击者 * @return 造成的真实伤害 */ public int useWeapon(Characters attacker,Characters victim);//使用武器 public String getName(); public int getAttackRange(); public Image getAppearance();&#125;以下是具体的武器实现（以剑为例，其他大同小异）：/** * 剑 * 实现武器接口 * 威力中等，攻击距离中等 */public class SwordBehavior implements WeaponBehavior &#123; private String name="剑"; private Image appearance; private static final int DAMAGE=6;//武器基础威力 private static final int ATTACK_RANGE=200;//武器攻击距离,单位px private static final String APPEARANCE_PATH="image\\Weapon\\Sword.png"; public SwordBehavior() &#123;&#125; public SwordBehavior(String _name) &#123; name=_name;//剑，岂能无名OVO &#125; /** * 使用武器攻击 * @param attacker 攻击者 * @param victim 被攻击者 */ @Override public int useWeapon(Characters attacker,Characters victim) &#123; int attackDamage=DAMAGE+attacker.getDamage();//造成的伤害为攻击者的伤害加上武器威力 int finalDamage=victim.hitBy(attacker, attackDamage); System.out.println(attacker.getName()+"使用"+name+"对"+victim.getName()+"造成了"+finalDamage+"点伤害"); return finalDamage;//返回最终伤害 &#125; public String getName() &#123;return name;&#125; public int getAttackRange() &#123;return ATTACK_RANGE;&#125; public Image getAppearance() &#123; appearance=Toolkit.getDefaultToolkit().getImage(APPEARANCE_PATH); return appearance; &#125;&#125; 这里面主要的代码是useWeapon方法里面调用的角色类的hitBy方法，里面有着伤害计算逻辑： 123456789101112131415/** * 被某个角色攻击 * @param attacker 攻击者 * @param attackDamage 攻击者给予的攻击伤害 * @return 最后造成的真实伤害 */public int hitBy(Characters attacker,int attackDamage) &#123;//被攻击 if(!isAliveFlag) return 0; int finalDamage=(attackDamage-defense);//伤害计算：最终伤害=敌方攻击伤害-我方防御力 if(incHP(-finalDamage)==-1) &#123;//如果血量被扣到负数 this.killedBy(attacker); &#125; return finalDamage;//返回最后造成的真实伤害&#125; 魔法的实现机制大同小异，不做特殊说明。 武器切换和魔法切换实现方法是在角色类里面声明数组： 1234protected WeaponBehavior weaponSlots[];//武器栏位，用于存储角色携带的武器protected MagicBehavior magicSlots[];//魔法栏位protected int weaponSlotsIndex=0;//栏位索引，指示当前武器protected int magicSlotsIndex=0; 以切换武器为例，如果当前武器是最后一把，那么换回第一把，否则索引自增： 123456789101112/** * 按顺序切换武器 */public void changeWeapon() &#123; setWeaponBehavior(weaponSlots[weaponSlotsIndex]); if(weaponSlotsIndex+1&gt;=weaponSlots.length) &#123; weaponSlotsIndex=0; &#125; else &#123; weaponSlotsIndex++; &#125;&#125; 时钟线程时钟线程用于进行一些游戏周期性方法的调用，比如周期性恢复HP，对角色属性值的判断等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 时钟线程，用于一些周期性的计算 */public class ClockThread implements Runnable&#123; private Characters player1;//玩家1 private Characters player2;//玩家2 private FightFieldFrame fff; private int interval=1000;//时钟周期 public ClockThread(Characters p1, Characters p2, FightFieldFrame f) &#123; player1=p1; player2=p2; fff=f; &#125; /** * 核对属性，并对于特定属性作出不同的事情 * @param c 核对角色c的属性 */ public void cheackStatus(Characters c) &#123; switch (c.getStatus()) &#123; case Characters.ST_INVISIBLE://隐身魔法每个周期扣除一定的魔力 if(c.incMP(-InvisibleBehavior.COST)==-1) &#123;//如果魔力不够 c.setStatus(Characters.ST_NORMAL); &#125; break; default: break; &#125; &#125; /** * 周期性恢复属性值（回血回魔） * @param c 周期性恢复角色c的HP和MP */ public void recover(Characters c) &#123; if(!c.getIsAliveFlag()) return;//角色死亡就不再回血 c.incHP(Characters.HP_RECOVER); c.incMP(Characters.MP_RECOVER); &#125; public void run() &#123; while(true) &#123; //做这个周期要做的事情 cheackStatus(player1); cheackStatus(player2); recover(player1); recover(player2); fff.repaint(); //等待下一个周期 try &#123; Thread.sleep(interval); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 参考链接 java.awt.Image官方文档 使用eclipse生成javadoc-博客园 java双缓冲技术-CSDN java获取屏幕大小-CSDN java线程传参三种方式-脚本之家 游戏角色移动流畅度的处理-ITeye eclipse调试方式和快捷键-CSDN 一个讲eclipse调试的b站视频（靠这个视频解决了调试问题）-bilibili 绘制字体修改-CSDN 体会这次课设对我来说是个挑战，首先时间比较紧张，和考试放在了一周，并且用的是学了几周还没私底下练习多少的JAVA。不过还是做的让我自己比较满意。 我选择的是看上去较为简单的一道题目，虽然简单，但是这个题目的可扩展性很强，可以尽情开脑洞，我看中的就是这一点。我在高中的时候就尝试使用Visual Basic来编写类似的小游戏，一些可能会遇到的困难在那时已经思考过了，所以总体来说没有遇到太过麻烦的地方。 随着经验的增长，我逐渐开始一边编程一边整理，让以后的自己也能够回顾这一次的项目。在写完这个课设之后，我用录屏软件录制了一个视频来整体讲述我编写过程中的思路，并上传到了Bilibili弹幕视频网站，总结经验，分享思路，以及为了便于以后回顾。地址是（https://www.bilibili.com/video/av54526303/） 当然，过程中也遇到了一些问题。 比如绘制图片的时候遇到了只能使用绝对路径的问题，在老师上课演示的过程中也遇到过这个问题，后来我知道了JAVA相对路径是以项目根目录为基准而不是以文件目录为基准的。 比如角色控制按键冲突。解决方法是使用多线程，两个线程控制分别控制两个角色。 比如游戏周期性事件。在以前我使用Visual Basic的时候，是利用时钟控件来解决这个问题的，而JAVA里面可以使用线程来模拟那个时钟控件。这让我对时钟控件的原理有了比较好的认识。 在假期里面，我可能会通过继续完善这个小游戏，来更加深入地学习JAVA。]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记5爬虫类结构优化]]></title>
    <url>%2Fpost%2Fpython_spider_note5optimization_of_the_spider_class%2F</url>
    <content type="text"><![CDATA[打算全部以cookie来登陆，而不依赖于session（因为听组长说session没cookie快，而且我想学些新东西而不是翻来覆去地在舒适区鼓捣）。弄了几天终于弄出来个代码不那么混乱的爬虫类了，更新一下博文来总结一下。代码在我github的spider库里面。 初步思路既然要封装成爬虫类，那么就以面向对象的思维来思考一下结构。 从通用的爬虫开始，先不考虑如何爬取特定的网站。 以下只是刚开始的思路，并不是最终思路。 爬虫的行为步骤并不复杂，分为以下几步： 请求并获取网页（往往需要模拟登录） 解析网页提取内容（还需要先获取需要爬取的url） 保存内容（保存到数据库） 爬虫类方法（初步设计）： 方法 说明 login 登录 parse 解析 save 保存 crawl 爬取（外部调用者只需调用这个方法即可） 爬虫类属性（初步设计）： 属性 说明 headers 请求的头部信息，用于伪装成浏览器 cookies 保存登录后得到的cookies db_data 数据库的信息，用于连接数据库 进一步设计我想将这个爬虫类设计得更为通用，也就是只修改解析的部分就能爬取不同的网站。组长说我这是打算写一个爬虫框架，我可没那么厉害，只是觉得把逻辑写死不能通用的类根本不能叫做类罢了。 参考代码我看了一下组长给出的参考代码，大致结构是这样的： 首先一个Parse解析类（为了关注结构，具体内容省略）： 12345678910111213141516171819202122232425262728293031323334353637383940class Parse(): def parse_index(self,text): ''' 用于解析首页 :param text: 抓取到的文本 :return: cpatcha_url, 一个由元组构成的列表(元组由两个元素组成 (代号，学校名称)) ''' pass def parse_captcha(self, content, client): ''' 解析验证码 :return: &lt;int&gt; or &lt;str&gt; a code ''' pass def parse_info(self, text): ''' 解析出基本信息 :param text: :return: ''' pass def parse_current_record(self, text): ''' 解析消费记录 :param text: :return: ''' return self.parse_info(text) def parse_history_record(self, text): ''' 解析历史消费记录 :param text: :return: ''' return self.parse_info(text) 这个思路不错，将解析部分独立形成一个类，不过这样要如何与爬虫类进行逻辑上的关联呢？解析类的对象，是什么？是解析器吗？解析器与爬虫应该是什么关系呢？ 我继续往下看： 12345678910111213141516class Prepare(): def login_data(self,username, password, captcha, schoolcode, signtype): ''' 构造登陆使用的参数 :return:data ''' pass#省略代码，下同 def history_record_data(self, beginTime, endTime): ''' 历史消费记录data :param beginTime: :param endTime: :return: data ''' pass 这是一个Prepare类，准备类？准备登录用的数据。说起来似乎比解析类更难以让我接受。解析器还可以说是装在爬虫身上，但是，但是“准备”这件事情分明是一个动作啊喂！ 好吧，“一类动作”倒能说得过去吧。我看看怎么和爬虫类联系起来： 12class Spider(Parse, Prepare):#??? pass 等会儿等会儿…… 继承关系？ 让我捋捋。 为了让爬虫能解析和能准备还真是不按套路出牌啊…… 子类应该是父类的特化吧不是吗，就像猫类继承动物类，汽车类继承车类一样，猫是动物，汽车也是车。 算了不继续了，毕竟我不是为了故意和我组长作对。只是将其作为一个例子来说明我的思路。 解析器类参考代码虽然不太能让我接受，但是它的结构仍然带给了我一定启发。就是解析函数不一定要作为爬虫的方法。 解析这个步骤如果真的只写在一个函数里面真的非常非常乱，因为解析不只一个函数。比如解析表单的隐藏域，解析页面的url，解析页面内容等。 单独写一个解析类也可以。至于它和爬虫类的关系，我觉得组合关系更为合适（想象出了一只蜘蛛身上背着一个红外透视仪的样子），spider的解析器可以更换，这样子我觉着更符合逻辑一些。 关于更换解析器的方式，我打算先写一个通用的解析器类作为基类，而后派生出子解析器类，子解析器根据不同的网站采取不同的解析行为。 然后新建my_parser.py文件，写了一个MyParser类。解析方式是xpath和beautifulsoup。这里面的代码是我把已经用于爬取学校网站的特定代码通用化之后的示例代码，实际上并不会被调用，只是统一接口，用的时候会新写一个类继承它，并覆盖里面的函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class MyParser(object): def login_data_parser(self,login_url): ''' This parser is for chd :param url: the url you want to login :return (a dict with login data,cookies) ''' response=requests.get(login_url) html=response.text # parse the html soup=BeautifulSoup(html,'lxml') #insert parser,following is an example example_data=soup.find('input',&#123;'name': 'example_data'&#125;)['value'] login_data=&#123; 'example_data':example_data &#125; return login_data,response.cookies def uni_parser(self,url,xpath,**kwargs): response=requests.post(url,**kwargs) html=response.text tree=etree.HTML(html) result_list=tree.xpath(xpath) return result_list def get_urls(self,catalogue_url,**kwargs): ''' get all urls that needs to crawl. ''' #prepare base_url='http://example.cn/' cata_base_url=catalogue_url.split('?')[0] para = &#123; 'pageIndex': 1 &#125; #get the number of pages xpath='//*[@id="page_num"]/text()' page_num=int(self.uni_parser(cata_base_url,xpath,params=para,**kwargs)) #repeat get single catalogue's urls xpath='//a/@href'#link tag's xpath url_list=[] for i in range(1,page_num+1): para['pageIndex'] = i #get single catalogue's urls urls=self.uni_parser(cata_base_url,xpath,params=para,**kwargs) for url in urls: url_list.append(base_url+str(url)) return url_list def get_content(self,url,**kwargs): ''' get content from the parameter "url" ''' html=requests.post(url,**kwargs).text soup=BeautifulSoup(html,'lxml') content=soup.find('div',id='content') content=str(content) return content 我把构造登录信息的部分放在了解析器中。并在登录中调用。 登录之后得到的cookies就在参数中传递。 数据库类由于只打算存到数据库，所以并没有写一个“存档宝石类“，或许之后会写。 目前我只写了一个保存函数，以及自己封装的一个数据库类。 这个数据库类是my_database.py中的MyDatabase（应该不会撞名吧），目前只封装了insert函数，传入的参数有三个：数据库名，表名，装有记录的字典。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import pymysqlclass MyDatabase(object): def __init__(self,*args,**kwargs): self.conn=pymysql.connect(*args,**kwargs) self.cursor=self.conn.cursor() def insert(self,db,table,record_dict): ''' :param db:name of database that you want to use :param table:name of table that you want to use :param record_dict:key for column,value for value ''' #1.use the database sql='use &#123;&#125;'.format(db) self.cursor.execute(sql) self.conn.commit() #2.connect the sql commend sql='insert into &#123;&#125;('.format(table) record_list=list(record_dict.items()) for r in record_list: sql += str(r[0]) if r != record_list[-1]: sql += ',' sql+=') values(' for r in record_list: sql += '"' sql += str(r[1]) sql += '"' if r != record_list[-1]: sql += ',' sql+=')' #3.commit self.cursor.execute(sql) self.conn.commit() def show(self): pass def __del__(self): self.cursor.close() self.conn.close()if __name__ == "__main__": db_data=&#123; 'host':'127.0.0.1', 'user':'root', 'passwd':'password', 'port':3306, 'charset':'utf8' &#125; test_record=&#123; 'idnew_table':'233' &#125; mydb=MyDatabase(**db_data) mydb.insert('news','new_table',test_record) 封装之后用起来比较方便。 save函数123456def save(content,**save_params): mydb=MyDatabase(**save_params) record=&#123; 'content':pymysql.escape_string(content) &#125; mydb.insert('dbase','bulletin',record) pymysql.escape_string()函数是用于将内容转义的，因为爬取的是html代码（就不解析那么细了，直接把那一块html代码全部存下来，打开的时候格式还不会乱），有些内容可能使组合成的sql语句无法执行。 爬虫类给构造函数传入特定的解析器和保存函数，然后调用crawl方法就可以让spider背着特制的parser去爬取网站内容啦~ 登录函数和上次不太一样，做了一些修改，不过主要功能仍然是获取登录之后的cookies的。 简单说一下修改：我们学校网站登录之后会从登陆页面开始，经过三四次跳转之后才到达首页，期间获取到的cookies都需要保留，这样才能利用这些cookies来进入新闻公告页面。于是禁止重定向，手动获取下一个url，得到这一站的cookies之后再手动跳转，直到跳转到首页。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import requestsclass MySpider(object): def __init__(self,parser,save,**save_params): self.parser=parser#parser is a object of class self.save=save#save is a function self.save_params=save_params self.cookies=None self.headers=&#123; "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36" &#125; def login(self,login_url,home_page_url): ''' login :param login_url: the url you want to login :param login_data_parser: a callback function to get the login_data you need when you login,return (login_data,response.cookies) :param target_url: Used to determine if you have logged in successfully :return: response of login ''' login_data=None #get the login data login_data,cookies=self.parser.login_data_parser(login_url) #login without redirecting response=requests.post(login_url,headers=self.headers,data=login_data,cookies=cookies,allow_redirects=False) cookies_num=1 while(home_page_url!=None and response.url!=home_page_url):#if spider is not reach the target page print('[spider]: I am at the "&#123;&#125;" now'.format(response.url)) print('[spider]: I have got a cookie!Its content is that \n"&#123;&#125;"'.format(response.cookies)) #merge the two cookies cookies=dict(cookies,**response.cookies) cookies=requests.utils.cookiejar_from_dict(cookies) cookies_num+=1 print('[spider]: Now I have &#123;&#125; cookies!'.format(cookies_num)) next_station=response.headers['Location'] print('[spider]: Then I will go to the page whose url is "&#123;&#125;"'.format(next_station)) response=requests.post(next_station,headers=self.headers,cookies=cookies,allow_redirects=False) cookies=dict(cookies,**response.cookies) cookies=requests.utils.cookiejar_from_dict(cookies) cookies_num+=1 if(home_page_url!=None and response.url==home_page_url): print("login successfully") self.cookies=cookies return response def crawl(self,login_url,home_page_url,catalogue_url): self.login(login_url,home_page_url) url_list=self.parser.get_urls(catalogue_url,cookies=self.cookies,headers=self.headers) for url in url_list: content=self.parser.get_content(url,cookies=self.cookies,headers=self.headers) self.save(content,**self.save_params) def __del__(self): pass 调用为了更好地展示结构，大部分内容都pass省略掉。想看具体代码可以去我github的spider库 这个文件内首先创建了一个特定解析类，继承自通用解析类，再写了一个保存函数，准备好参数，最后爬取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from my_spider import MySpiderfrom my_parser import MyParserfrom my_database import MyDatabasefrom bs4 import BeautifulSoupimport requestsimport pymysqlclass chdParser(MyParser): def login_data_parser(self,login_url): ''' This parser is for chd :param url: the url you want to login :return (a dict with login data,cookies) ''' pass return login_data,response.cookies def get_urls(self,catalogue_url,**kwargs): ''' get all urls that needs to crawl. ''' #prepare pass #get page number pass #repeat get single catalogue's urls pass for i in range(1,page_num+1): para['pageIndex'] = i #get single catalogue's urls pass return url_list def save(content,**save_params): passif __name__ == '__main__': login_url="pass"#省略 home_page_url="pass" catalogue_url="pass" parser=chdParser() save_params=&#123; 'host':'127.0.0.1', 'user':'root', 'passwd':'password', 'port':3306, 'charset':'utf8' &#125; sp=MySpider(parser,save,**save_params) sp.crawl(login_url,home_page_url,catalogue_url)]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记4模拟登录函数的优化]]></title>
    <url>%2Fpost%2Fpython_spider_note4optimization_of_the_login_function%2F</url>
    <content type="text"><![CDATA[前面写的代码虽然完成了爬取的功能，但是过于凌乱，于是打算重构一遍。首先从登陆开始 改进前的代码面向过程这是第一次写的登录函数，获取登录信息和登录本身是放在一起的。 1234567891011121314151617181920212223242526272829303132333435363738def login(): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ #设置 login_url = 'http://ids.chd.edu.cn/authserver/login?service=http%3A%2F%2Fportal.chd.edu.cn%2F' headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36', &#125; #新建会话 session=requests.session() #获取登录校验码 html=session.post(login_url,headers=headers).text soup=BeautifulSoup(html,'lxml') lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; #登录 response=session.post(login_url,headers=headers,data=login_data) if response.url=='http://portal.chd.edu.cn/': print('登录成功！') return session 面向对象第二次是将全部函数封装到类中，这次将获取登录信息从其中分出来。但是两者关系仍然太过于紧密。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class spider: ''' 爬虫类 ''' def __init__(self,headers): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 self.headers=headers#头信息 self.cookiejar=http.cookiejar.LWPCookieJar('cookie.txt') def get_login_data(self,login_url): ''' 获取登录需要的数据 :param login_url: 登录页面url :return: 一个存有登录数据的字典 ''' # 获取登录校验码 html = self.session.post(login_url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') lt = soup.find('input', &#123;'name': 'lt'&#125;)['value'] dllt = soup.find('input', &#123;'name': 'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data = &#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn': '', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ if self.load_cookie(): self.is_login = True else: #获取登录信息 login_data=self.get_login_data(login_url) # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url: print("登录成功") self.is_login=True self.save_cookie() else: print("登录失败") return self.session #省略后面的函数 改进这次改进，我打算让login()函数与获取登录信息用的函数关系没有那么紧密，让后者可以被替换或者不用。 所以使用了回调函数，也就是将函数指针作为参数传入，不过python变量本身就像指针一样，直接传变量即可。 函数头1def login(self,login_url,login_data_parser=None,target_url=None): 传入了三个参数， login_url : 显而易见，这是登录页面的url login_data_parser : 这是一个函数，用于解析页面中随机生成的隐藏域代码的函数，可以不传入 target_url : 用于判断是否登录成功，这是登录之后会跳转到的页面 获取登录信息接着判断参数是否为函数（是否可调用），如果可以调用，就调用它获取登录信息。在这里不需要关心函数内部具体如何获取，而只用关心它的接口。 这个函数的返回值是一个装有登录信息的dict，和一个cookies。 12345def login(self,login_url,login_data_parser=None,target_url=None): login_data=None #get the login data if(login_data_parser!=None and callable(login_data_parser)): login_data,cookies=login_data_parser(login_url) 登录然后就完成了 1234567891011121314151617181920212223def login(self,login_url,login_data_parser=None,target_url=None): ''' login :param login_url: the url you want to login :param login_data_parser: a callback function to get the login_data you need when you login,return (login_data,response.cookies) :param target_url: Used to determine if you have logged in successfully :return: response of login ''' login_data=None #get the login data if(login_data_parser!=None and callable(login_data_parser)): login_data,cookies=login_data_parser(login_url) #login response=requests.post(login_url,headers=self.headers,data=login_data,cookies=cookies) if(target_url!=None and response.url==target_url): print("login successfully") self.cookies=cookies return response 获取登录信息函数这个和前面就是一样的了。只要修改传给login函数的函数，就可以获取不同网站的登录信息。login函数变得更加通用了，不再过于依赖登录信息函数存在。 1234567891011121314151617181920212223242526def chd_login_data_parser(self,url): ''' This parser is for chd :param url: the url you want to login :return (a dict with login data,cookies) ''' response=requests.get(login_url) html=response.text # parse the html soup=BeautifulSoup(html,'lxml') lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': input('input account:'), 'password': input('input passwd:'), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data,response.cookies]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git恢复误提交的内容]]></title>
    <url>%2Fpost%2Fgit_reset_incorrect_commit%2F</url>
    <content type="text"><![CDATA[在图书馆敲下最后几行代码，然后就着手机热点把爬虫代码push上去之后，突然想起来，我好像忘了把账号密码部分改成手动输入，现在push上去的是明文啊！掀桌！早知道就回宿舍上传了，说不准还能想起来。 问题及其解决方案已经上传了，即便我再改回来上传，别人也可以从git log里面看到我的账号密码。 那就版本回退，重新更新再上传。但是在我使用GitHub Desktop的Revert this commit的时候它却让我解决一大堆冲突……等会儿，啥时候多出来那么多“changes”？？？刚刚还一个都没有啊，怎么我用了这个选项还没回退就出现一大堆冲突？ 我对git其实不熟练，用GUI界面也是，解决这些冲突比较麻烦。所以最后的解决方案比较粗暴： 删除本地库以解决那一大堆的冲突文件 从远程库clone回来 把库内文件全部打包复制在别的路径 在库里面打开git bash，使用git reset --hard 版本号回到没出事的版本 将前面备份的文件复制回来 修改之后重新提交，完成 教训 得多准备一条分支，别直接在主分支上边写 一定要注意代码中的隐私信息！]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手工归档编程项目]]></title>
    <url>%2Fpost%2Farchive_project%2F</url>
    <content type="text"><![CDATA[以前写代码建立的工程到处堆放，导致不能很好的找到以前的代码。虽然以前简单地划分了一下文件夹，但并没有投入太多精力去想如何分类。所以打算养成归档编程项目的好习惯，记录一下过程。也给读者们一个参考。 不放图了，文件树结构就用无序列表来显示。 分类整理首先把所有项目文件夹全部放进一个专门的文件夹里面，最好不要中文名，也不要拼音，这是个好习惯，以后的命名也是。我将它起名为DEVELOP。 将它放置在F盘（我拿F盘当文件盘），并且设置一个快捷方式在桌面，嘿嘿我还给快捷方式选了一个很炫酷的图标让自己开心一下。 然后根据语言将其分为cpp_develop，py_develop，vb_develop，web_develop等（html，css，js等统一划分到web_develop里面，因为我个人觉得它们三个分不开） 在每一个语言文件夹里面再细分(用cpp举例) 文件夹名 内容 cpp_archive 用于归档已经完成的项目，方便以后查找 cpp_project 用于存放正在开发的项目 cpp_test 用于测试。这里面我建立了几个空项目用于在别人问我代码问题的时候测试 cpp_example 用于存放从各种渠道得到的源代码，用于研究学习，里面的代码是别人的 cpp_lessonwork 用于存放课设或者课程实验代码，可并入cpp_project cpp_pratice 用于存放一些不足以称为项目的代码 现在的目录大概是这样的： DEVELOP cpp_develop cpp_archive cpp_project cpp_test cpp_example cpp_lessonwork cpp_pratice py_develop vb_develop web_develop 归档规则项目名称+六位数日期(附加信息) 比如： 词法分析代码高亮190403(修复了xx) 日期是为了手动版本控制，利用肉眼就能知道哪些信息。以前做课设的时候就这样弄的（不过队友都不配合我这样搞，我发的版本是多少，改了之后发过来还是多少……） 总之归档时保证下次打开这个项目时能够唤醒当时编写时的记忆即可。]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简易倒排索引]]></title>
    <url>%2Fpost%2Fsimple_inverted_index%2F</url>
    <content type="text"><![CDATA[智能信息检索这门课程有个上机作业，题目是“实现倒排索引”。 用到了以前没有学的STL中的vector。 经过两次课上写代码（3小时）加上课后修bug的时间（晚上十点到十二点）总共5个小时，终于完成了一个简易的倒排索引。因为十点时已经太困，喝了柠檬茶提神结果现在睡不着，所以继续熬夜把博客写完吧。 前言勿抄袭代码，代码仅供参考。转载注明出处 倒排索引简介为了从文档集（collection）中检索出想要的结果，首先要将文档集中的每个词项（term）建立索引，以确定词项所在的文档（document）的id，从而返回根据关键字查询的结果。 倒排索引的格式大概是下图这样（代码成果图）： 每一个词项后面跟着它在文档集中出现的次数，以及出现过的文档的id所组成的一个序列。 例如第一条： 词项 词频 倒排记录表 API 6 4，5，6 就代表API这个词在文档集（六个文件）中出现了六次，这六次分布在文档4、文档5和文档6。 搜索引擎大致就是这个原理，建立好了索引之后，只需要把你搜索的关键词对应的posting求交集然后把对应的文档显示出来就可以了。 数据结构设计文档（Document）文档其实在这里就是文件，对于每个文档，都有一个文档名，以及相对应的文档ID，它们得绑定好，否则会混乱。因此将它们放在一个结构体里面。 12345struct Document&#123; string docName;//文档名 int docID;//文档id&#125;; 索引项（IndexItem）同样的，每一个记录的词项、词频和记录表也是绑定的，所以也打包起来。文档id的数目不定，又不想自己写链表或者动态数组怕出错，因此采用了STL（标准模板库）里面的动态数组vector（向量容器）。 123456struct IndexItem&#123; string term;//词项 int frequence;//词频 vector&lt;int&gt; posting;//记录表&#125;; 索引类（CIndex）代码应该不难看懂。 12345678910111213141516171819202122class CIndex&#123; vector&lt;IndexItem&gt; indexList;//索引表 vector&lt;Document&gt; collection;//文档集public: CIndex(); //利用文件名数组初始化文档集 CIndex(string p_collection[], int n); //显示文档集内所有文档的文件名 void showCollection(); //显示当前倒排索引表 void showIndexList(); //索引单篇文档 int indexDocument(FILE*fp, int docID); //索引文档集 int indexCollection(); //排序索引表 int sortIndex(); //索引表合并同类项 int mergeIndex(); ~CIndex();&#125;; 大致思路 扫描一篇文档，将这篇文档对应的文档ID加入对应词项的posting 对文档集中每一篇文档重复第一步，获取所有词项及其对应的posting加入索引表，此时每个词项的posting中只有一个文档ID，并且有很多重复的词项记录； 排序索引表； 将重复的项的posting合并，并且增加词频，删除重复项。 2019-4-4补充：想到一个新思路——直接按照ID从小到大扫描一遍整个文档集，每扫描一个词项，就在词典中查找这个词项，增加词频，然后把现在正在处理的文档的ID加入到posting，最后再排个序即可。 代码实现有参构造函数初始化文档集 12345678910111213//@name &lt;CIndex::CIndex&gt;//@brief &lt;初始化文档集&gt;//@param &lt;string p_collection[]:文档文件名数组&gt;&lt;int n:数组长度&gt;CIndex::CIndex(string p_collection[], int n)&#123; Document nextDoc; for (int i = 0; i &lt; n; i++) &#123; nextDoc.docName = p_collection[i]; nextDoc.docID = i+1;//编号从1开始 collection.push_back(nextDoc); &#125;&#125; 索引单篇文档大致思路是，一个个字符读取进来，如果是字母就一直读完整个单词，并把这个单词作为词项加入表中。 12345678910111213141516171819202122232425262728293031323334353637//@name &lt;CIndex::indexDocument&gt;//@brief &lt;索引单篇文档&gt;//@param &lt;FILE * fp:已打开的文件指针&gt;&lt;int docID:此文件的编号&gt;//@return &lt;扫描到的词项数量&gt;int CIndex::indexDocument(FILE * fp, int docID)&#123; char ch;//扫描用的变量 IndexItem indexItem;//打包用的变量 int num = 0;//扫描到的词项数量 while (!feof(fp)) &#123;//一次循环获取一个单词 //找到第一个字母 do &#123; ch = fgetc(fp); if (feof(fp)) break;//防止空文件导致的无限循环 &#125; while (!isalpha(ch)); if (feof(fp)) break;//防止因文件后面的空行而索引空字符串 //读取单词，给索引项赋值 while (isalpha(ch)) &#123; indexItem.term += ch; ch = fgetc(fp); &#125; indexItem.frequence = 1; indexItem.posting.push_back(docID);//将本文件的文档ID加入posting //把索引项加入词典 indexList.push_back(indexItem); num++; //清空索引项，准备下一次 indexItem.term=""; indexItem.posting.clear(); &#125; return num;&#125; 索引文档集索引文档弄好之后，索引整个文档集不过是加个循环而已 123456789101112131415161718//@name &lt;CIndex::indexCollection&gt;//@brief &lt;索引文档集&gt;//@return &lt;词项总数目&gt;int CIndex::indexCollection()&#123; int num = 0; //打开对应的文件并索引 for (int i = 0; i &lt; collection.size(); i++) &#123; //打开文件 FILE* fp = fopen(collection[i].docName.c_str(), "r"); //索引单篇文档 num+=indexDocument(fp, collection[i].docID); //关闭文件 fclose(fp); &#125; return num;&#125; 排序索引表直接使用&lt;algorithm&gt;头文件里面的sort()函数进行排序，自定义比较函数cmp() 123456789bool cmp(IndexItem a, IndexItem b)&#123; return a.term&lt;b.term;//词项按照从小到大排序&#125;int CIndex::sortIndex()&#123; sort(indexList.begin(), indexList.end(), cmp); return 0;&#125; 去重12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//@name &lt;CIndex::mergeIndex&gt;//@brief &lt;索引表合并同类项&gt;int CIndex::mergeIndex()&#123; IndexItem item1,item2; sortIndex(); vector&lt;IndexItem&gt;::iterator it_cur=indexList.begin();//创建迭代器 vector&lt;IndexItem&gt;::iterator it_next = it_cur + 1; vector&lt;int&gt; temp; vector&lt;int&gt;::iterator p1, p2;//用于合并posting的迭代器 while (it_cur != indexList.end()) &#123; if(it_cur+1!=indexList.end()) it_next = it_cur + 1; else break; while((*it_cur).term == (*it_next).term) &#123;//这个循环内处理掉所有与当前词项重复的词项 //将二者的posting排序 sort((*it_cur).posting.begin(), (*it_cur).posting.end()); sort((*it_next).posting.begin(), (*it_next).posting.end()); //有序合并两者的posting p1 = (*it_cur).posting.begin(); p2 = (*it_next).posting.begin(); while (p1 != (*it_cur).posting.end() &amp;&amp; p2 != (*it_next).posting.end()) &#123; if ((*p1) &lt; (*p2))//结果集中加入较小的元素 &#123; temp.push_back(*p1); //这个while用于跳过重复的元素 p1++; &#125; else if((*p1) &gt; (*p2)) &#123; temp.push_back(*p2); p2++; &#125; else &#123; temp.push_back(*p1); //遇到相同的则两个都后移，避免出现重复 p1++; p2++; &#125; &#125; while(p1 != (*it_cur).posting.end())//如果串1没有合并完则将串1后面部分直接复制 &#123; temp.push_back(*p1); p1++; &#125; while(p2 != (*it_next).posting.end()) &#123; temp.push_back(*p2); p2++; &#125; //删除结果集重复部分 temp.erase(unique(temp.begin(), temp.end()), temp.end()); (*it_cur).frequence++;//词频增加 (*it_cur).posting.assign(temp.begin(), temp.end());//将结果复制 indexList.erase(it_next);//删除重复项 temp.clear(); if (it_cur + 1 != indexList.end()) it_next = it_cur + 1; else break; &#125; it_cur++; &#125; /*失败代码 for (int i = 0; i &lt; indexList.size()-1; i++) &#123; item1 = indexList[i]; item2 = indexList[i + 1]; int j = 1;//j是相对于item1的偏移量 while (item1.term == item2.term) &#123; vector&lt;int&gt; temp(item1.posting.size()+item2.posting.size()); sort(item1.posting.begin(), item1.posting.end()); sort(item2.posting.begin(), item2.posting.end()); merge(item1.posting.begin(), item1.posting.end(), item2.posting.begin(), item2.posting.end(), temp.begin()); indexList[i].posting.assign(temp.begin(), temp.end()); indexList.erase(indexList.begin()+i+j); indexList[i].frequence++; item2 = indexList[i + j]; &#125; j = 1; &#125; */ return 0;&#125; 一开始使用的是普通的for循环，但是发现随着元素的删除，循环次数应该改变，因此改成了迭代器加while的方式。 迭代器还是个蛮有用的东西，就是一个封装得比较好的指针。 main测试1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;algorithm&gt;#include "CIndex.h"#include &lt;string&gt;using namespace std;string fileList[6] =&#123; "doc1.txt", "doc2.txt", "doc3.txt", "doc4.txt", "doc5.txt", "doc6.txt"&#125;;int main()&#123; CIndex in(fileList,6); in.showCollection(); in.indexCollection(); in.mergeIndex(); in.showIndexList(); return 0;&#125;]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[老鼠和毒药问题]]></title>
    <url>%2Fpost%2Frats_and_poison%2F</url>
    <content type="text"><![CDATA[昨天在上完课回宿舍的路上，楠哥提起了一道他在某个基础知识竞赛上遇到的题目，我觉得解法很巧妙，分享记录一下。 题目有1024瓶水，其中一瓶有毒，你有10只老鼠用于试毒（这里是题目假设，所以别下不了手让老鼠试毒OVO），老鼠如果喝到毒药，会在一星期后死亡。你有一周时间，如何找出这一瓶毒药？ 解法楠哥说他刚开始想用二分，可是时间上不允许。 也就是把瓶子分两组，每组的瓶子里都倒出一点混合在一起给一只老鼠喝，哪一组的老鼠中毒了，就再把这一组的瓶子分两组，以此类推。但是这样时间上来不及，第一周缩小范围到512瓶……第九周2瓶，第十周找到。耗时太长。 于是他想到了另一种解法： 给每个瓶子标号，给老鼠也标号0到9。 从逻辑上将10只老鼠当成10位的二进制数。 将瓶子的编号转换为二进制数，比如第5号瓶子转换为第101号瓶子，将编号第0位（即最右边一位）为1的水给0号老鼠喝，编号第1位（即从右边数第二位）为1的水给1号老鼠喝，以此类推。 也就是说，0号老鼠喝了1,11,101,111……这些瓶子的水，1号老鼠喝了10,11，110,111……这些瓶子的水，后面的老鼠也是如此。 如果一周时间到，0号老鼠嗝屁了，那么就说明有毒的水的编号的第0位（最右边的位）为1；如果1号老鼠嗝屁了，就说明有毒的水编号的第1位是1…… 最后根据10只老鼠中毒情况，得到一个10位的二进制数，这个数转换为十进制就是毒药的编号。 我觉得这个解法很巧妙。 这让我想起了在听我们学校ACM协会的某节课的时候提到的状态压缩，也是使用二进制的，不过我当时没听懂，也就没记下来。 老鼠有10只，它们的死活可以表示2^10种状态，恰好是1024种。]]></content>
      <categories>
        <category>算法模型</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记3封装爬虫类]]></title>
    <url>%2Fpost%2Fpython_spider_note3class_spider%2F</url>
    <content type="text"><![CDATA[在完成了基本的爬取任务之后，接到了将其封装为一个爬虫类的任务 传送门： python爬虫学习笔记1一个简单的爬虫 python爬虫学习笔记2模拟登录与数据库 前言转载注明出处。 任务介绍1、尝试不使用session去进行爬取，最好能将cookies保存下来可以供下次使用。2、第二个是尝试将这些封装成面向对象的方式，模拟登陆，爬取，解析，写入数据库这几个部分分离开来。 先做第二个任务 过程记录190310 周日创建爬虫类12345678class spider: ''' 爬虫类 ''' def __init__(self): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 获取登录所需信息获取登录信息（账号密码以及校验码）这部分与登录可以分开，单独写一个成员函数。 在输入密码这个地方，本来查到可以使用getpass这个库里面的getpass()函数来使用类似linux的密码不回显，用法如下： 123import getpasspasswd=getpass.getpass()print(passwd)#测试用输出 但是直接在pycharm里面运行是会卡在输入那里，并且也会回显。后来查到了，这个方法是在命令行当中才管用，我试了一下在python命令行中使用， 123456&gt;&gt;&gt;import getpass&gt;&gt;&gt;passwd=getpass.getpass()Warning: Password input may be echoed.Password: &gt;? 123&gt;&gt;&gt;print(passwd)123 虽然可以使用了，但是仍然会回显。所以这个命令行说的应该不是python命令行，而是cmd或者shell。 在虚拟环境的cmd里面，成功了，Password后面未回显我的输入，下面的数字是测试用的输出，将密码打印出来。 123(venv) F:\DEVELOP\py_develop\spider&gt;python test.pyPassword:123 不过为了方便调试代码，我还是使用了input()函数 参考链接： python3-password在输入密码时隐藏密码-博客园 Python之控制台输入密码的方法-博客园 12345678910111213141516171819202122232425def get_login_data(self,login_url): ''' 获取登录需要的数据 :param login_url: 登录页面url :return: 一个存有登录数据的字典 ''' # 获取登录校验码 html = self.session.post(login_url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') lt = soup.find('input', &#123;'name': 'lt'&#125;)['value'] dllt = soup.find('input', &#123;'name': 'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data = &#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn': '', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data 登录123456789101112131415def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ login_data=self.get_login_data(login_url)#获取登录信息 # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url:#如果没有跳转回登录页面，那么就是登录成功 print("登录成功") self.is_login=True else: print("登录失败") return self.session day8进度 了解了一下Python类与对象的语法，尝试将代码封装到类中（一些中间代码未保留），不过想要将它改的有通用性（能够爬取其他网站）有些困难，还是先固定只能爬取信息门户 接下来的计划：将类完成之后再慢慢优化，学习使用cookie代替session保持登录，以及数据库的更多知识 190311 周一day9进度 图书馆借了一本mysql的书籍，在mysql命令行上练习创建数据库，表以及字段的操作 在将代码封装成类的过程中，学习了如何将参数作为一个字典传入，以及将一个字典作为参数传入 190312 周二获取单页目录内的公告url目录网页的内容： 关于……的通知 关于……获奖 …… 2700条记录，分为138页显示，下一页 123456789101112131415161718192021222324252627def get_url_from_cata(self,url,params): ''' 返回当前页面的url组成的列表 :param url: 无参数的url#如：http://portal.xxx.edu.cn/detach.portal :param params:url的？后参数#如：?pageIndex=1 :return:以页面指向的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取url域名部分 #如：http://portal.xxx.edu.cn base=url.split('/') base=base[0]+'//'+base[2] #获取当前页所有链接 html = self.session.post(url,params=params).text#用params参数来拼接参数 soup = BeautifulSoup(html, 'lxml') rss_title = soup.find_all('a', class_='rss-title')#获取所有链接 result_list=[] for url in rss_title: title=url.get_text().strip() page_url=base+'/'+url['href']#将url拼接完整 l=(title,page_url) result_list.append(l) #print(result_list) return result_list 获取所有目录内的公告url1234567891011121314151617181920212223242526272829303132333435def get_url_from_cata_all(self, url): ''' 获取页面的底部跳转到其他页的链接并获取目录，给出一个目录页的url，获取相关的所有目录页的url并获取链接 :param url: 其中任何一个目录页的url#如：http://portal.xxx.edu.cn/detach.portal?pageIndex=1 :return:以所有页面的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取除去参数之后的url #如：http://portal.xxx.edu.cn/detach.portal base=url.split('?')[0] html = self.session.post(url).text soup = BeautifulSoup(html, 'lxml') # 获取页数 reg = '共.*?条记录 分(.*?)页显示' reg = re.compile(reg, re.S) num = int(re.findall(reg, html)[0]) #获取url para = &#123; 'pageIndex': 1, 'pageSize': '', '.pmn': 'view', '.ia': 'false', 'action': 'bulletinsMoreView', 'search': 'true', 'groupid': 'all', '.pen': 'pe65' &#125; ret=[] for i in range(1,num+1): ret.extend(self.get_url_from_cata(base,params=para)) para['pageIndex'] = i return ret day10进度实现了自动获取目录页数，并从每一页目录获取所有的url，返回当前所有公告的url的列表 190313 周三获取正文123456789101112131415def get_page(self,url): ''' 提取页面中的公告正文 :param url: 页面url :return: 正文 ''' html = self.session.post(url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') bulletin_content = soup.find('div', class_='bulletin-content') bulletin_content =bulletin_content.get_text() return bulletin_content 保存到txt123456789101112131415def save_by_txt(self,file_content,file_name): ''' 获取单个公告页面的公告并保存到txt :param file_content:文件内容(str) :param file_name:输出文件名(str) :return:无 ''' # 转换为可以作为文件名字的形式 reg = r'[\/:*?"&lt;&gt;|]' file_name = re.sub(reg, "", file_name) with open(file_name, 'w', encoding='utf8') as fout: fout.write(file_content) print('成功保存到&#123;&#125;'.format(file_name)) 保存到db1234567def save_by_db(self,content,title): #未改造完成 db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') cursor = db.cursor() cursor.execute("insert into spider(`title`,`content`) values('&#123;0&#125;','&#123;1&#125;')".format(title, content)) db.commit() print('已经成功保存公告到数据库：“&#123;&#125;”'.format(title)) day11进度尝试将保存到数据库的函数里面的数据库参数放到函数形参处，怎么弄都觉得不太合适，于是还是将原本的代码放入 190314 周四cookie保持登录参考链接： Python——Cookie保存到本地-知乎（解决了问题的主要链接） 爬虫保存cookies时重要的两个参数（ignore_discard和ignore_expires）的作用 首先是库 1import http.cookiejar 初始化12345def __init__(self,headers): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 self.headers=headers#头信息 self.cookiejar=http.cookiejar.LWPCookieJar('cookie.txt') 保存cookie的函数大概是将已登录的session对象的cookies转换为字典（用了一个类似列表生成式的东西，查了一下，是字典生成式，python还真是方便，这么多简写方式），然后保存到cookiejar对象中，调用save()函数来将cookie内容保存到第一个参数指定的文件中，即使cookie已经被抛弃和过期。 1234def save_cookie(self): requests.utils.cookiejar_from_dict(&#123;c.name: c.value for c in self.session.cookies&#125;, self.cookiejar) # 保存到本地文件 self.cookiejar.save('cookies', ignore_discard=True, ignore_expires=True) 加载cookie的函数首先初始化一个LWPCookieJar对象 1load_cookiejar = http.cookiejar.LWPCookieJar() 接着从文件中加载cookie 1load_cookiejar.load('cookies', ignore_discard=True, ignore_expires=True) 这里有个问题，这里如果加载失败了（没有这个文件，之前没有保存），需要知道已经失败了。所以使用一个try语句块测试一下。 然后把这个LWPCookieJar对象给转换成字典，再转换赋值给session.cookie，这样就加载成功了 1234567891011121314151617def load_cookie(self): ''' 加载cookie :return: 是否成功 ''' load_cookiejar = http.cookiejar.LWPCookieJar() # 从文件中加载cookies(LWP格式) try: load_cookiejar.load('cookies', ignore_discard=True, ignore_expires=True) except: return False # 转换成字典 load_cookies = requests.utils.dict_from_cookiejar(load_cookiejar) # 将字典转换成RequestsCookieJar，赋值给session的cookies. self.session.cookies = requests.utils.cookiejar_from_dict(load_cookies) return True 修改后的login()1234567891011121314151617181920def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ if self.load_cookie(): self.is_login = True else: #获取登录信息 login_data=self.get_login_data(login_url) # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url: print("登录成功") self.is_login=True self.save_cookie() else: print("登录失败") return self.session day12进度 完成了爬虫类的封装 使用http.cookiejar库实现了登录一次，在cookie有效期内不必再次登录的功能 代码总览import12345import requestsfrom bs4 import BeautifulSoupimport pymysqlimport reimport http.cookiejar 构造函数12345678910class spider: ''' 爬虫类 ''' def __init__(self,headers): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 self.headers=headers#头信息 self.cookiejar=http.cookiejar.LWPCookieJar('cookie.txt') 获取登录信息12345678910111213141516171819202122232425def get_login_data(self,login_url): ''' 获取登录需要的数据 :param login_url: 登录页面url :return: 一个存有登录数据的字典 ''' # 获取登录校验码 html = self.session.post(login_url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') lt = soup.find('input', &#123;'name': 'lt'&#125;)['value'] dllt = soup.find('input', &#123;'name': 'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data = &#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn': '', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data 登录12345678910111213141516171819202122def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ if self.load_cookie(): self.is_login = True else: #获取登录信息 login_data=self.get_login_data(login_url) # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url: print("登录成功") self.is_login=True self.save_cookie() else: print("登录失败") return self.session 获取单页目录1234567891011121314151617181920212223242526def get_url_from_cata(self,url,params): ''' 返回当前页面的url组成的列表 :param url: 无参数的url :param params:url的？后参数 :return:以页面指向的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取url域名部分 base=url.split('/') base=base[0]+'//'+base[2] #获取当前页所有链接 html = self.session.post(url,params=params).text#用params参数来拼接参数 soup = BeautifulSoup(html, 'lxml') rss_title = soup.find_all('a', class_='rss-title')#获取所有链接 result_list=[] for url in rss_title: title=url.get_text().strip() page_url=base+'/'+url['href']#将url拼接完整 l=(title,page_url) result_list.append(l) #print(result_list) return result_list 获取全部目录123456789101112131415161718192021222324252627282930313233def get_url_from_cata_all(self, url): ''' 获取页面的底部跳转到其他页的链接并获取目录，给出一个目录页的url，获取相关的所有目录页的url并获取链接 :param url: 其中任何一个目录页的url :return:以所有页面的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取除去参数之后的url base=url.split('?')[0] html = self.session.post(url).text soup = BeautifulSoup(html, 'lxml') # 获取页数 reg = '共.*?条记录 分(.*?)页显示' num = int(re.findall(reg, html)[0]) #获取url para = &#123; 'pageIndex': 1, 'pageSize': '', '.pmn': 'view', '.ia': 'false', 'action': 'bulletinsMoreView', 'search': 'true', 'groupid': 'all', '.pen': 'pe65' &#125; ret=[] for i in range(1,num+1): ret.extend(self.get_url_from_cata(base,params=para)) para['pageIndex'] = i return ret 获取正文123456789101112131415def get_page(self,url): ''' 提取页面中的公告正文 :param url: 页面url :return: 正文 ''' html = self.session.post(url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') bulletin_content = soup.find('div', class_='bulletin-content') bulletin_content =bulletin_content.get_text() return bulletin_content 保存到txt123456789101112131415def save_by_txt(self,file_content,file_name): ''' 获取单个公告页面的公告并保存到txt :param file_content:文件内容(str) :param file_name:输出文件名(str) :return:无 ''' # 转换为可以作为文件名字的形式 reg = r'[\/:*?"&lt;&gt;|]' file_name = re.sub(reg, "", file_name) with open(file_name, 'w', encoding='utf8') as fout: fout.write(file_content) print('成功保存到&#123;&#125;'.format(file_name)) 保存到数据库123456def save_by_db(self,content,title): db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') cursor = db.cursor() cursor.execute("insert into spider(`title`,`content`) values('&#123;0&#125;','&#123;1&#125;')".format(title, content)) db.commit() print('已经成功保存公告到数据库：“&#123;&#125;”'.format(title)) 保存cookie1234def save_cookie(self): requests.utils.cookiejar_from_dict(&#123;c.name: c.value for c in self.session.cookies&#125;, self.cookiejar) # 保存到本地文件 self.cookiejar.save('cookies', ignore_discard=True, ignore_expires=True) 加载cookie123456789101112131415161718def load_cookie(self): ''' 加载cookie :return: 是否成功 ''' load_cookiejar = http.cookiejar.LWPCookieJar() # 从文件中加载cookies(LWP格式) try: load_cookiejar.load('cookies', ignore_discard=True, ignore_expires=True) except: print('cookie加载失败') return False # 转换成字典 load_cookies = requests.utils.dict_from_cookiejar(load_cookiejar) # 将字典转换成RequestsCookieJar，赋值给session的cookies. self.session.cookies = requests.utils.cookiejar_from_dict(load_cookies) return True 爬取12345678def crawl(self,login_url,cata_url): self.login(login_url)#登陆 item_list=self.get_url_from_cata_all(cata_url)#获取所有标题以及对应链接 for i in item_list: title,url=i#解包 text=self.get_page(url)#获取内容 self.save_by_txt(text,title+'.txt')#保存 #self.save_by_db(text,title) 调用123456789headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',&#125;login_url='http://xxx.xxx.xxx.cn/authserver/login?service=http%3A%2F%2Fportal.chd.edu.cn%2F'cata_url='http://xxxxxx.xxx.xxx.cn/detach.portal?pageIndex=1&amp;pageSize=&amp;.pmn=view&amp;.ia=false&amp;action=bulletinsMoreView&amp;search=true&amp;groupid=all&amp;.pen=pe65'#调用spiderman=spider(headers)spiderman.crawl(login_url, cata_url)]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记2模拟登录与数据库]]></title>
    <url>%2Fpost%2Fpython_spider_note2login_and_database%2F</url>
    <content type="text"><![CDATA[为了加入学校里面一个技术小组，我接受了写一个爬取学校网站通知公告的任务。这个任务比以前写的爬虫更难的地方在于，需要模拟登录才能获得页面，以及将得到的数据存入数据库。 本文按照日期来记录我完成任务的过程，然后再整理一遍全部代码。读者可以通过侧栏目录跳转阅读。不介绍库的安装。 传送门：爬虫学习笔记1 转载声明关于参考链接：本文用到的其他博客的链接都以（我自己对内容的概括或者文章原标题-来源网站-作者名）的格式给出，关于作者名，只有博客作者自己明确声明为“原创”，我才会加上作者名。引用的文章内容我会放在来源链接的下方。 关于本文：我发一下链接都注明出处了，如果想转载，也请这样做。作者憧憬少，链接的话看浏览器地址栏。 任务介绍爬取信息门户新闻并且存入数据库。 首先分解任务： 实现爬取综合新闻页面的公开新闻存入markdown文件中(190303完成) 将数据存到数据库（190304完成） 学习模拟登录（190305到190307完成） 爬取信息门户新闻（190308完成） （进阶）将代码进行封装、优化（目前未封装） （进阶）动态更新（目前未着手） 过程记录190303 周日练习爬取公开页面我的第一个爬虫是在2月多的时候在家写的，那个只是个简单的爬虫，目标是公开的页面，不需要模拟登录，也不需要存储到数据库，直接存到txt文件中。 先爬取学校官网的综合新闻页面复习一下。 首先讲一下我的思路： 由于新闻和公告页面通常是有一个目录页面的，也就是包含子页面的链接，在目录的子页面内才是正文内容。 假设这一页目录有三个新闻，就像是下面： 新闻目录 新闻一 新闻二 新闻三 点击查看下一页 这样的结构。 如果要写一个爬虫函数来爬取所有新闻页面，那么就要从目录着手。目录中含有前往别的新闻页面的链接，所以可以在目录页获取本页所有新闻的链接，遍历所有链接并提取新闻内容。 至于翻页也可以这样做到，“下一页”按钮也是一个链接，可以通过这个链接获取到下一页的内容。翻页部分原理比较简单，我是先攻克其他难关，把它留到最后写的。 提取单页面新闻首先是提取单个页面的新闻。向目标url发出访问请求： 1234567891011import requestsdef getNews(url): ''' 提取页面的新闻与图片并存储为markdown文件 :param url: 要爬取的目标网页url :return: 无 ''' #发送请求 r=requests.get(url)#r为response对象 html=r.text#r.text是请求的网页的内容 print(html) 编码问题这里遇到了第一个问题，提取到的页面有乱码。 解决方法：先获取响应对象的二进制响应内容，然后将其编码为utf8 参考链接： python中response.text与response.content的区别-CSDN requests.content返回的是二进制响应内容 而requests.text则是根据网页的响应来猜测编码 UNICODE,GBK,UTF-8区别（一个比较好的编码的教程，便于理解编码的概念）-博客园 Python解决抓取内容乱码问题（decode和encode解码）-CSDN-浅然_ 字符串在Python内部的表示是unicode编码，在做编码转换时，通常需要以unicode作为中间编码，即先将其他编码的字符串解码（decode）成unicode，再从unicode编码（encode）成另一种编码。 decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gb2312编码的字符串str1转换成unicode编码。 encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘utf-8’)，表示将unicode编码的字符串str2转换成utf-8编码。 修改代码为： 1234#发送请求r=requests.get(url)html=r.content#获取二进制字节流html=html.decode('utf-8')#转换为utf8编码（该网页使用的是utf8编码） 解析网页（bs4）一开始我和之前一样使用正则表达式来提取，但是不够熟悉，总是写不出匹配的上的正则表达式。还是使用另一个东西——BeautifulSoup库 具体如何使用请查看其他教程，本文只说我自己用到的部分。 参考链接： Python爬虫常用的几种数据提取方式-CSDN-凯里潇 零基础入门python3爬虫-bilibili（里面的视频p11） beautifulsoup（基本选择器，标准选择器，css选择器）-CSDN-Halosec_Wei（基本上是上面一个b站链接的文字版，不知道是不是同一个人） beautifulsoup详细教程-脚本之家 beautifulsoup基本用法总结-CSDN-kikay BeautifulSoup是Python的一个库，最主要的功能就是从网页爬取我们需要的数据。BeautifulSoup将html解析为对象进行处理，全部页面转变为字典或者数组，相对于正则表达式的方式，可以大大简化处理过程。 我目前的理解是，这个BeautifulSoup库需要用到其他html解析库，可以使用python自带的，也可以安装第三方库，其他的库就像功能扩展插件一样，没有的话它自己也能解析。我安装了名为lxml的解析库。 查看源代码，找到网页中有关新闻的代码，手动将其格式化之后如下（内容不重要，省略）： 12345678910111213141516171819202122232425&lt;h1 class="arti-title"&gt;标题省略&lt;/h1&gt;&lt;p class="arti-metas"&gt; &lt;span class="arti-update"&gt;发布时间：2019-01-23&lt;/span&gt; &lt;span class="arti-update1"&gt;作者：xx&lt;/span&gt; &lt;span class="arti-update2"&gt;来源：xxx&lt;/span&gt;&lt;/p&gt;&lt;div class="entry"&gt; &lt;article class="read"&gt; &lt;div id="content"&gt; &lt;div class='wp_articlecontent'&gt; &lt;p&gt;新闻前言省略 &lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;新闻内容省略 &lt;/p&gt; &lt;p&gt; &lt;img width="556" height="320" align="bottom" src="url省略" border="0"&gt; &lt;/p&gt; &lt;p style="text-align:right;"&gt;（审稿：xx &amp;nbsp;网络编辑：xx） &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; 接着上面的代码： 123456789#解析htmlsoup=BeautifulSoup(html,"lxml")#返回已解析的对象#获取标题title=soup.find('h1',class_='arti-title').string#获取时间update=soup.find('span',class_='arti-update').string#获取正文标签content=soup.find('div',class_='wp_articlecontent') 提取图片我打算将新闻保存到markdown文件中，提取新闻中的图片的链接的地址，这样在md文件中就能显示出图片了。 1234567#获取图片链接base='学校官网url，用于和img标签中的相对地址拼接成绝对地址'imgsTag=content.find_all('img')imgsUrl=[]for img in imgsTag: imgsUrl.append(base+img['src'])#拼接成完整的url img.extract()#删除图片标签 删除多余标签123456#删除多余标签for p in content.find_all('p',&#123;'style':"text-align:center;"&#125;): p.extract()p=content.find('p', &#123;'style': "text-align:right;"&#125;)if(p!=None): p.extract() 保存到文件123456789101112# 拼接成字符串#后来知道这样的提取方式其实不能完全提取到所有内容fileContent=''for i in content.contents:#遍历正文内容的所有子标签 if(i.string!=None):#如果子标签里面有内容 #print(i.string)#调试 fileContent+=i.string#基本只剩下p标签了 fileContent+='\n\n' #保存到md文件with open('data.md','w') as fout: fout.write(fileContent) 代码总览12345678910111213141516171819202122232425262728293031323334353637383940414243444546import requestsfrom bs4 import BeautifulSoup#第4个版本改名bs4而不是全名那么长了def getNews(url): ''' 提取页面的新闻与图片并存储为markdown文件 :param url: 要爬取的目标网页url :return: 无 ''' #发出请求 r=requests.get(url) html=r.content html=html.decode('utf-8')#转换编码 #解析html soup=BeautifulSoup(html,"lxml") content=soup.article #获取标题 title=soup.find('h1',class_='arti-title').string #获取时间 update=soup.find('span',class_='arti-update').string #获取正文 content=soup.find('div',class_='wp_articlecontent') #获取图片链接 base='http://xxxxx.xxx'#学校官网url，用于和img标签中的相对地址拼接成绝对地址 imgsTag=content.find_all('img') imgsUrl=[] for img in imgsTag: imgsUrl.append(base+img['src'])#拼接成完整的url img.extract()#删除图片标签 #删除多余标签 for p in content.find_all('p',&#123;'style':"text-align:center;"&#125;): p.extract() p=content.find('p', &#123;'style': "text-align:right;"&#125;) if(p!=None): p.extract() # 拼接成字符串 fileContent='' for i in content.contents: if(i.string!=None): #print(i.string)#调试 fileContent+=i.string fileContent+='\n\n' with open('data.md','w') as fout: fout.write(fileContent) 提取多页面新闻原理在上面说了，提取完单页基本上就完成了。 12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupdef getNewsContents(url): ''' 爬取目录页面链接到的页面 :param url: 新闻目录页面的url :return: 无 ''' #获取网页内容 r=requests.get(url)#以get方式访问 html=r.content html=html.decode('utf-8') #获取每篇新闻的链接 base='http://xxxxx.xxx'#学校官网url，用于和相对地址拼接成绝对地址 soup=BeautifulSoup(html,'lxml') for page_url in soup.find_all('a',class_='column-news-item'): page_url=base+'/'+page_url['href'] print(page_url) getNews(page_url)#调用提取单页函数 day1进度 实现爬取长安大学综合新闻页面的公开新闻存入markdown文件中 复习了requests库的使用 学习了BeautifulSoup4库的基本使用 190304 周一这一天主要是将前一天爬取的数据存入数据库。 将数据存入数据库安装MySQL数据库参考链接： 零基础入门python3爬虫-bilibili（里面的视频p4） 使用MySQL WorkbenchMySQL Workbench是一个可视化工具，安装MySQL的时候自带（我安装的是最新版的），在安装目录找到它的exe然后加个快捷方式在桌面，可以方便地查看数据和执行SQL查询指令，具体使用方法可以问度娘。我现在也不是很会。 我创建的数据库名为news，里面创建了一个数据表chdnews。 连接数据库和大多数数据库一样，MySQL是C/S模式的，也就是客户端（client）/服务端（server）模式的。数据库有可能在远程服务器上。想要使用数据库，就需要连接到数据库。 python中要使用数据库需要一个pymysql库。 下面是连接的代码： 123import pymysql#连接数据库db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') 这个连接函数看参数名就可以看出含义了。 host：主机ip，127.0.0.1是回传地址，指本机。也就是连接本电脑的MySQL的意思 port：端口号，用来和ip一起指定需要使用数据库的软件。在安装的时候会让你设置，默认3306 user&amp;passwd：用户名和密码，在安装的时候已经设置好了 db：你要连接的数据库的名字。一台电脑上可以有很多数据库，数据库里面可以有很多数据表。 charset：字符编码 插入数据接着可以准备一个游标，游标大概是一个用于存储结果集开头地址的指针吧，我是这么理解的。在我学了更多数据库知识后可能会更新这一部分。 12#创建游标cursor = db.cursor() 接着执行SQL的插入语句： 12#插入cursor.execute("insert into chdnews(`title`,`article`) values('&#123;0&#125;','&#123;1&#125;')".format(title,fileContent))#此处变量为上文代码中的变量 这里的SQL语句是这样的： 1insert into 数据表名(字段名1，字段名2) values(值1，值2) 后面的format函数是python的格式化函数，将变量的值加入到字符串中对应位置。 最后提交： 12#提交更改db.commit() 接着打开workbench，就会发现已经存入数据库了。（你得把代码放在上面提取单页新闻的函数那里，放在保存到文件的那部分代码那儿） day2进度 下载并安装MySQL以及MySQL Workbench 使用pymysql库进行数据库的连接，实现了把第一天得到的数据存入数据库 190305 周二初步了解模拟登录最后的任务需要爬取登录后才能查看的页面，于是我去搜索了很多博客，只放一部分对我有帮助的链接。 参考链接： 模拟登录CSDN-博客园 模拟登录github-博客园 首先查看一下需要的登录数据： 打开登录网页，用F12打开开发者工具，选择network（网络）选项卡 登录你的账号，此时控制台会显示一大堆请求与响应，找到以post方式发送的请求，一般排在第一个 那里会显示几个栏目，找到Form Data（表单数据），这个里面是你填写登录表单之后使用POST方式发送给服务端的内容。这里面除了自己填写的账号密码之外还有一些东西，比如下图的lt,dllt,execution,_eventId,rmShown这些都是在表单的隐藏域中，查看登录页面的源代码是可以看的到的。这些隐藏起来的东西是为了检验你是否是从浏览器进来的，只要获取到这些东西，再加上头部信息，就能伪装成浏览器了 至于头部信息，在下图也可以看到我折叠起来的几个栏目，有一个是Request Headers，这是我们在点击登录按钮时发送的POST请求信息的信息头。将里面的User-Agent给复制到你代码里面存在一个字典里面等会用 把头部信息和表单数据都看一下，准备一下 12345678910111213141516171819202122#登录前的准备login_url = 'http://xxxx.xxx'#登录页面的url#头部信息headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' #加上后面这些会后悔的，别加。 'Host':'xx.xx.xx.xx', 'Referer':'http://xxx.xxx?xxx=http://xxx.xx', 'Origin':'http://xxx.xxx.xx'&#125;#登录用的数据login_data=&#123; 'username': '你的账号', 'password': '你的密码', 'btn':'', 'lt': LT-790162-J9kW2aEFsK3ihu4AzXcovdsJy6cYBM1552123884047-D1Nx-cas， #实际上lt并不能这样写上去，下文会解释。这里记录我自己的错误 'dllt': 'userNamePasswordLogin', 'execution': 'e1s1', '_eventId': 'submit', 'rmShown': 1 &#125; 数据准备好之后就开始登录，使用的是requests的另一个方法——post。 向服务器发出请求（request）的方式有get和post，查看html源代码的时候在表单标签处可以看到表单提交的方法。如： 1&lt;form id="casLoginForm" method="post"&gt; 像这样写html代码会让浏览器在你按下登录按钮的时候以post的方式提交表单，也就是以post的方式向服务器发起request，将form data发送过去。 post方法的好处是在发送过程中会隐藏你的表单数据，不会被直接看到； 而前面使用过的get方法，会把你的表单数据加在url后面，网址后边以问号开头，以&amp;连接的就是发送过去的参数。 涉及登录用post比较好，以免轻易泄露密码。 12#以post方式发出登录请求r=requests.post(login_url,headers=headers,data=login_data) 按理来说应该可以了呀，为什么不行？仍然得到登录页面。在这一天我折腾了很久，没有得到答案。 不过在找资料时却学到了其他的一些知识，关于cookie和session。 cookie和session我目前的理解（如果不对欢迎留言）： http是无状态协议，两次访问都是独立的，不会保存状态信息。也就是你来过一次，下次再来的时候网站还是当你第一次来。那么怎么知道你来过，从而给你还原之前的数据呢？就有人想出cookie和session两种方式。 cookie（直译：小甜饼）是服务端（网站服务器）收到客户端（你电脑）的request（请求）的时候和response（响应）一起发给客户端的数据。客户端把它存在文件里面，并在下一次访问这个网站时将cookie随着request一起发送过去，这样服务端就会知道你就是之前来过的那个人了。cookie存储在客户端。 客户端发送request 服务端发送response附带一个cookie（一串数据） 客户端第二次访问时把cookie复制一份一起发过去 服务端看到你的cookie就知道你是谁了 session（会话）是在服务端内存中保存的一个数据结构，一旦有客户端来访问，那么就给这个客户端创建一个新的session在服务端的内存，并将它的session ID随着response发回给客户端。客户端第二次访问时，会将被分配的SID随着request一起发过来，服务端在这边验证SID之后就会知道你来过。session存储在服务端。 客户端发送request 服务端发送response并在自己这边创建一个session（一堆数据）并发送一个session ID给客户端 客户端第二次访问时把session ID一起发过去 服务端看到你的session ID就知道你是谁了 不过这俩是用来保持登录的，我还没登录成功想这个干啥？请看下一天。 day3进度 初步了解cookie和session的概念 了解如何使用chrome浏览器的控制台查看post表单信息 尝试使用requests的post方法模拟登录，失败，返回登录页面 190306 周三表单校验码（非验证码）怎么弄都不成功，都跳回登录页面。我只好去询问组长这是为什么。 原来我没发现表单校验码会变的！ 一直没注意啊啊啊啊啊啊！ 我没有认真比对过两次打开的乱码不一样，看结尾一样就以为一样了。其中的lt这个域每次打开网页都是不一样的，随机出的！ 既然知道了问题，就好解决了。 123456789101112131415161718#获取登录校验码html=requests.post(login_url,headers=headers).textsoup=BeautifulSoup(html,'lxml')lt=soup.find('input',&#123;'name':'lt'&#125;)['value']dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value']execution = soup.find('input', &#123;'name': 'execution'&#125;)['value']_eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value']rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value']login_data=&#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown&#125; 为了保险，我把其他的表单域也给解析赋值给变量了。 不过仍然无法登陆成功，而是进入了一个诡异的页面: 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 确实有进展，但是这是啥？nginx？查了一下是一个高性能的HTTP和反向代理服务器，但是和我现在登录有什么关系呢？（黑人问号.jpg） 利用session保持校验码即使登录成功，还有一个问题无法解决，那就是我获取校验码的request和登录用的request是两次不同的访问请求呀，这样校验码又会变化。 我想起了前一天看到的session，这玩意不就能让服务端记住我？（cookie试了一下，保存下来的是空的文件不知道怎么回事） 于是新建一个会话： 12#新建会话session=requests.session() 在获取校验码的时候改成使用session变量来发起请求： 12#获取登录校验码html=session.post(login_url,headers=headers).text 这里的session是在客户端创建的，并不是服务端那个，我想它可能存储的是服务端发送过来的session ID吧。 同理在正式发送请求时这样： 12#登录r=session.post(login_url,headers=headers,data=login_data) 这样就能让服务端知道我是刚刚获取校验码的那个小伙汁：D 在这一天我没有办法验证是否有效，不过在之后我验证了这个方法的成功性。 day4进度 知道了原来有个每次会变化的校验码“lt”，找到了跳转回登录页面的原因。使用Beautifulsoup来获取每次的校验码，不过仍然没有解决无法登录的问题 使用session对象来保证获取校验码和登录时是同一个会话，未验证 190307 周四多余的头部信息我终于发现了问题所在！！！！！ 1234567headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36', #'Host':'xxx.xxx.xxx.xxx', #'Referer':'http://xxx.xxx.xxx.xxx...',#不详细打码了 #'Origin':'http://xxx.xxx.xxxx' #去掉多余的头信息才成功登录！！！！！卡了很久没想到是因为这个&#125; 头部信息写多了，我只保留了User-Agent之后成功登录了，你们能体会到我当时有多开心吗！ 我将成为新世界的卡密小组里面最快完成的人！ 解决了这个问题，剩下的就特别简单了。 当时我有一个下午的时间，于是我将进度迅速推进。 爬取通知公告设登录页面为pageA，登录之后的页面跳转到pageB，而pageB有一个按钮跳转到pageC，这个pageC就是day1的时候的目录页面，里面有着pageC1、pageC2、pageC3……等页面的链接，而这个pageC最后面还有个按钮用于跳转到目录的下一页，也就是pageC?pageIndex=2，还有137页公告栏目录。 没有什么新的东西，和day1说的爬取方式差不多，只是页面正文的格式和day1的新闻不太一样。核心结构如下，我省略了很多： 12345678910111213141516&lt;html&gt; &lt;body&gt; &lt;div class="bulletin-content" id="bulletin-contentpe65"&gt; &lt;p style=";background: white"&gt; &lt;a name="_GoBack"&gt; &lt;/a&gt; &lt;span style="font-size: 20px;font-family: 仿宋"&gt; 校属各单位： &lt;/span&gt; &lt;/p&gt; &lt;p&gt; &lt;br/&gt; &lt;/p&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 大概就是一个&lt;p&gt;标签里面放一个或多个&lt;span&gt;标签，而这里面可能还会嵌套几个&lt;span&gt;标签，里面才有内容，而两个内部的&lt;span&gt;之间还可能有内容。 这要怎么解析？ 在尝试了很多方案之后，我终于百度到一个函数： 1tag.get_text()#提取名为tag的bs4标签的内部的所有文字 参考链接： BeautifulSoup获取标签中包含的文字-CSDN-niewzh（正是这个博客解决了我的问题） BeautifulSoup中的.text方法和get_text()方法的区别-CSDN 解决方案： 12345678910#获取正文内容html=session.post(url,headers=headers).textsoup=BeautifulSoup(html,'lxml')article=soup.find('div',class_='bulletin-content')news_content=''for p in article.find_all('p'): if p.span!=None:#如果p含有一层span text=str(p.get_text()).strip()#获取内容并去除多余空格 news_content+=text+'\n' 接着我就把爬下来的东西存到数据库里面去了。弄完之后得去赶作业了，这一天的时间用完了。 day5进度1.找到无法登录且跳转到未知页面的原因是头部信息加了多余的值，解决之后成功登录到信息门户，实现模拟登陆2.利用之前爬取单个页面到文件的方法，用beautifulsoup解析并保存内容到文件3.存入MySQL数据库中4.还差爬取多页目录的功能，预计明天完成。整理代码后可提交 190308 周五更多的目录页开了一个新文件准备整理一下代码，并完成最后一个功能——爬取完目录页第一页之后爬取后面更多的页。 查看源代码的时候，找“第二页”这个按钮对应的链接，发现了规律： 12345678910111213&lt;div class="pagination-info clearFix"&gt; &lt;span title='共2740条记录 分137页显示'&gt; 2740/137 &lt;/span&gt; &lt;a href='detach.portal?pageIndex=1&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第1页'&gt;&amp;lt;&amp;lt;&lt;/a&gt; &lt;div title="当前页"&gt;1&lt;/div&gt; &lt;a href='detach.portal?pageIndex=2&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第2页'&gt;2&lt;/a&gt; &lt;a href='detach.portal?pageIndex=3&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第3页'&gt;3&lt;/a&gt; &lt;a href='detach.portal?pageIndex=4&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第4页'&gt;4&lt;/a&gt; &lt;a href='detach.portal?pageIndex=5&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第5页'&gt;5&lt;/a&gt; &lt;a href='detach.portal?pageIndex=6&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65'&gt;&amp;gt;&lt;/a&gt; &lt;a href='detach.portal?pageIndex=137&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到最后页'&gt;&amp;gt;&amp;gt;&lt;/a&gt;&lt;/div&gt; 可以看出，指向其他目录页的相对链接，只是参数略有不同，参数中只有pageIndex发生了变化。至于给url加参数，我记得前几天看到过。 123456789101112131415161718#作为参数的字典para=&#123; 'pageIndex':1,#这里需要修改，先爬第一页 'pageSize':'', '.pmn':'view', '.ia':'false', 'action':'bulletinsMoreView', 'search':'true', 'groupid':'all', '.pen':'pe65'&#125;catalogue_url='http://xxx.xx.xx.cn/detach.portal'#未加参数的新闻目录页url session = login() # 获取已登录的session,这个自定义函数会在下面列出 for i in range(1,page_count+1):#page_count是要获取的页数 para['pageIndex']=i#设置新闻当前页的索引 # 从目录页获取新闻页面链接 html = session.post(catalogue_url,params=para).text 整理代码要用到的库 1234import requestsimport refrom bs4 import BeautifulSoupimport pymysql get_bulletin123456789101112131415161718192021222324252627282930313233343536def get_bulletin(page_count): ''' 目录有多页，从第一页开始获取，往后获取page_count页的目录，并读取目录指向的所有公告 :param page_count: 要爬取的目录页面的数量 :return: 无 ''' para=&#123; 'pageIndex':1, 'pageSize':'', '.pmn':'view', '.ia':'false', 'action':'bulletinsMoreView', 'search':'true', 'groupid':'all', '.pen':'pe65' &#125; catalogue_url='http://xxx.xxx.xxx.cn/detach.portal'#未加参数的公告目录页url session = login() # 获取已登录的session for i in range(1,page_count+1): para['pageIndex']=i#设置公告当前页的索引 # 从目录页获取公告页面链接 html = session.post(catalogue_url,params=para).text soup = BeautifulSoup(html, 'lxml') rss_title = soup.find_all('a', class_='rss-title') #将得到的链接与标题组装成字典 bulletin_dict = &#123;&#125; for url in rss_title: bulletin_title = str(url.span.string).strip() bulletin_url = 'http://xxx.xx.xx.cn/' + url['href'] bulletin_dict.setdefault(bulletin_title, bulletin_url)#添加一条公告记录 #保存公告到数据库 for bulletin_title, bulletin_url in bulletin_dict.items(): #saveInTXT(bulletin_url, session, bulletin_title)#这个是保存到txt文件的函数，用于测试 saveInDB(news_url, session, news_title) login1234567891011121314151617181920212223242526272829303132333435363738def login(): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ #设置 login_url = 'http://xxx.xx.xx.cn/authserver/login?service=http%3A%2F%2F%2F' headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' &#125; #新建会话 session=requests.session() #获取登录校验码 html=session.post(login_url,headers=headers).text soup=BeautifulSoup(html,'lxml') lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; #登录 response=session.post(login_url,headers=headers,data=login_data) if response.url=='http://xxx.xx.xx.cn/': print('登录成功！') return session saveInTXT12345678910111213141516171819202122232425262728293031323334353637def saveInTXT(url, session, title): ''' 获取单个公告页面的公告并保存到txt :param url: 要获取的页面的url :param session:已经登录的会话 :param title:公告标题 :return:无 ''' #将标题转换为可以作为文件名字的形式 reg = r'[\/:*?"&lt;&gt;|]' title = re.sub(reg, "", title) path='bullet\\' + title+'.txt'#保存在py文件目录下的bulletin文件夹内，以txt格式保存 headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' &#125; ''' #测试代码，从文件读取手动获取的公告html页面，单机测试 with open('new.txt','r',encoding='utf8') as fin: html=fin.read() ''' html=session.post(url,headers=headers).text soup=BeautifulSoup(html,'lxml') #print(soup.prettify()) bulletin_content=soup.find('div', class_='bulletin-content') bulletin_content= '' for p in bulletin_content.find_all('p'): if p.span!=None:#如果p含有一层span text=str(p.get_text()).strip() bulletin_content+= text + '\n' with open(path,'w',encoding='utf8') as fout: fout.write(bulletin_content) print('“&#123;&#125;”成功保存到&#123;&#125;'.format(title,path)) saveInDB1234567891011121314151617181920212223242526272829def saveInDB(url, session, title): ''' 获取单个公告页面的公告并保存到txt :param url: 要获取的页面的url :param session:已经登录的会话 :param title:公告标题 :return:无 ''' headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' &#125; html=session.post(url,headers=headers).text soup=BeautifulSoup(html,'lxml') bulletin_content=soup.find('div', class_='bulletin-content') bulletin_content= '' for p in bulletin_content.find_all('p'): if p.span!=None:#如果p含有一层span text=str(p.get_text()).strip() bulletin_content+= text + '\n' #保存到数据库 db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') cursor = db.cursor() cursor.execute("insert into chdnews(`title`,`content`) values('&#123;0&#125;','&#123;1&#125;')".format(title, bulletin_content)) db.commit() print('已经成功保存公告到数据库：“&#123;&#125;”'.format(title)) 调用12#调用get_bulletin(10)#爬取10页公告 暂时没有将其通用化，直接将网址写死在函数里面了。 day6进度 通过调整服务门户的url中的参数来获取通知公告的每一个目录页的url，从而爬取所有公告 将学习中写的测试代码重新构造整理，添加函数注释，提交任务 190309 周六day7进度写了本篇博客进行总结]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换了一个主题]]></title>
    <url>%2Fpost%2Fhexo_change_theme%2F</url>
    <content type="text"><![CDATA[把主题从shana（夏娜）换成了NexT，记录一下这个过程，以及遇到的一些有用的博客链接。流水账，主要保存链接。 缘起刚弄了hexo博客很兴奋，于是去鼓捣各种东西，首先选了一个二次元的主题shana，虽然这个主题我很喜欢，但是想要加目录或者是其他的一些东西，网上根本就搜不到相关的内容，在解决各种问题的过程中，我发现搜索到的几乎都是关于NexT这个主题的解决方法，应为这个主题很多人用。 在又一次发现主题的文件被我“弄坏了”（背景图片的幻灯片播放只显示一次）之后，我想还是换成NexT吧，这样就能专注于写博客，而不是为设置博客而烦恼。 安装NexT主题NexT主题安装和其他主题一样，clone下来再改一下站点配置文件_config.yml就好了。 然后再设置这个主题的配置文件。这个主题的配置文件与shana相比起来不知道详细了多少，各种设置都准备齐全了。没费多少功夫就配置好很好看的站点了。接着就是把站长统计之类的东西设置一下。 cnzz站长统计，统计访问 leanCloud数据统计，统计文章阅读数，参考链接：Hexo-Next搭建个人博客（添加统计访客量以及文章阅读量） 当然我也试着弄了一下gitment评论，仍然不行，那么只能继续采用“直接在菜单中给出issue页面链接”的方式了。参考链接：Hexo-Next 添加 Gitment 评论系统]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++学生信息管理系统（一）]]></title>
    <url>%2Fpost%2Fcpp_student_info_management_system1%2F</url>
    <content type="text"><![CDATA[尝试重新设计与编写大一第一学期的c++课设——学生信息管理系统。本文作简单思路分析与代码分享。B站视频内录制了从头开始写的整个过程：课程设计|c++控制台简易学生信息管理系统 思路要求：能够录入，显示，查找，删除，文件存取学生信息 以当时的知识是以链表来实现的，这次也是使用链表。 首先，创建一个链表结点类用于存放学生的信息，每个对象都是一个学生。 其次，创建一个链表类用于将结点连接起来。 最后，利用链表类已经创建好的各种接口，在main函数中进行装配，实现所需要的各种功能。 链表结点类 类名：CStudent 属性：姓名、性别、成绩、其余本质相同的属性（如班级号，学号）省略。 方法：以不同方式显示该学生所有信息、手动录入学生信息 类声明12345678910111213141516171819202122232425262728#pragma once//链表结点类//学生类//属性：姓名、性别、成绩//方法：录入、显示class CStudent&#123; char name[20]; bool sex;//true为男，false为女 int score;public: //链表需要的指针域 CStudent* next;//=======================================public: //构造函数 CStudent(const char p_name[], bool p_sex, int p_score); CStudent(); //录入与显示 void input(); void show(int method); //get char* getName() &#123; return name; &#125; bool getSex() &#123; return sex; &#125; int getScore() &#123; return score; &#125; //析构函数 ~CStudent();&#125;; 类实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//@fileName &lt;CStudent.cpp&gt;#include "CStudent.h"#include &lt;cstring&gt;#include &lt;iostream&gt;using namespace std;CStudent::CStudent(const char p_name[], bool p_sex, int p_score)&#123;//有参构造则自动录入信息 strcpy(name, p_name); sex = p_sex; score = p_score;&#125;CStudent::CStudent()&#123; input();//如果无参构造，则手动录入信息&#125;void CStudent::input()&#123; cout &lt;&lt; "请输入学生姓名：" &lt;&lt; endl; cin &gt;&gt; name; //如果遇到cin连续输入出错的问题，可以在每次输入后加个cin.get() cout &lt;&lt; "请输入学生性别（1为男，0为女）：" &lt;&lt; endl; int isex; cin &gt;&gt; isex; sex = isex ? true : false; cout &lt;&lt; "请输入学生成绩：" &lt;&lt; endl; cin &gt;&gt; score;&#125;void CStudent::show(int method)&#123; switch (method) &#123; case 0://横向显示，一行一条记录 cout &lt;&lt; name&lt;&lt;"\t" &lt;&lt; (sex ? "男" : "女")&lt;&lt;"\t" &lt;&lt; score&lt;&lt;"\t" &lt;&lt; endl; break; case 1://纵向显示，每行一个属性 cout &lt;&lt; "姓名：" &lt;&lt; name &lt;&lt; endl &lt;&lt; "性别：" &lt;&lt; (sex ? "男" : "女") &lt;&lt; endl &lt;&lt; "成绩：" &lt;&lt; score &lt;&lt; endl &lt;&lt; endl; break; default: break; &#125; &#125;CStudent::~CStudent()&#123;&#125; 链表类类声明12345678910111213141516171819202122232425262728//@fileName &lt;CStudentList.h&gt;#pragma once#include "CStudent.h"//链表类（带头结点的单向链表）//属性：指向头结点的头指针//方法：构造函数（手动输入）、构造函数（传入结点对象数组）、析构函数//方法：显示表、查找、删除、把数据存入文件、从文件中读取数据class CStudentList&#123; CStudent* head;//头指针public: //构造函数 CStudentList(int n);//手动录入n个学生的信息 CStudentList(CStudent s[],int n);//通过对象数组自动录入n个学生的信息 CStudentList(const char fileName[]);//读取文件中的数据信息来初始化 CStudentList(); //功能 void showList();//显示整个链表的信息 int search(const char name[]);//按名字查找并返回找到的个数 void deleteNode(CStudent*p);//删除指针p指向的结点 int deleteByName(const char name[]);//删除表中第一个匹配的记录，同时返回是否删除成功 //文件读写 void save(const char fileName[]); void open(const char fileName[]); //析构函数 ~CStudentList();&#125;; 类实现构造函数我设计了四个构造函数。 1.如果没有参数，那么就只建立一个空链表，即只有一个头结点的链表。 1234567//@funcName &lt;CStudentList::CStudentList&gt;//@brief &lt;创建空链表&gt;CStudentList::CStudentList()&#123; head = new CStudent("HEAD", 1, 100);//头结点本身的数据并不重要，所以随意填写。 head-&gt;next = NULL;&#125; 2.手动录入信息的构造函数 1234567891011121314//@funcName &lt;CStudentList::CStudentList(int n)&gt;//@brief &lt;创建n个结点的链表，并手动录入信息&gt;//@parameter &lt;n:初始链表结点数目（不计头结点）&gt;CStudentList::CStudentList(int n)&#123; head = new CStudent("HEAD",1,100); head-&gt;next = NULL; for (int i = 0; i &lt; n; i++) &#123; CStudent *newNode = new CStudent(); newNode-&gt;next = head-&gt;next; head-&gt;next = newNode; &#125;&#125; 3.通过数组自动录入信息的构造函数 和上一个差不多。 1234567891011121314//@funcName &lt;CStudentList::CStudentList(CStudent s[],int n)&gt;//@brief &lt;创建n个结点的链表，并自动从数组中获取信息&gt;//@parameter &lt;s:结点类对象数组&gt;&lt;n:数组s的长度&gt;CStudentList::CStudentList(CStudent s[], int n)&#123; head = new CStudent("HEAD", 1, 100); head-&gt;next = NULL; for (int i = 0; i &lt; n; i++) &#123; CStudent *newNode = new CStudent(s[i].getName(),s[i].getSex(),s[i].getScore()); newNode-&gt;next = head-&gt;next; head-&gt;next = newNode; &#125;&#125; 4.通过文件自动录入信息的构造函数 使用到了另一个成员函数open() 123456789//@funcName &lt;CStudentList::CStudentList(const char fileName[])&gt;//@brief &lt;自动从文件中读取信息&gt;//@parameter &lt;fileName:数据来源文件的名字&gt;CStudentList::CStudentList(const char fileName[])&#123; head = new CStudent("HEAD", 1, 100); head-&gt;next = NULL; open(fileName);&#125; 显示链表123456789101112//@funcName &lt;CStudentList::showList()&gt;//@brief &lt;显示整个链表&gt;void CStudentList::showList()&#123; CStudent*p = head-&gt;next; cout &lt;&lt; "姓名\t性别\t成绩" &lt;&lt; endl; while (p != NULL) &#123; p-&gt;show(0);//以一行一记录的形式显示 p = p-&gt;next;//工作指针向后移动 &#125;&#125; 查询结点1234567891011121314151617181920//@funcName &lt;CStudentList::search&gt;//@brief &lt;按照名字查找数据并显示&gt;//@parameter &lt;name:要查找的学生的名字&gt;//@return &lt;找到的记录数目&gt;int CStudentList::search(const char name[])&#123; int num = 0; CStudent*p = head-&gt;next; cout &lt;&lt; "---------查找结果---------" &lt;&lt; endl; while (p != NULL)//遍历链表 &#123; if (strcmp(p-&gt;getName(),name)==0)//找到了需要的信息 &#123; num++; p-&gt;show(0); &#125; p = p-&gt;next; &#125; return num;&#125; 查找删除由于删除结点与查找要删除的结点相对独立，因此将删除结点独立出来一个函数，以便查找删除不同属性的数据。 1234567891011121314//@funcName &lt;CStudentList::deleteNode&gt;//@brief &lt;删除指针指向的链表节点&gt;//@parameter &lt;p:要删除的结点的指针&gt;void CStudentList::deleteNode(CStudent*p)&#123; CStudent*p1 = head, *p2 = head-&gt;next; while (p2 != p) &#123; p1 = p1-&gt;next; p2 = p2-&gt;next; &#125; p1-&gt;next = p-&gt;next; delete p;&#125; 以查找姓名的删除函数为例子： 123456789101112131415161718//@funcName &lt;CStudentList::deleteByName&gt;//@brief &lt;按名字查找并删除第一个符合条件的结点&gt;//@parameter &lt;name:要删除的结点的名字&gt;//@return &lt;是否删除成功(成功返回0，失败返回-1)&gt;int CStudentList::deleteByName(const char name[])&#123; CStudent*p = head-&gt;next; while (p != NULL) &#123; if (strcmp(p-&gt;getName(), name) == 0)//找到了需要的信息 &#123; deleteNode(p); return 0; &#125; p = p-&gt;next; &#125; return -1;&#125; 读取数据123456789101112131415161718192021222324252627282930//@funcName &lt;CStudentList::open&gt;//@brief &lt;从文件中读取数据并以覆盖形式写入链表&gt;//@parameter &lt;fileName:数据文件名&gt;void CStudentList::open(const char fileName[])&#123; //清空链表 CStudent* p = head-&gt;next; while (p != NULL) &#123; delete head; head = p; p = p-&gt;next; &#125; //未删除head //从文件读取数据 ifstream fin(fileName); while (!fin.eof())//end of file &#123; char name[20]; bool sex=0; int score=0; fin &gt;&gt; name &gt;&gt; sex &gt;&gt; score; //利用头插法把数据插入到链表中 CStudent *newNode = new CStudent(name,sex,score); newNode-&gt;next = head-&gt;next; head-&gt;next = newNode; &#125;&#125; 保存数据123456789101112131415161718//@funcName &lt;CStudentList::save&gt;//@brief &lt;将链表存入文件&gt;//@parameter &lt;fileName:保存到的数据文件名&gt;void CStudentList::save(const char fileName[])&#123; ofstream fout(fileName);//打开文件，创建文件流对象 //遍历链表 CStudent *p = head-&gt;next; while (p != NULL) &#123; //将数据存入 fout &lt;&lt; p-&gt;getName() &lt;&lt; " " &lt;&lt; p-&gt;getSex() &lt;&lt; " " &lt;&lt; p-&gt;getScore() &lt;&lt;endl; p = p-&gt;next; &#125;&#125; 析构函数1234567891011CStudentList::~CStudentList()&#123; CStudent* p = head-&gt;next; while (p != NULL) &#123; delete head; head = p; p = p-&gt;next; &#125; delete head;&#125; 菜单菜单比较简单，整个程序主要流程： 显示菜单选项，等待输入选项编号 分支语句，按照不同选项调用链表提供的函数 如果没有选择退出选项就循环 菜单函数示例123456789void menu()&#123; cout &lt;&lt; "========学生信息管理系统========" &lt;&lt; endl; cout &lt;&lt; "1.显示学生信息表" &lt;&lt; endl; cout &lt;&lt; "2.查找学生信息" &lt;&lt; endl; cout &lt;&lt; "3.从文件读取" &lt;&lt; endl; cout &lt;&lt; "4.将数据存入文件" &lt;&lt; endl; cout &lt;&lt; "0.退出" &lt;&lt; endl;&#125; 选项分支示例12345678910111213141516171819202122int main()&#123; int opt = -1; CStudentList list("data.txt"); while (opt != 0) &#123; menu(); cin &gt;&gt; opt; switch (opt) &#123; case 1: system("cls"); list.showList(); break; default: break; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notepad++添加文件关联]]></title>
    <url>%2Fpost%2Fnotepadpp_file_association%2F</url>
    <content type="text"><![CDATA[将一些常用notepad++打开的文件设置为默认notepad++打开的方法 在研究hexo博客的各种功能的时候，总是需要打开配置文件.yml，一开始我是用右键菜单里面的【用notepad++打开】的方式来打开，后来又遇到了各种.ejs,.styl之类的文件也需要用notepad++来编辑，就将这些文件类型的默认打开方式设置为notepad++。 方法一 菜单栏【设置】-&gt;【首选项】 如图选择【文件关联】，找到需要添加的文件类型，如果没有就选择【customize】（自定义）自己输入，然后添加。 方法二 【ctrl+r】打开【运行】输入control（也就是打开控制面板） 小图标查看方式，找到【默认程序】，选择【将文件类型或协议与程序关联】，找到需要的后缀名，选择它的默认程序即可。]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记1简易爬虫]]></title>
    <url>%2Fpost%2Fpython_spider_note1simple_spider%2F</url>
    <content type="text"><![CDATA[学了python语法之后在b站搜索练手的小项目，发现了这个视频：Python实用练手小项目（超简单） 视频里面讲解了一个爬取图片网站图片的小爬虫。后面用到了我还没学的数据库，不过前面的部分是已经学了的，于是我就打算写一个不用数据库的，爬取某个盗版小说内容的爬虫。 声明：本人不会将得到的小说内容作任何商业用途，也请阅读此文章的各位读者遵纪守法，此文章只用作学习交流，原创内容，转载请注明出处。 项目描述爬虫，在我理解中就是模拟人的浏览行为来获取网站上的信息的脚本，爬虫能得到的信息，一般情况下人也有权限可以得到。 盗版小说网站，不需要登录就可以看到小说内容，内容是写死在html文件里面的，通过右键菜单的查看源代码就能够查看到小说内容，很适合拿来练手。 再次声明：本人不会将得到的小说内容作任何商业用途，也请阅读此文章的各位读者遵纪守法，此文章只用作学习交流，原创内容，转载请注明出处。 思路爬虫的思路是向服务器发出请求，并收到服务器回复的数据，接着从获取的数据中取得想要的信息，保存在数据库中。 由于是小说，就直接保存在文本文件当中。 所以分为以下几步： 发出请求 接收数据 提取信息 保存数据 编程原理发出请求和接收数据发出请求需要一个库，名字叫做requests，它是基于python自带的urllib库写的第三方库，差不多就是升级版的意思吧。 要注意是requests，不是request，结尾有个s，确实存在一个不带s的库，注意区分。 可以使用下面的命令进行安装： 1pip install requests pip 是 Python 包管理工具，总之有了这个玩意，你不用管它从哪里下载，在哪里安装，总之就告诉它要安装啥，它就帮你安排得明明白白的。以后会遇到很多这样的东西，比如npm啥的。 命令在cmd里面输就行了，如果电脑上没有这东西就百度一下怎么下载，一般来说安装了python应该就有了。 如果使用的是pyCharm这种IDE，那就可以直接在代码import这个库，等库的名字变红再在旁边找安装按钮，很方便的。 这个库里面有个get函数，是采用get的方式（除此之外还有post方式，学html表单的时候应该有学到）来向服务器发出访问请求，并将获得的数据作为返回值。 1234import requests#省略代码r = requests.get(url)#url是你要访问的网址print(r)#如果输出是&lt;Response [200]&gt;，那么就是访问成功了 此时返回变量是请求对象，要从中获取数据，就需要使用它的两个属性text和content r.text是数据的html形式，r.content是字节流的形式。二者的区别 前者返回文本格式（即二进制格式经过编码后），后者返回二进制格式。后者一般用于图片的保存。 我们需要获取的是文本内容，因此需要前者。 1html=r.text 提取信息我们打开笔趣阁（一个盗版小说网站）的一个小说页面，随便选一章点进去，查看源代码，发现小说的内容是放在一个&lt;div&gt;里面的： 1&lt;div class="content" id="booktext"&gt;小说内容&lt;center&gt;翻页信息&lt;/center&gt;&lt;/div&gt; 其他章节也是如此，所以就可以利用这个规律将其提取出来，用的就是正则表达式。 正则表达式使用正则表达式需要使用一个内置的库re，根据上面的规律可以写出下面的正则表达式： 123456789import rereg = r'&lt;div class="content" id="booktext"&gt;(.*?)&lt;center&gt;'#正则表达式reg = re.compile(reg)#将字符串转换为正则表达式对象，加快匹配速度content= re.findall(reg, html)#返回一个列表，列表项为匹配到的内容if content==[]:#未匹配到小说内容 print("获取失败！")else: content=str(content[0])#将列表转换为字符串 re.compile()函数 编码转换但是我写到这里的时候遇到了一个问题，就是获取到的内容是乱码。一看到乱码就应该想到是编码出了问题。 右键菜单查看网页编码，是GBK编码，需要转换编码。现在的情况是，网页利用GBK的编码来“加密”了小说文本，而我们需要用同样的方式来“解码”。需要用到decode函数 1html=r.content.decode("GBK", "ignore")#转换编码 将获得的二进制数据按照网页原本的编码GBK来解码，就能获取到正确的内容了。 去除分隔字符此时提取到的内容还有这很多HTML实体，比如&amp;nbsp;和&lt;br /&gt;，注意到它们的分布也有规律： 123&lt;div class="content" id="booktext"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;小说内容&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;小说内容&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;……省略……&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;大雪落下，悄然覆盖着这一切。&lt;br /&gt;&lt;center&gt; 除了开头和结尾之外，都是以&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;进行分隔的。 可以利用split()函数将其分割之后重新组合， 也可以使用字符串的替换函数replace() 1content=content.replace("&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;","\n\n ") 保存数据保存在文本文件中就ok了： 123with open(fileName,'w') as fout:#fileName为保存路径加文件名 fout.write('\n\n=====================\n\n' + fileName + '\n\n=====================\n\n') fout.write(content) 获取单章节内容代码12345678910111213141516171819202122232425262728293031323334353637import requestsimport reimport osdef getNovelByURL(url,fileName): ''' :param url: 网页的url :param fileName: 保存数据的文件的名字 :return: -1为失败，0为成功 ''' #筛选文件名内非法字符 #调试的时候前面几百章都行突然一章不行，发现是因为章节名字里面有非法字符 reg=r'[\/:*?"&lt;&gt;|]' fileName=re.sub(reg,"",fileName)#利用正则表达式去除非法字符 # 获取网页 r = requests.get(url) html = r.content html = html.decode("GBK", "ignore") # 获取网页中小说内容 reg = '&lt;div class="content" id="booktext"&gt;\n&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(.*?)&lt;br /&gt;\n&lt;center&gt;' reg = re.compile(reg)#预编译 content = re.findall(reg, html) #保存到文件 if content==[]: print("获取失败！") return -1 else: content=str(content[0])#转换为字符串 content=content.replace("&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;","\n\n ") with open(fileName,'w') as fout: fout.write('\n\n=====================\n\n' + fileName + '\n\n=====================\n\n') fout.write(content) print("成功爬取（&#123;&#125;），存储在&#123;&#125;".format(url,os.path.dirname(__file__)+'/'+fileName)) return 0 获取全部章节内容的思路盗版小说网站章节的url有个规律，就是url的最后一串数字是连续的，照这个规律，知道第一章的url，就可以获得后续章节的url。于是我着手写这么个函数： 123456789def getNovelByIndexInc(url, number=1): ''' 此函数用于通过已知的起始url来获取仅有尾部索引不同且连续的一系列网页内的小说， 不连续时会跳过获取失败的网址，不过有可能连续几千个网址都是无效网址，所以慎用此函数 或改用getNovelByContentPage函数 :param url:起始章节的url :param number: 要获取的章节数 :return:无 ''' 从我写的注释里面也可以看出，我失败了。 一开始的一百多章还是没什么问题的，只有偶尔几个网址是无效网址，但是后面爬取的时候等了十分钟还没爬取到下一章，一直输出“无效网址”，我查看了那断片的两个连续章节之后才发现，最后的一串数字差了几万。不会是因为作者断更吧！ 这种方式不可靠，还是换一种方式。 那么要如何可以改进呢？ 我写了另一个函数： 1234567def getNovelByContentPage(url,path='novel'): ''' 通过获取目录页面链接与标题，进一步调用获取已知链接页面的函数来保存页面内容 :param url: 书籍目录页面 :param path:保存路径，默认为同目录下的novel文件夹 :return:-1为失败，0为成功 ''' 网站的书籍页面会有一个目录，而目录下隐藏的就是我需要的全部章节的链接呀！ 这个函数用到的内容上面也都讲到了，就直接放代码吧。 获取全部章节内容的代码1234567891011121314151617181920212223242526272829303132333435import requestsimport reimport osdef getNovelByContentPage(url,path='novel'): ''' 通过获取目录页面链接与标题，进一步调用获取已知链接页面的函数来保存页面内容 :param url: 书籍目录页面 :param path:保存路径，默认为同目录下的novel文件夹 :return:-1为失败，0为成功 ''' # 获取网页 r = requests.get(url) html = r.content#获取网页二进制内容 html = html.decode("GBK", "ignore")#转换编码 # 获取网页中小说内容 reg = '&lt;dd&gt;&lt;a href="(.*?)" title="(.*?)"&gt;.*?&lt;/a&gt;&lt;/dd&gt;'#获取链接和标题 reg = re.compile(reg, re.S) info= re.findall(reg, html) #由于是分组匹配，得到的列表中每个元素的[0]是链接，[1]是标题 #保存到文件 if info==[]: print("获取章节目录失败") return -1 else: if not os.path.exists(path):#检查目录是否已经存在 os.makedirs(path) for i in info: realpath=path+"\\"+i[1]+".txt" if os.path.exists(realpath):#避免重复爬取 continue else: getNovelByURL(i[0],realpath)#调用获取单页面内容的函数 return 0]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为博客增加访问统计]]></title>
    <url>%2Fpost%2Fhexo_visit_count%2F</url>
    <content type="text"><![CDATA[用CNZZ统计网站访问量 我用的主题是shana，网站统计的配置部分是这样的： 12345# 网站统计# 站长统计 填写id# eg: # CNZZ: 123456789CNZZ: 百度了一下发现CNZZ和百度统计都可以统计网站访问量。当然想统计呀，这样就更有动力来写了。 尝试了百度统计一个多小时之后还没弄好，我就开始试CNZZ，毕竟主题的作者直接写在配置里面了，还是按照规矩来吧。 CNZZ不是中国站长（cnzz.cn）那个，而是友盟（cnzz.com），我一开始进的是中国站长……找了老半天统计功能才发现进错网站了。 步骤 注册一个cnzz账号 填写网站信息 复制统计代码 粘贴统计id到配置文件 粘贴统计代码到需要统计的页面开头 粘贴代码到哪里又是个问题，根据前面尝试弄百度统计的经验，在主题文件夹下的\layout\_partial内是用于生成页面的代码，摸索一阵后发现应该粘贴到head.ejs里面以达到生成在页面前面的效果。 一开始没显示“站长统计”的字样我以为是无效，甚至还去issue里面问shana的作者怎么弄。 后来发现，原来是shana主题在切换背景图片的时候会掩盖字样……好吧是我太心急了。 今日收获hexo的页面生成方式theme\&lt;themeName&gt;\layout\_partial下的文件都是.ejs文件，应该是“扩展的js”文件，用于生成相应的页面。 例如head.ejs中专门存储生成html文件的&lt;head&gt;部分 more文章一开始是全部展开的，浏览起来比较难受，查了之后发现其实只需要在文章中加上一个标记就可以折叠。 .md文件里面是下面这种结构： 12345显示出来的文章提要&lt;!--more--&gt;正文]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo部署博客的过程记录]]></title>
    <url>%2Fpost%2Fhexo_deploy_log%2F</url>
    <content type="text"><![CDATA[建立Hexo博客的相关知识整理成的笔记，不太全面。 缘起这部分算是年终总结一样的东西吧。 在2018年8月底的时候，我还是对域名、服务器等名词没有了解的一个web小白，那时一个朋友在群里发了一个非常好看的个人博客，我一下子就被吸引了，羡慕但是又没有能力自己弄，感觉太难了。当时我只学过一点HTML和CSS，javaScript还未怎么学，而且已经很久没有练习过，已经忘得差不多。 那个朋友东给我发了一个《基于CentOS搭建WordPress个人博客》的页面，东说他想弄，已经租了个腾讯云服务器，问我有没有兴趣。我当时还是蛮犹豫的，对于我来说难度还是很大的，那个网页上并不是个教程，说的内容我大部分看不懂。不过我还是想整一个的，于是尝试去学。 我属于那种“如果不能基本上理解一个概念，那么就会完全拒绝相关的知识输入，即便已经记住了也不会长久”的学习类型，而且以前又比较自闭，不想问别人，只在网上查找已有的问题答案，所以学习起来特别困难。 买了域名，备了案，租了学生价服务器，照着教程《新手如何用腾讯云服务器搭建一个wordPress博客-简书》鼓捣了好久终于弄出来一个wordPress博客。但我当时仅仅是“知其然而不知其所以然”，并不认为自己学到了什么，弄好了主页就一直搁置在那里，感觉心疼租服务器的钱但是又没办法。开学了又有很多事情要忙，大学并不像高中的时候想象的那么悠闲。 直到一个学期结束我才考虑起开始重新弄个博客。经过了一个学期，我学会了更多的东西，把上个暑假的建博客的流程给理解了应该没什么问题。 我开始整理以前编程留下的笔记。以前使用的是vnote，但是我觉得界面不太好看，而且功能大多用不上，遇到问题百度也搜不到，碰巧它这时又不知道出了什么毛病，于是换成了Typora，把笔记重新筛选了一遍。分类尽可能少，渐渐地开始“一元化”笔记。 随后又想起了以前只学了一点的git。没有一次性学完它，导致我没有去用它，顶多只是使用网页版github上传一下代码点亮小绿块让自己爽一下，也搁置了很久。说起来我真是喜欢半途而废。 不如把这整理好的笔记传到github上面备份吧，感觉比网盘备份b格高。刚好前几天学Python的时候找到了廖雪峰的git教程，这让我有些后悔没有听另一个朋友朱的推荐。 整github的时候又发现了github page的功能，想起来github也可以搭博客，所以今天（2019-2-6）就研究了一整天搭好了这个博客。挑了个和我以前羡慕的个人博客相同的主题，美滋滋，成就感爆棚。 好了，接下来我来分享一下我是如何搭建这样一个博客的。不保证零基础能看懂。 Hexo——一个博客框架和WordPress差不多，都是用来搭建博客的一个框架。但是问题来了—— 框架，又是啥？ 自学计算机类的知识最大的问题就在于百度到的东西需要各种各样的前置知识，很难一下子理解那是什么意思，越听越迷糊。不管在看这篇博客的你知不知道框架的意思，反正上个暑假的我是不明白的。而且这是简称，光百度一个“框架”好像又搜不到明确的定义。 这个障碍一直妨碍着我对bootstrap、vue、MFC、QT等框架的准确理解，后来我才知道，软件框架到底是个啥 简而言之，在我的理解里面，框架，就是可以复用的代码，就是“不要重复造轮子”中的“轮子”，就是别人已经写好的封装了各种复杂API的库。框架可以帮你完成一些基础语法本身也可以完成的事情，让你不必在建房子的时候从烧砖开始，而是可以解放思维直接开始画楼房设计图。 Hexo，就是一个可以帮助你生成静态网页的一个工具，所有的核心功能比如打标签归档加时间，以及界面美化工作都帮你做好了，你可以专注于博客内容的创作，而不必学习如何“烧砖”（写前端代码）。网上搜索“hexo”，可以找到它的官网。hexo的官网文档做得非常好，不仅提供准确的中文版文档，还附有视频，让我学得非常之愉快。 不过作为一个“楼房设计工程师”，你还是需要一些其他的帮手来帮助你“建房子”。 在Hexo官网的文档里面有详细的教程教你如何安装必须的东西，我在这里只讲一些理解性的东西，详细的指令不多讲。 安装Hexo的时候你需要俩工具： node.js git node.jsnode.js-百度百科 Node.js 是一个让 JavaScript 运行在服务端的开发平台，实质是对Chrome V8引擎进行了封装。 引用自node.js和JavaScript的关系-博客园 JavaScript是一门语言 node.js不是一门语言，也不是一种特殊的JavaScript方言 - 它仅仅就是用于运行普通JavaScript代码的东西 所有浏览器都有运行网页上JavaScript的JavaScript引擎。Firefox有叫做Spidermonkey的引擎，Safari有JavaScriptCore，Chrome有V8 node.js就是带有能操作I/O和网络库的V8引擎，因此你能够在浏览器之外使用JavaScript创建shell脚本和后台服务或者运行在硬件上 个人理解：node.js是javaScript的解释器 为啥要安装它呢？应该是为了使用node.js的npm（Node Package Manager，是一个node.jS包管理和分发工具）,可以理解为一个安装程序，可以给你安装官方已经整合好的包。当然其他作用我也不知道。 如果已经有git和node.js，直接使用下面指令进行安装： 1`$ npm install -g hexo-cli` 虽然前面有个linux系统的shell的命令提示符，但是安装好node.js之后用windows系统的cmd里面也是可以用的。至于打开cmd，win+R打开运行窗口输入“cmd”，回车就出现了。记得输入时不要输入前面的$符号，那是命令提示符。 git——版本控制系统git的安装和使用就不多说了。用于将Hexo生成好的页面给推送到github这个远程库里。至少要知道git的一些基本概念。 Hexo的使用这Hexo安装好之后你可以在cmd使用它的指令。 初始化初始化Hexo的指令（命令提示符不写了，下同）： 1hexo init 指定目录（省略则初始化当前目录） 初始化hexo之后会在你指定的目录生成一大堆文件，这些文件和文件夹是从它的官方github库里面clone下来的，这也是一开始要下好git的原因。 比较重要的几个文件是： _config.yml 配置文件，使用YAML来写的数据文件 scaffolds 模板文件夹，存放新文章的模板 source 文章，图片，草稿等资源都放在这里 themes 主题文件夹，Hexo根据主题生成静态页面 新建文章1hexo new [layout] &lt;title&gt; 生成静态页面1hexo generate 或者简写成 1hexo g 前文说过，Hexo是用来帮助你生成静态网页的一个工具，就是用这个指令。这个指令将目前编写好的文章以及主题等东西给包装好，生成用于上传到你的网站上（这里我们用github page）的网页。至于此命令的详细说明，请看Hexo文档。 部署网站说实话我在看文档的时候没看懂“部署网站”是啥意思，后来知道了，这就是将hexo generate生成的静态页面推送到你的github库里面去的意思。 指令是： 1hexo deploy 也可以简写成： 1hexo d deploy可以与generate共同简写成： 1hexo d -g 或者 1hexo g -d 一个意思，都是先生成静态页面，再部署网站。 本地测试1`$ hexo server` 在自己电脑上运行服务器来查看博客的效果，默认情况下，访问网址为： http://localhost:4000/ 其余指令我目前还没用到，详情请见Hexo文档 github page在github仓库的setting里面，有一栏叫做github page，在其中的source选项内选择作为数据源的分支，一般将博客部署在master分支，所以选择master作为数据源。 你可以选择两种方式来给你用来存放博客数据的仓库起名字，第一种就是你百度经常看到的：你的用户名.github.io的形式，这种形式会让你在选择好数据源之后提示： 1Your site is published at https://你的用户名.github.io/ 然后你可以使用给出的链接来打开你的博客，点击链接，会默认打开你数据源分支内的index.html文件作为主页，如果没有这个文件就会404：找不到页面，当然，Hexo会帮你生成好index.html，只要你把生成好的页面给push上github就可以。如果你知道包含哪些文件的话，自己手动上传应该也ok。 有了这种方式，其实你甚至可以不需要Hexo，自己写html页面也能做一个博客，不过这样就像前文说的从“烧砖”开始建楼了。 第二种起名方式就是不按照第一种来，随便起，比如用户名是HaneChiri，创建的仓库名叫blog,那么选择完数据源分支之后呢，得到的提示可能是： 1Your site is published at https://hanechiri.github.io/blog/ 这样需要写的网址就会长一些，要加上仓库名。试了一下这样是可以的。我的两个仓库就分别使用了以上两种方式。 markdown的图片这里的文章使用的是markdown语法，一个比较容易学习的标记语言，可以让你手不离键盘地完成排版，我现在就是在用markdown来写.md文件，然后放进source文件夹的_post子文件夹里面，之后再上传。 markdown可以方便的插入图片和超链接。但是图片一般来说是利用相对路径放在.md文件的附近的，生成静态页面的时候图片的路径又会被打乱，导致图片显示失败。 Hexo文档里面提供了几种方式来插入图片，比如插件。但是那种方式无法实时预览，而且难弄。 所以干脆使用外部图片链接，在github上面再建立一个仓库用来存放图片，提供链接给博客使用。 要这样使用的前提是去开启github page这个设置。 比如用户名是HaneChiri，创建的仓库名叫blog_images，那么在这个仓库根目录下的图片avatar.jpg的链接就是 1https://hanechiri.github.io/blog_images/avatar.jpg 而不是 1https://github.com/HaneChiri/blog_images/avatar.jpg 后者是浏览编辑这个图片的链接，而不是图片本身。 上传之后无法访问这个链接也不要急，等几分钟就可以了。 参考教程 【持续更新】最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析-遇见西门 步骤总结由于网上教程很多，我在这里只是简单把我部署博客的步骤总结一下： 开一个github空仓库（注册和新建仓库应该不用多说） 在一个本地空文件夹内初始化hexo 此文件夹内，与远程库建立关联（其实这一步可以不必，不过以后可能用得到，先弄着吧） 给_config.yml文件内deploy属性设置好type（: git，记得冒号后面有个空格）、url（github仓库的链接）和branch（推送到的分支，一般用master） 修改其他配置比如title、author、new_post_name、language、post_asset_folder 安装一个git部署的东西npm install --save hexo-deployer-git 生成并在本地测试页面效果 生成并部署网站hexo d -g 新建，编辑文章然后重复上一步]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello-world]]></title>
    <url>%2Fpost%2Fhello_world%2F</url>
    <content type="text"><![CDATA[Hello World这是利用Hexo和github建立的个人博客。使用了hexo的二次元主题夏娜 我是憧憬少，初めまして，よろしくお願いします！ 1234567//代码高亮测试#include &lt;iostream&gt;using namespace std;int main()&#123; cout&lt;&lt;"hello world"&lt;&lt;endl;&#125; $$\Sigma$$]]></content>
  </entry>
</search>
