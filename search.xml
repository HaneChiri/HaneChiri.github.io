<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[python读取ini文件失败的原因]]></title>
    <url>%2Fpost%2Fpython_read_ini_No_section%2F</url>
    <content type="text"><![CDATA[尝试使用python的configparser来读取ini配置文件，但是遇到了No Section的错误。 最终发现其实是路径出了问题。 问题描述初始代码简化之后是： 1234567from configparser import ConfigParserif __name__ == '__main__': config=ConfigParser()#创建配置对象 config.read('test.ini')#读取配置文件 result=config.get(section='test',option='name')#读取test下的name print(result) 同目录的test.ini的内容如下： 12[test]name = tom 但是运行出现了configparser.NoSectionError: No section: &#39;test&#39;的错误 原因探索经过单步调试后发现并没有读取到文件的内容，猜测可能是没有找到文件。 以前在import自定义模块的时候遇到过类似的问题，当时的解决方法是把当前工作路径设置为正在执行的文件所在的路径。 解决方案 使用绝对路径 将当前工作路径改为当前文件路径，再使用相对路径 第二种方法的代码如下： 12345678910from configparser import ConfigParserimport osif __name__ == '__main__': curpath=os.path.dirname(os.path.realpath(__file__)) filename=os.path.join(curpath,"test.ini") config=ConfigParser()#创建配置对象 config.read(filename)#读取配置文件 result=config.get(section='test',option='name')#读取test下的name print(result)]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[excel实现周总结签到积分制]]></title>
    <url>%2Fpost%2Fexcel_weekly_sign%2F</url>
    <content type="text"><![CDATA[我在自己一个学习群里设定了一个周总结制度，这篇博客记录一下如何使用excel函数来实现计算打卡相册的积分。这里其实我用的是wps表格，但是函数一样，所以我就分类在excel里面。 简介每周日，每个人在群聊天发一个周总结，内容是自己这周学习了什么，没有限制，只是给大家一个自我反省的机会。 如果没有可以写的东西，那么也在群里面报备，方式为在群聊天中说：“本周无总结”或者别的能表明这一事实的话。别有压力，只是回复一句话的功夫。 如果没有报备也没有在截止之前发周总结，将会被艾特提醒。可以在下一周总结之前补。 为了方便描述，下文把发送周总结称为“签到” 积分规则 如果本周签到了，积分=原本积分+正调整参数 如果未签到，积分=原本积分+负调整参数 如果补签到，积分=原本积分 签到登记表样例 成员ID 昵称 正常签到次数 周总结积分 week1 week2 week3 week4 week5 week6 1 憧憬少 1 1 1 2 听星缘 1 1 1 3 简白 1 1 1 4 HUST 1 1 1 5 咸鱼米 1 1 1 1表示已签到，-1表示未签到，补签改为0 编写公式正常签到次数即计算1出现的次数（补签不算），如果用之前的SUMIF函数就是： 1=SUMIF(对应成员的签到区域,1) 但是我又查到一个更适合的函数：COUNTIF 参数和SUMIF差不多含义，写成公式也是一样 1=COUNTIF(对应成员的签到区域,1) 但是前者只能计算1出现的次数，如果计算-1出现的次数就不行了。 周总结积分比较简单，不赘述了。 1=COUNTIF(E2:ZZ2,1)*积分规则!$C$3+COUNTIF(E2:ZZ2,-1)*积分规则!$D$3*(-1)]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[excel实现打卡相册积分制]]></title>
    <url>%2Fpost%2Fexcel_clock_in_album%2F</url>
    <content type="text"><![CDATA[我在自己一个学习群里设定了一个打卡相册制度，这篇博客记录一下如何使用sumif函数来实现计算打卡相册的积分。这里其实我用的是wps表格，但是函数一样，所以我就分类在excel里面。 规则说明群员可以申请建立打卡相册，需要自己下载群文件中的登记表填写相关信息，然后就可以创建群相册，在群相册描述里面写上打卡内容。 相册状态 相册状态：正在进行、放弃、失败、归档 如果连续三天未打卡，管理员就删除相册，并在登记表内将相册状态设置为“失败”。 对于有期限的相册，比如打卡目标是“两周读完《xxx》”，那么在结束日期时，可以将其状态设置为“归档”。相册资源回收（删除或改作他用），避免资源闲置。若持续时间大于等于一百天，则可以选择保留相册。（可以给其他群员作榜样） 对于没有期限的相册，比如“每天背单词”，那么在创建时间满三十天后就可以选择“归档”（三十天应该够养成一个小习惯了） 相册删除后，相册记录还会保留在登记表里面 积分计算 创建相册不需要积分，但是“放弃”或“失败”每个会扣除5积分 一个成员的打卡相册总积分=他所有相册的积分之和 单个相册的积分： 若相册状态是“正在进行”，则积分=持续天数1*正调整参数=正调整参数*（当前日期-创建日期） 若相册状态是“归档”，则积分=正调整参数*持续天数2=正调整参数*（结束日期-创建日期）,目前参数为0.5 若相册状态是“放弃”或“失败”，则积分=负调整参数，也就是扣除积分，目前参数为-5，即扣除5积分 相册字段 字段 描述 相册ID 这个手动赋值为：最大的相册ID+1 申请人昵称 可以写真名或者自己的群名片昵称，只要大家能通过这个知道是谁即可 相册名称 无特别要求，不过最好写明昵称和目的，例如：憧憬少的英语流利说APP打卡 相册目标描述 描述你要通过这个打卡相册达到的目标，例如：每天读口语10分钟 如何判断目标完成 上传到打卡相册的图片应当满足怎样的要求，例如：每天在相册内上传一张可以表明读了10分钟的截图 相册状态 目前用到的状态：正在进行，放弃，失败，归档（仅留表中记录，相册本身删除，若打卡满100天可选择保留） 创建日期 用于计算持续天数的字段 结束日期 归档日期，或有期相册结束日期。 持续天数 除了正在进行状态，其他状态都停止增加持续天数 相册类型 目前的类型：无期（未规定期限，满30天可以选择归档），有期（规定了完成期限，若期限内完成则归档，未完成则为失败） 打卡相册积分 利用表格的自动填充功能复制上一个相册的公式 编写公式计算相册持续天数相册持续天数有两种情况，一种是“正在进行”，一种是其他状态，只有“正在进行”的打卡相册会继续计算天数。 也就是说： “正在进行”的相册的持续天数=今天日期-创建日期 其他状态相册的持续天数=结束日期-创建日期 因此需要一个IF判断。 IF函数的语法是： 1IF(条件，条件为真时的返回值，条件为假时的返回值) 公式如下（中文处替换为对应的单元格） 1=IF(相册状态=&quot;正在进行&quot;,TODAY()-创建日期,结束日期-创建日期) 计算打卡相册积分根据上述规则，我们需要用IF函数判断一下相册状态。 这里还用到了一个函数OR excel里面的与或非不是用逻辑运算符的，而是用函数。 公式如下： 1=IF(相册状态=&quot;正在进行&quot;,1,0)*(积分规则!$C$2)*持续天数+IF(相册状态=&quot;归档&quot;,1,0)*(积分规则!$C$2)*持续天数+IF(OR(相册状态=&quot;失败&quot;,相册状态=&quot;放弃&quot;),1,0)*(积分规则!$D$2)*(-1) 其中，积分规则!$C$2代表的是我在另一个名为“积分规则”的表中的C2格中设置的一个正调整参数。积分规则!$D$2同理。 计算个人总积分一个成员可以有多个相册，因此需要将他所有的相册的积分相加。 相加可以使用SUM函数，来将已知区域求和。 例如现在的情况是这样的： 申请人昵称 相册名称 打卡相册积分 憧憬少 憧憬少的英语流利说打卡 18 简白 简白的英语打卡 12 咸鱼米 米米的啃书打卡 12 咸鱼米 米米的每日练习 5 H.U.S.T. H.U.S.T.的小说练笔 3 米米有两个相册，她的积分就是12+5=17，相加的格子不确定，要如何用公式计算她的积分呢？ 我查到了SUMIF这个函数，也就是“条件相加”，格式如下： 1SUMIF(条件区域,求和条件,[实际求和区域]) 它的官方文档链接 条件区域：也就是要按条件计算的单元格区域。不太好理解，我的理解是，这个函数对于“条件区域”内符合条件的单元格进行求和。 求和条件：定义进行求和的单元格需要满足的条件。例如：32、”&gt;32”、B5、”32”、”苹果” 或 TODAY ()。任何文本条件或任何含有逻辑或数学符号的条件都必须使用双引号 (“) 括起来。 如果条件为数字，则无需使用双引号。 实际求和区域：如果省略，则将条件区域当作实际求和区域。 在这里，条件区域是“申请人昵称”，实际求和区域是“打卡相册积分”，求和条件是要计算积分的成员昵称。这样我们就可以将某个成员的所有相册数据所在的那几行给筛选出来，再将这几行的打卡相册积分相加，得到这个成员的总积分了。 某成员打卡相册总积分计算公式： 1=SUMIF(打卡相册登记表!B:B,某成员昵称,打卡相册登记表!K:K) 这里的B:B和K:K就分别对应了“申请人昵称”和“打卡相册积分”这两列。]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WIN10共享文件夹]]></title>
    <url>%2Fpost%2Fwin10_share_folder%2F</url>
    <content type="text"><![CDATA[这是我以前写的第一篇博客的补档，由于图片太多于是就发在了CSDN，现在不愁图片的问题了，于是就在整理电脑文件时把这篇博客在个人博客这边发一下。 这是在CSDN的第一篇博客，也是我第一篇正式的博客。 我们的linux老师上课时用到了共享文件夹，于是我就百度学习了一下。 来写一下刚刚学到的共享文件夹的方法。 共享方法 首先右键你想要共享的文件夹，【共享】-&gt;【特定用户】 2.在选择框里面选择Everyone，接着点击旁边的【添加】 3.调整权限后，点击【共享】即可 4.共享完成 别人进入共享文件夹的方法1.你可以复制系统给你的链接给局域网内（我只试过局域网）的别人，让他复制到文件资源管理器地址栏 2.或者找到资源管理器最左下角的【网络】，让他点进去就是了。 点进去之后的效果是这样： 然后你就可以用这个文件夹和局域网里的各位来分享文件了。]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个好用的图床管理工具PicGo]]></title>
    <url>%2Fpost%2FPicGo_imgur%2F</url>
    <content type="text"><![CDATA[先前给hexo博客插图片都是把图片commit到github上再手动构造链接，比较麻烦，又不想把图片直接放在博客所在的库。 这次找到了一个好东西：PicGo 测试一下图片： 很方便的一个工具，简单地截图然后上传剪切板图片，它就自动帮我上传到github上我准备好的库里面，然后把markdown格式的图片引用复制到我的剪切板里面。 具体如何下载安装和使用，它的官方文档肯定比我写的详细，不赘述。 我使用的是github图床（当然，它还支持别的图床），提一下与它相关的一个比较重要的插件。 PicGo插件：github-plus它的github库链接 它的作用是，让本地的PicGo相册和github库的内容同步。 PicGo本体只负责上传，不负责删除。我在发现上传错图片，在PicGo相册中删除了图片之后，发现github上面并没有删除这些图片。这是个比较严重的问题。而手动删除的时候很麻烦，要clone到本地，删除之后再提交。 好在找到了这个插件。 这个插件的功能： 将删除操作同步到github 从github上把图片同步到本地相册，从而可以复制链接]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python相对路径是相对于哪里]]></title>
    <url>%2Fpost%2Fpython_relative_path%2F</url>
    <content type="text"><![CDATA[在学习scrapy时，保存数据到文件的时候，发现一直出现“找不到这样的文件或文件夹”的错误，最后发现是因为python的相对路径。 问题描述学习scrapy时，编写pipeline来将数据保存到文件当中，代码如下： 123456class NovelPipeline(): def process_item(self, item, spider): with open('Novel/'+str(item['title'][0])+'.txt','w') as f: for p in item['content']: f.write(p+'\n') return item 看着爬取时调试信息飞快闪过（爬取的东西有点多），却没有发现我准备好的Novel文件夹里面多出文件，连忙把爬虫停下来。发现出现了“找不到这样的文件或文件夹”的错误。 分析过程查看日志信息，发现文件名是对的，但是为什么不行呢？ 于是我在pipelines.py里面写了测试代码： 12with open('Novel/'+'文件名'+'.txt','w') as f: print(1) 发现同样的错误。 我把前面的文件夹去掉，也就是： 12with open('文件名'+'.txt','w') as f: print(1) 发现文件生成在了我的工作目录下！ 这个时候我才注意到相对路径的问题。 当前目录是这样的（略去无关文件）： learn_scrapy 文件名.txt practice practice pipeline.py 我本来以为这个相对路径会使得文件生成在pipelines.py的同级目录下，但是却生成在了我的VScode的工作文件夹？ 我回忆起java课时老师写错相对路径导致无法显示图片的问题。那时也是需要相对当前项目的根目录来写相对路径的。我认为这是eclipse的特性。 会不会这个也是vscode的特性？ 于是我搜索“python 相对路径”，找到了和我遇到类似问题的朋友：vscode中使用python相对路径问题?-知乎 我的工作目录是/Work 我在工作目录中创建了文件/Work/Program/main.py 并且运行main.py 生成了 file.txt文件 123&gt; with open('file.txt','w') as f:&gt; f.write('HelloWorld')&gt; 我以为file.txt在/Work/Program路径下，和创建它的main.py在一个路径中 结果file.txt这个文件却在/Work路径下面（/Work/file.txt），而不是我所期望的/Python/Program路径下面 所以应该怎么配置，或者安装什么插件，能让py创建的文件在自己的相对路径下，而不是直接跑到了工作路径那里？ 这个问题怎么解决啊，困扰了我好久，而我又比较喜欢vscode的界面不想放弃它。求解答！ 看了回答之后我继续搜索，终于解决了困惑。 解决方案参考链接： Python里使用相对路径的坑-简书 Python里写这种相对路径, 是相对于终端的当前目录的. 解决办法是, 获取脚本所在目录, 构造绝对路径]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[调整博客分类]]></title>
    <url>%2Fpost%2Fadjust_categories%2F</url>
    <content type="text"><![CDATA[目前个人博客内的分类不太合理，于是重新调整分类 调整前分类 c++ java python 工具 日志 流程 算法 标签 awt c++ git hexo java mysql notepad++ python scrapy stl 信息检索 小游戏 日志 爬虫 算法 项目 题目 分析c++、java、python等大类很明显和标签重叠了。 我目前对二者的理解： 分类表明一个事物是什么； 标签表明一个事物有什么。 按照文章区别于其他类型文章的特征来分类。 新建一个标签前，要考虑这个标签的可重用性，比如c++、java这类标签肯定会经常用到，但是notepad++这类基本只用一次了，所以将它归到IDE这个标签内。stl和awt这类标签不常用，可以删去。 调整后分类 分类 内容 工具使用 工具的获取（下载安装）、使用，类似教程 编程语言 记录遇到的一些语法问题 项目总结 主要记录过程及遇到的问题与方案，包括一些感想和体会 算法模型 记录一些算法相关的题目以及概念 过程记录 记录解决方案和过程，记录经验总结，简化版的项目总结，侧重过程 调整之后，分类比之前清晰多了，我写新的博文，就知道应该归类到哪里，找的时候也知道应该到哪里去找了。 标签减少了一些标签 IDE ：分得比较宽泛，连notepad++都算进去 c++ git hexo java mysql python scrapy 框架 爬虫]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo日记本]]></title>
    <url>%2Fpost%2Fhexo_diary%2F</url>
    <content type="text"><![CDATA[打算从纸质日记转到电子日记。 之前是一个月的日记放在一个markdown文件里面，每天一个一级标题。昨天突发奇想，为啥不用Hexo来搭建日记本呢？它本来就是用来写博客（blog网络日志）的呀。 于是今天就来搭建hexo日记本 前言本文只分享设计思路以及步骤，不提供详细教程，详细教程可以看这个：【持续更新】最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析-遇见西门 优点利用hexo搭建日记本有很多优点： 好看，并且可以随时换主题 比我之前的方式更加地将日记格式化，便于以后编写脚本来管理 可以在scaffolds里面设置日记模板 可以设置分类与标签 有的主题甚至能搜索文章 需求 不联网：这个日记本和我部署到github上面的博客有些不一样，因为这个是比较隐私的，我不打算放在网上，仅利用移动硬盘备份。并且看日记都在本地，不使用外链图片，以免断网的时候无法查看 功能少：并且不需要评论，阅读计数等功能，起到的只是一个阅读器的作用。 重美观：需要能够方便地切换主题。 无需侧边目录：因为我打算一篇只记录一天的内容，写不了太多，标题层级不会太多。 写日记要便捷 步骤参考链接： hexo官方中文文档 初始化hexo刚开始的时候我不是很清楚hexo在一台电脑上是否可以搭多个博客，后来发现，hexo的每个博客其实就是一个“项目”，那些命令得在已经搭建博客的文件夹里面才能使用，而不是我之前想的“全局命令”。 首先初始化： 123hexo init &lt;folder&gt;cd &lt;folder&gt;npm install 然后就可以通过以下命令查看本地内容了： 1hexo server 或简写为 1hexo s 全局设置在博客根目录下的_config.yml内配置 标题相关1234567title: 日记subtitle:description:keywords:author: 憧憬少language: zh-CNtimezone: permalink这个设置会决定你的文件最后渲染之后放在哪里。 利用hexo g来渲染markdown文件，它会将渲染好的html文件放在public目录下，部署到github时，上传的就是这个文件夹里面的内容。 比如最开始的设置： 1permalink: :year/:month/:day/:title/ 则会将最开始的hello-world.md示例文章给生成在public\2019\06\26\hello-world这个文件夹当中。 我觉得一天的内容单独放一个文件夹有点不太合适，而一年的内容全部放在一个文件夹的话，三百多个文件也不好管理，所以按照一个月的内容放在一个文件夹内的规则，将这个设置改成： 1permalink: :year/:month/:title/ new_post_name新建文章的文件名，因为日记按照时间管理比较方便，因此在文件名中加入日期 1new_post_name: :year-:month-:day-:title.md # File name of new posts 图片问题我之前一直是将图片上传到github的一个repo上面然后使用链接的，看了文档之后才发现原来还有更简便的方法！ 方法一 外链首先开启仓库的github page这个设置。 比如用户名是HaneChiri，创建的仓库名叫blog_images，那么在这个仓库根目录下的图片avatar.jpg的链接就是 1https://hanechiri.github.io/blog_images/avatar.jpg 而不是 1https://github.com/HaneChiri/blog_images/avatar.jpg 后者是浏览编辑这个图片的链接，而不是图片本身。 上传之后无法访问这个链接也不要急，等几分钟就可以了。 日记本不能使用这个，因为我需要在不联网的时候也能看。 方法二 资源文件夹来自资源文件夹-hexo官方文档 资源（Asset）代表 source 文件夹中除了文章以外的所有文件，例如图片、CSS、JS 文件等。比方说，如果你的Hexo项目中只有少量图片，那最简单的方法就是将它们放在 source/images 文件夹中。然后通过类似于 ![](/images/image.jpg) 的方法访问它们。 早知道认真看文档了，插图片就简单多了。 这个是将图片放在source/images中，而我将Typora设置成将图片自动保存在同目录下的images中，编辑之后只要将这个文件夹内图片给复制到前者所述文件夹，就可以在编辑以及渲染时都看到图片了。 方法三 下载插件Hexo文章中插入图片的方法-CSDN 我不需要每个文章的图片分开管理，这样会导致source\_posts\内有太多没用的空文件夹，因此我使用方法二，读者可以选择适合自己的方法。 主题为了防止和联网的博客弄混（毕竟一旦将日记上传上去，repo里面就会留下痕迹，哪怕删掉也看得到，除非删repo），我打算换个别的主题。 找到了几个心仪的： Gal ：galgame。和我第一次用的夏娜 shana主题是同类型的 Sakura ：樱。贼好看，功能蛮多的样子 One ：单页面。每个文章都可以配图，上面的几个也是 但是考虑到个人的一些因素，还是先用着Next吧，反正可以换。 外观Next主题有四种外观（scheme），在配置文件（themes\next\_config.yml）中可以找到并修改： 12345# Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini 侧边栏社交链接最右边的||后面跟着的是文字边上显示的图标 12345678910111213social: GitHub: https://github.com/HaneChiri || github #E-Mail: mailto:yourname@gmail.com || envelope #Weibo: https://weibo.com/yourname || weibo #Google: https://plus.google.com/yourname || google #Twitter: https://twitter.com/yourname || twitter #FB Page: https://www.facebook.com/yourname || facebook #VK Group: https://vk.com/yourname || vk #StackOverflow: https://stackoverflow.com/yourname || stack-overflow #YouTube: https://youtube.com/yourname || youtube #Instagram: https://instagram.com/yourname || instagram #Skype: skype:yourname?call|chat || skype Bilibili: https://space.bilibili.com/13290087 头像在对应的位置放上头像图片 123456789101112# Sidebar Avataravatar: # In theme directory (source/images): /images/avatar.gif # In site directory (source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /images/avatar.gif # If true, the avatar would be dispalyed in circle. rounded: false # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false 左右为了防止和博客混淆而误将日记上传，而将侧边栏调整到相反方向 1234sidebar: # Sidebar Position, available values: left | right (only for Pisces | Gemini). #position: left position: right 回到顶部这么好用的小功能当然要开着呀！ 123456back2top: enable: true # Back to top in sidebar. sidebar: true # Scroll percent label in b2t button. scrollpercent: true 菜单首先在解开“分类”(categories)和“标签”(tags)的注释 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 但是这个还只是在侧边的菜单栏处显示了“分类”和“标签”两项，还没有功能。 需要在根目录下使用指令来生成这两个页面： 12hexo new page categorieshexo new page tags 这下能显示了，但是仍然不够，因为Next还没有识别出这两个页面就是分类和标签页面。 打开source\categories\index.md，里面是： 1234---title: categoriesdate: 2019-06-26 15:44:09--- 在里面加上一句，变成： 123title: categoriesdate: 2019-06-26 15:44:09type: "categories" 这样就能识别出这是分类页面了，能够使用了。标签页面同理。 本地搜索第一步 修改主题设置找到这个设置： 1234567891011# Local search# Dependencies: https://github.com/theme-next/hexo-generator-searchdblocal_search: enable: true # If auto, trigger search by changing input. # If manual, trigger search by pressing enter key or search button. trigger: auto # Show top n results per article, show all results by setting to -1 top_n_per_article: 1 # Unescape html strings to the readable one. unescape: false 将enable改为true之后就会在菜单显示一个”搜索“，但是还无法使用。 照着注释里面那个github项目内的说明 第二步 下载插件：1npm install hexo-generator-searchdb --save 第三步 添加全局设置在根目录下的_config.yml加上如下设置： 123456search: path: search.xml field: post format: html limit: 10000 content: true 我把帮助中的注释复制过来就是下面这样： 12345678910111213141516171819# see https://github.com/theme-next/hexo-generator-searchdbsearch: # file path. By default is search.xml . If the file extension is .json, the output format will be JSON. Otherwise XML format file will be exported. path: search.xml # the search scope you want to search, you can chose: # post (Default) - will only covers all the posts of your blog. # page - will only covers all the pages of your blog. # all - will covers all the posts and pages of your blog. field: post # the form of the page contents, works with xml mode, options are: # html (Default) - original html string being minified. # raw - markdown text of each posts or pages. # excerpt - only collect excerpt. # more - act as you think. format: html #define the maximum number of posts being indexed, always prefer the newest. limit: 10000 # whether contains the whole content of each article. If false, the generated results only cover title and other meta info without mainbody. By default is true. content: true 意外地轻松便捷呢。 编写快捷打开的脚本虽然弄好了，但是每次想写还得打开命令行输入命令，再进入文件夹用Typora打开文件，太麻烦了。 于是写一下bat批处理脚本。 这东西其实就是把在命令行执行的命令放在一个文本文件然后把后缀名改成.bat而已。 不过我不是很熟命令，弄了很久。 快速打开本地预览首先是快速查看我的日记。目标是双击一下脚本文件就可以在浏览器中看到我的日记。 一般情况下的步骤： 在根目录打开命令行 输入hexo s 打开浏览器 在地址栏输入localhost:4000 我写出来的.bat文件是这样的： 12set browser="C:\Program Files (x86)\Tencent\QQBrowser\QQBrowser.exe"%browser% localhost:4000 &amp;&amp; hexo s 只有两行，第一行是设置用于打开日记本的浏览器所在的位置，当然，如果设置了环境变量，这里可以直接写浏览器的名字。 第二行是利用这个浏览器打开localhost:4000，打开成功才执行hexo s来启动hexo。 新建日记一般情况下的步骤： 在根目录打开命令行 输入hexo new &lt;title&gt; 打开source\_posts\ 找到并打开新建的日记 获取标题1set /p title=请输入标题: /p表示动态输入 创建文件1hexo new "%title%" %变量名%表示引用已经赋值的变量。 打开文件由于我设置的文件名不只是标题，因此还需要获取日期来组成文件名。 12345678set year=%date:~0,4%set month=%date:~5,2%set day=%date:~8,2%rem 在这里设置你的文件名格式set new_post_name=%year%-%month%-%day%-%title%"S:\Program Files\Typora\Typora.exe" source\_posts\%new_post_name%.md 其中： %date%是系统变量，用于获取系统时间，返回的值的格式是2019/06/26 周三 %date:~x,y%代表从第x个字符开始，获取y个字符 刚开始的脚本代码是这样的： 12345678910set /p title=请输入标题:hexo new "%title%"set year=%date:~0,4%set month=%date:~5,2%set day=%date:~8,2%rem 在这里设置你的文件名格式set new_post_name=%year%-%month%-%day%-%title%"S:\Program Files\Typora\Typora.exe" source\_posts\%new_post_name%.md 但是我发现在执行完hexo new &quot;%title%&quot;之后，命令行直接退出，加pause都没用。 猜测是因为，hexo创建文件需要时间，还没创建好就打开，于是出错了。 后来改成： 1hexo new "%title%" &amp;&amp; call z_open_editor.bat 在创建完之后，才会执行后面的内容，后面的代码都放在z_open_editor.bat里面 最终代码： 1234rem z_new_diary.bat@echo offset /p title=请输入标题:hexo new "%title%" &amp;&amp; call z_open_editor.bat 1234567891011121314151617rem z_open_editor.batrem 本文件只支持打开默认布局的文件@echo offset year=%date:~0,4%set month=%date:~5,2%set day=%date:~8,2%dir source\_postsif not defined title set /p title=请输入标题:rem 在这里设置你的文件名格式set new_post_name=%year%-%month%-%day%-%title%echo source\_posts\%new_post_name%.md"S:\Program Files\Typora\Typora.exe" source\_posts\%new_post_name%.md]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[练习利用Scrapy爬取b站排行榜]]></title>
    <url>%2Fpost%2FScrapy_spider_bilibiliRank%2F</url>
    <content type="text"><![CDATA[开始学python的Scrapy框架了，参考书是崔庆才的《python3网络爬虫开发实战》 跟着示例敲完之后，又试着按照一样的逻辑去爬取了B站排行榜的数据。 通过这个小项目学习使用Scrapy框架。 步骤新建项目首先新建一个名为practice的项目 1$scrapy startproject practice 这个项目的目录结构（省略init文件）： practice practice items.py middlewares.py pipelines.py settings.py scrapy.cfg 这一个项目里面的代码是整个项目的爬虫通用的。 新建Spider新建一个爬虫bilibiliRank 12$cd practice$scrapy genspider bilibiliRank 然后与在此目录下出现了一个spider文件夹，用于存放这个新的爬虫 spider bilibiliRank.py bilibiliRank.py中： 123456789import scrapyclass BilibilirankSpider(scrapy.Spider): name = 'bilibiliRank'#爬虫名字 allowed_domains = ['bilibili.com']#允许爬取的域名 start_urls = ['https://www.bilibili.com/ranking/']#初始url def parse(self, response): pass spider文件夹里面是用于爬取不同网站的爬虫，它继承自scrapy.Spider，scrapy的引擎Engine就是利用你写的爬虫里面的parse()方法来解析页面获取数据，可以在这个方法里面将数据以item的形式返回出去，给ItemPipeline继续处理。 创建Itemitems.py里面定义了不同的item，这些item都继承自scrapy.Item，文件生成的内容如下（无关注释已删去）： 123456import scrapyclass PracticeItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() pass 在这里你可以照着它的模板新建一个类，也可以直接修改，总之只要符合要求就可以： 1234567import scrapyclass RankItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() num=scrapy.Field() title=scrapy.Field() 在这个Item子类当中，我新建了两个域，也可以说是字段。按照注释给出的格式来就可以了。 解析response适当简化的流程大概是：引擎利用爬虫的start_url发起请求，然后将得到的响应response作为参数传入爬虫的parse()方法中。parse()将解析出的数据装入Item并返回给引擎。 需要解析的html页面内容（只展示其中一个项的结构）： 12345678910111213141516171819202122232425&lt;ul class="rank-list"&gt; &lt;li class="rank-item"&gt; &lt;div class="num"&gt;1&lt;/div&gt; &lt;div class="content"&gt; &lt;div class="img"&gt; &lt;a href="//www.bilibili.com/video/av56121331/" target="_blank"&gt; &lt;div class="lazy-img cover"&gt; &lt;img alt="视频标题" src="图片url"&gt; &lt;/div&gt; &lt;/a&gt; &lt;div class="watch-later-trigger w-later"&gt;&lt;/div&gt; &lt;/div&gt; &lt;div class="info"&gt; &lt;a href="视频url" target="_blank" class="title"&gt;视频标题&lt;/a&gt;&lt;!----&gt; &lt;div class="detail"&gt;&lt;span class="data-box"&gt; &lt;i class="b-icon play"&gt;&lt;/i&gt;366.8万&lt;/span&gt; &lt;span class="data-box"&gt;&lt;i class="b-icon view"&gt;&lt;/i&gt;3.8万&lt;/span&gt; &lt;a target="_blank" href="视频url"&gt; &lt;span class="data-box"&gt; &lt;i class="b-icon author"&gt;&lt;/i&gt;作者名&lt;/span&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class="pts"&gt; &lt;div&gt;3798978&lt;/div&gt; 综合得分 &lt;/div&gt;&lt;/div&gt;&lt;!----&gt;&lt;/div&gt; &lt;/li&gt;&lt;/ul&gt; 爬虫文件： 1234567891011121314151617import scrapyfrom practice.items import RankItem#这是之前自定义的itemclass BilibilirankSpider(scrapy.Spider): name = 'bilibiliRank' allowed_domains = ['bilibili.com'] start_urls = ['https://www.bilibili.com/ranking/'] def parse(self, response): #获取所有的项目 rank_items=response.css('.rank-list .rank-item') #获取每一项中的数据 for rank_item in rank_items: item=RankItem() item['num']=rank_item.css('.num::text').extract_first() item['title']=rank_item.css('.content .info .title::text').extract_first() yield item#每次调用就会返回一个item 遇到的问题： 注意获取的所有项目得是一个节点，不能用extract()读取其中的数据，第一次写时，写成了： 123rank_list=response.css('.rank-item').extract()for rank_item in rank_list: #…… 爬取1$scrapy crawl bilibiliRank -o bilibiliRank.json 利用名为bilibiliRank爬虫进行爬取，并将得到的结果保存在bilibiliRank.json文件中 参考链接 scrapy官方中文文档]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>scrapy</tag>
        <tag>框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java基于AWT的对战小游戏]]></title>
    <url>%2Fpost%2Fjava_game_FightFieldFrame%2F</url>
    <content type="text"><![CDATA[这学期的java课设弄完了，写个博客总结一下。 哔哩哔哩对应视频的传送门 课设目的与要求根据讲义中策略模式的案例，设计和实现一个基于策略模式的角色扮演游戏。其中包括主要有角色类及其子类、相关的行为类集合和测试类等。 通过本次实验，能够在掌握面向对象程序设计的基本思想基础上；深化理解 Java 面向对象程序设计中消息、继承、多态、接口、抽象类和抽象方法等概念和实现方式；并进一步掌握 Java 程序设计中的基本语法和 Java程序运行方法等；理解和应用包（package）。 内容一个游戏中有多种角色(Character)，例如：国王（King）、皇后（Queen）、骑士（Knight）、老怪（Troll）。角色之间可能要发生战斗(fight)，每场战斗都是一个角色与另一角色之间的一对一战 斗。 每个角色都有自己的生命值 (hitPoint) 、 魔法值（magicPoint）、攻击力值(damage)和防御力值(defense)。 每种角色都有一种武器进行攻击（fight）；在程序运行中，可以动态修改角色的武器(setWeaponBehavior)。 每种角色都有一种魔法对自己或者其他角色施法（performMagic）；可以动态改变拥有的魔法（setMagicBehavior）。 首先设计和实现抽象类 Characters。 设计和实现 Character 类的几个子类：King、Queen、Knight、Troll。位 设计接口 WeaponBehavior 和 MagicBehavior。 接 口 WeaponBehavior 的 实 现 类 ： KnifeBehavior （ 用 刀 ） BowAndArrowBehavior （ 用 弓 箭 ） AxeBehavior （ 用 斧 ） SwordBehavior（用剑） 接口MagicBehavior的实现类： HealBehavior（治疗） InvisibleBehavior（隐身）。 实现接口中的抽象方法，可以只在屏幕输出简单信息，也可以结合生命值(hitPoint)、攻击力值(damage)和防御力值(defense)计算。 编写测试代码，对以上设计的系统进行测试。要求在程序运行期间，能动态改变角色拥有的武器或者魔法。 自己添加一种角色、或者添加一种武器及魔法，设计相应的类，并编写测试代码进行测试。 按照 Java 的规范，添加详细的文档注释，并用 Javadoc 生成标准的帮助文档。 将上述编译、运行、生成帮助文档的命令，填写至实验报告相应位置。 填写实验报告。并将程序代码及生成的帮助文档打包上交。 涉及的主要内容 单例模式。游戏窗口只能有一个对象，因此使用了单例模式。 策略模式。在角色类中有两个抽象策略（武器策略和魔法策略），具体策略在类中实现。 双缓冲技术。在绘制游戏画面的时候使用了双缓冲技术，防止画面闪烁。 多线程。在两处使用了多线程，一处是为了解决按键冲突的问题，另一处是为了实现游戏周期性判定的功能。 awt。 基本逻辑流程 抽象角色类由具体子类实现，子类主要实现了抽象方法getAppearance，用于获取角色的外貌（即图片），外貌会根据角色状态的不同而改变，比如角色死亡时外貌是墓碑； 根据角色的坐标以及属性（例如是否隐身，当前武器是什么）来绘制角色以及属性条、武器栏和魔法栏。 游戏时钟周期线程用于周期性地执行一些操作，例如每秒钟恢复一定的HP和MP，对于隐身状态的角色，每秒钟扣除一定量的MP等。 游戏说明 玩家1操作：键盘上A键D键分别对应左右移动，J键使用武器攻击，K键使用魔法，L键切换武器，O键切换魔法； 玩家2操作：键盘上←键→键分别对应左右移动，小键盘上，1键使用武器攻击，2键使用魔法，3键切换武器，6键切换魔法； 每把武器有自己的攻击威力和攻击距离，只有在两个角色的距离在武器的攻击范围内时，才能够攻击成功； 伤害计算公式为：被攻击者受到的最终伤害=攻击者攻击力+攻击者武器威力-被攻击者的防御力。若伤害小于等于0，则不予扣除； 每秒钟会恢复一定量的HP和MP； 一方死亡（HP降为0及以下）则游戏结束。 设计与实现主要框架12345678910111213141516171819202122public class FightFieldFrame extends Frame&#123; //一些游戏常量以及窗口 public static final Dimension SCREEN_DIMENSION=Toolkit.getDefaultToolkit().getScreenSize(); public static final int FFF_X=0; public static final int FFF_Y=0; public static final int FFF_HEIGHT=SCREEN_DIMENSION.height; public static final int FFF_WIDTH=SCREEN_DIMENSION.width;//……省略其他成员函数，下面会列举来说明/*******************main函数**************************/ public static void main(String args[]) &#123; FightFieldFrame f=getInstance("战斗领域"); f.initFrame(); //初始化角色 f.initCharacter(); //添加事件监听者 f.addWindowListener(new MyWindowListener()); f.addKeyListener(new GamePad(player1,player2,f)); //新建时钟线程，用于游戏中的周期性属性检查 Thread clockThread=new Thread(new ClockThread(player1, player2, fff)); clockThread.start(); &#125;&#125; 单例模式FightFieldFrame 类中： 1234567891011//只能有一个窗体对象，使用单例模式private static FightFieldFrame fff;//单例模式使用的对象private FightFieldFrame(String title) &#123; super(title);&#125;public static FightFieldFrame getInstance(String title) &#123; if(fff==null) &#123; fff=new FightFieldFrame(title); &#125; return fff;&#125; 双缓冲双缓冲因为有两个绘图对象而得名，先在一个image对象上绘图然后再将此对象绘制到Frame上，用于减少重绘时的闪烁。 123456789101112/** * 初始化框架的位置和大小，以及缓冲对象 */public void initFrame() &#123; //这里准备一些对象构造完成之后才能做的事情 fff.setVisible(true); setBounds(FFF_X, FFF_Y, FFF_WIDTH, FFF_HEIGHT); Dimension d=getSize(); imgBuffer=createImage(d.width, d.height); gBuffer=imgBuffer.getGraphics();&#125; 创建好缓冲对象后，在缓冲对象上绘制： 123456789101112131415161718192021222324public void paint(Graphics g) &#123; //全都先绘制在缓冲区 //绘制背景 Image background=getToolkit().getImage("image\\background.jpg"); if(background!=null) &#123; gBuffer.drawImage(background, FFF_X, FFF_Y, FFF_WIDTH, FFF_HEIGHT, this); &#125; //绘制人物 if(player1!=null) &#123; drawCharacter(gBuffer,player1); drawStrand(gBuffer, player1); &#125; if(player2!=null) &#123; drawCharacter(gBuffer,player2); drawStrand(gBuffer, player2); &#125; drawSlot(gBuffer); //drawStrand(gBuffer);//绘制绝对位置的属性条，由于没有什么技术含量就只做了一个示例 //由于使用了背景图片，所以不必特地清空背景 g.drawImage(imgBuffer, 0, 0, this);&#125; 但是，即便如此仍然会闪烁，这是因为重绘时调用的update函数会将Frame用背景色填充一次 再绘制。所以应该覆盖掉原本的方法，让它只绘制，不清空： 123456//======================//public void update(Graphics g) &#123; //覆盖原本的方法 paint(g);&#125;//======================/*/ 玩家操纵使用GamePad类作为键盘监听者，监听Frame的按键，调用角色对应的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 游戏手柄类 * 用于将键位与角色的动作对应起来 */public class GamePad implements KeyListener&#123; private Characters player1;//玩家1 private Characters player2;//玩家2 private FightFieldFrame fff; public GamePad(Characters p1, Characters p2, FightFieldFrame f) &#123; // TODO Auto-generated constructor stub player1=p1; player2=p2; fff=f; &#125; @Override public void keyPressed(KeyEvent e) &#123; int code=e.getKeyCode(); switch (code) &#123; case KeyEvent.VK_J://玩家1攻击 player1.fight(player2); player2.display(); break; case KeyEvent.VK_K://玩家1使用魔法 player1.performMagic(player2); break; case KeyEvent.VK_A://玩家1左 player1.setMoveLeftFlag(true); player1.setDirection(true);//false为朝右，true为朝左 break; case KeyEvent.VK_D://玩家1右 player1.setMoveRightFlag(true); player1.setDirection(false); break; case KeyEvent.VK_L://玩家1切换武器 player1.changeWeapon(); break; case KeyEvent.VK_O://玩家1切换魔法 player1.changeMagic(); break; /****************************************************************/ case KeyEvent.VK_NUMPAD1://玩家2攻击 player2.fight(player1); player1.display(); break; case KeyEvent.VK_NUMPAD2://玩家2使用魔法 player2.performMagic(player1); break; case KeyEvent.VK_LEFT://玩家2左 player2.setMoveLeftFlag(true); player2.setDirection(true);//false为朝右，true为朝左 break; case KeyEvent.VK_RIGHT://玩家2右 player2.setMoveRightFlag(true); player2.setDirection(false); break; case KeyEvent.VK_NUMPAD3://玩家2切换武器 player2.changeWeapon(); break; case KeyEvent.VK_NUMPAD6://玩家2切换魔法 player2.changeMagic(); break; default: break; &#125; fff.repaint();//重绘 &#125; @Override public void keyReleased(KeyEvent e) &#123; int code=e.getKeyCode(); switch (code) &#123; case KeyEvent.VK_J: break; case KeyEvent.VK_A://左 player1.setMoveLeftFlag(false); break; case KeyEvent.VK_D://右 player1.setMoveRightFlag(false); break; case KeyEvent.VK_K: break; case KeyEvent.VK_NUMPAD1: break; case KeyEvent.VK_LEFT: player2.setMoveLeftFlag(false); break; case KeyEvent.VK_RIGHT: player2.setMoveRightFlag(false); break; default: break; &#125; fff.repaint();//重绘 &#125; @Override public void keyTyped(KeyEvent e) &#123;&#125;&#125; 注意，这里控制角色左右移动并不是直接调用角色的移动方法，而是更改角色移动的标志变量，利用线程来调用角色的移动方法。这样可以解决角色的按键冲突问题。 角色移动线程移动线程只负责发送消息给角色，而角色移动的具体判定由角色自身完成，从而更好地实现面向对象的思想。 1234567891011121314public class MoveThread implements Runnable&#123; private Characters character; public MoveThread(Characters c) &#123; character=c; &#125; public void run() &#123; while(true) &#123; //线程只负责发送消息，让角色自己判断移动 character.moveRight(); character.moveLeft(); &#125; &#125;&#125; 下面这是Characters类中的角色移动函数，添加了延时以免在按下移动按键的一瞬间，角色移动太快出了屏幕外面。 12345678910111213141516171819202122232425262728293031/** * 向左移动&lt;br/&gt; * 由于两个线程各自操作自己的角色，所以此函数不需要同步 */public void moveLeft() &#123; if(!isAliveFlag) return; if(moveLeftFlag) &#123; x-=1; try &#123; Thread.sleep(1);//防止跑得太快 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;/** * 向右移动&lt;br/&gt; * 由于两个线程各自操作自己的角色，所以此函数不需要同步 */public void moveRight() &#123; if(!isAliveFlag) return; if(moveRightFlag) &#123; x+=1; try &#123; Thread.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 武器攻击实现机制在Characters类中，使用武器进行攻击的方法如下，它的主要逻辑是调用useWeapon方法： 1234567891011121314151617181920212223/** * 攻击某个角色 * @param c 要攻击的角色 * @return 造成的真实伤害 */public int fight(Characters c) &#123; //由于武器有不同的特性，所以伤害的逻辑让武器实现 //比如后期编写高级玩法时，弓需要计算射程 if(!isAliveFlag) return 0;//如果已死亡，直接返回，下同 if(weapon==null) &#123; System.out.println(name+"没有武器，无法攻击"); return 0; &#125; int attackRange=weapon.getAttackRange(); if(attackRange&gt;distance(c)) &#123; //攻击距离大于角色之间的距离才可攻击 return weapon.useWeapon(this,c);//此角色攻击角色c &#125; else &#123; return 0; &#125; &#125; 角色类的两个属性，武器和魔法，使用的都是对应接口的引用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152protected WeaponBehavior weapon;//武器protected MagicBehavior magic;//魔法以下是武器接口：public interface WeaponBehavior &#123; /** * 使用武器 * @param attacker 武器持有者 * @param victim 被攻击者 * @return 造成的真实伤害 */ public int useWeapon(Characters attacker,Characters victim);//使用武器 public String getName(); public int getAttackRange(); public Image getAppearance();&#125;以下是具体的武器实现（以剑为例，其他大同小异）：/** * 剑 * 实现武器接口 * 威力中等，攻击距离中等 */public class SwordBehavior implements WeaponBehavior &#123; private String name="剑"; private Image appearance; private static final int DAMAGE=6;//武器基础威力 private static final int ATTACK_RANGE=200;//武器攻击距离,单位px private static final String APPEARANCE_PATH="image\\Weapon\\Sword.png"; public SwordBehavior() &#123;&#125; public SwordBehavior(String _name) &#123; name=_name;//剑，岂能无名OVO &#125; /** * 使用武器攻击 * @param attacker 攻击者 * @param victim 被攻击者 */ @Override public int useWeapon(Characters attacker,Characters victim) &#123; int attackDamage=DAMAGE+attacker.getDamage();//造成的伤害为攻击者的伤害加上武器威力 int finalDamage=victim.hitBy(attacker, attackDamage); System.out.println(attacker.getName()+"使用"+name+"对"+victim.getName()+"造成了"+finalDamage+"点伤害"); return finalDamage;//返回最终伤害 &#125; public String getName() &#123;return name;&#125; public int getAttackRange() &#123;return ATTACK_RANGE;&#125; public Image getAppearance() &#123; appearance=Toolkit.getDefaultToolkit().getImage(APPEARANCE_PATH); return appearance; &#125;&#125; 这里面主要的代码是useWeapon方法里面调用的角色类的hitBy方法，里面有着伤害计算逻辑： 123456789101112131415/** * 被某个角色攻击 * @param attacker 攻击者 * @param attackDamage 攻击者给予的攻击伤害 * @return 最后造成的真实伤害 */public int hitBy(Characters attacker,int attackDamage) &#123;//被攻击 if(!isAliveFlag) return 0; int finalDamage=(attackDamage-defense);//伤害计算：最终伤害=敌方攻击伤害-我方防御力 if(incHP(-finalDamage)==-1) &#123;//如果血量被扣到负数 this.killedBy(attacker); &#125; return finalDamage;//返回最后造成的真实伤害&#125; 魔法的实现机制大同小异，不做特殊说明。 武器切换和魔法切换实现方法是在角色类里面声明数组： 1234protected WeaponBehavior weaponSlots[];//武器栏位，用于存储角色携带的武器protected MagicBehavior magicSlots[];//魔法栏位protected int weaponSlotsIndex=0;//栏位索引，指示当前武器protected int magicSlotsIndex=0; 以切换武器为例，如果当前武器是最后一把，那么换回第一把，否则索引自增： 123456789101112/** * 按顺序切换武器 */public void changeWeapon() &#123; setWeaponBehavior(weaponSlots[weaponSlotsIndex]); if(weaponSlotsIndex+1&gt;=weaponSlots.length) &#123; weaponSlotsIndex=0; &#125; else &#123; weaponSlotsIndex++; &#125;&#125; 时钟线程时钟线程用于进行一些游戏周期性方法的调用，比如周期性恢复HP，对角色属性值的判断等。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/** * 时钟线程，用于一些周期性的计算 */public class ClockThread implements Runnable&#123; private Characters player1;//玩家1 private Characters player2;//玩家2 private FightFieldFrame fff; private int interval=1000;//时钟周期 public ClockThread(Characters p1, Characters p2, FightFieldFrame f) &#123; player1=p1; player2=p2; fff=f; &#125; /** * 核对属性，并对于特定属性作出不同的事情 * @param c 核对角色c的属性 */ public void cheackStatus(Characters c) &#123; switch (c.getStatus()) &#123; case Characters.ST_INVISIBLE://隐身魔法每个周期扣除一定的魔力 if(c.incMP(-InvisibleBehavior.COST)==-1) &#123;//如果魔力不够 c.setStatus(Characters.ST_NORMAL); &#125; break; default: break; &#125; &#125; /** * 周期性恢复属性值（回血回魔） * @param c 周期性恢复角色c的HP和MP */ public void recover(Characters c) &#123; if(!c.getIsAliveFlag()) return;//角色死亡就不再回血 c.incHP(Characters.HP_RECOVER); c.incMP(Characters.MP_RECOVER); &#125; public void run() &#123; while(true) &#123; //做这个周期要做的事情 cheackStatus(player1); cheackStatus(player2); recover(player1); recover(player2); fff.repaint(); //等待下一个周期 try &#123; Thread.sleep(interval); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 参考链接 java.awt.Image官方文档 使用eclipse生成javadoc-博客园 java双缓冲技术-CSDN java获取屏幕大小-CSDN java线程传参三种方式-脚本之家 游戏角色移动流畅度的处理-ITeye eclipse调试方式和快捷键-CSDN 一个讲eclipse调试的b站视频（靠这个视频解决了调试问题）-bilibili 绘制字体修改-CSDN 体会这次课设对我来说是个挑战，首先时间比较紧张，和考试放在了一周，并且用的是学了几周还没私底下练习多少的JAVA。不过还是做的让我自己比较满意。 我选择的是看上去较为简单的一道题目，虽然简单，但是这个题目的可扩展性很强，可以尽情开脑洞，我看中的就是这一点。我在高中的时候就尝试使用Visual Basic来编写类似的小游戏，一些可能会遇到的困难在那时已经思考过了，所以总体来说没有遇到太过麻烦的地方。 随着经验的增长，我逐渐开始一边编程一边整理，让以后的自己也能够回顾这一次的项目。在写完这个课设之后，我用录屏软件录制了一个视频来整体讲述我编写过程中的思路，并上传到了Bilibili弹幕视频网站，总结经验，分享思路，以及为了便于以后回顾。地址是（https://www.bilibili.com/video/av54526303/） 当然，过程中也遇到了一些问题。 比如绘制图片的时候遇到了只能使用绝对路径的问题，在老师上课演示的过程中也遇到过这个问题，后来我知道了JAVA相对路径是以项目根目录为基准而不是以文件目录为基准的。 比如角色控制按键冲突。解决方法是使用多线程，两个线程控制分别控制两个角色。 比如游戏周期性事件。在以前我使用Visual Basic的时候，是利用时钟控件来解决这个问题的，而JAVA里面可以使用线程来模拟那个时钟控件。这让我对时钟控件的原理有了比较好的认识。 在假期里面，我可能会通过继续完善这个小游戏，来更加深入地学习JAVA。]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记5爬虫类结构优化]]></title>
    <url>%2Fpost%2Fpython_spider_note5optimization_of_the_spider_class%2F</url>
    <content type="text"><![CDATA[打算全部以cookie来登陆，而不依赖于session（因为听组长说session没cookie快，而且我想学些新东西而不是翻来覆去地在舒适区鼓捣）。弄了几天终于弄出来个代码不那么混乱的爬虫类了，更新一下博文来总结一下。代码在我github的spider库里面。 初步思路既然要封装成爬虫类，那么就以面向对象的思维来思考一下结构。 从通用的爬虫开始，先不考虑如何爬取特定的网站。 以下只是刚开始的思路，并不是最终思路。 爬虫的行为步骤并不复杂，分为以下几步： 请求并获取网页（往往需要模拟登录） 解析网页提取内容（还需要先获取需要爬取的url） 保存内容（保存到数据库） 爬虫类方法（初步设计）： 方法 说明 login 登录 parse 解析 save 保存 crawl 爬取（外部调用者只需调用这个方法即可） 爬虫类属性（初步设计）： 属性 说明 headers 请求的头部信息，用于伪装成浏览器 cookies 保存登录后得到的cookies db_data 数据库的信息，用于连接数据库 进一步设计我想将这个爬虫类设计得更为通用，也就是只修改解析的部分就能爬取不同的网站。组长说我这是打算写一个爬虫框架，我可没那么厉害，只是觉得把逻辑写死不能通用的类根本不能叫做类罢了。 参考代码我看了一下组长给出的参考代码，大致结构是这样的： 首先一个Parse解析类（为了关注结构，具体内容省略）： 12345678910111213141516171819202122232425262728293031323334353637383940class Parse(): def parse_index(self,text): ''' 用于解析首页 :param text: 抓取到的文本 :return: cpatcha_url, 一个由元组构成的列表(元组由两个元素组成 (代号，学校名称)) ''' pass def parse_captcha(self, content, client): ''' 解析验证码 :return: &lt;int&gt; or &lt;str&gt; a code ''' pass def parse_info(self, text): ''' 解析出基本信息 :param text: :return: ''' pass def parse_current_record(self, text): ''' 解析消费记录 :param text: :return: ''' return self.parse_info(text) def parse_history_record(self, text): ''' 解析历史消费记录 :param text: :return: ''' return self.parse_info(text) 这个思路不错，将解析部分独立形成一个类，不过这样要如何与爬虫类进行逻辑上的关联呢？解析类的对象，是什么？是解析器吗？解析器与爬虫应该是什么关系呢？ 我继续往下看： 12345678910111213141516class Prepare(): def login_data(self,username, password, captcha, schoolcode, signtype): ''' 构造登陆使用的参数 :return:data ''' pass#省略代码，下同 def history_record_data(self, beginTime, endTime): ''' 历史消费记录data :param beginTime: :param endTime: :return: data ''' pass 这是一个Prepare类，准备类？准备登录用的数据。说起来似乎比解析类更难以让我接受。解析器还可以说是装在爬虫身上，但是，但是“准备”这件事情分明是一个动作啊喂！ 好吧，“一类动作”倒能说得过去吧。我看看怎么和爬虫类联系起来： 12class Spider(Parse, Prepare):#??? pass 等会儿等会儿…… 继承关系？ 让我捋捋。 为了让爬虫能解析和能准备还真是不按套路出牌啊…… 子类应该是父类的特化吧不是吗，就像猫类继承动物类，汽车类继承车类一样，猫是动物，汽车也是车。 算了不继续了，毕竟我不是为了故意和我组长作对。只是将其作为一个例子来说明我的思路。 解析器类参考代码虽然不太能让我接受，但是它的结构仍然带给了我一定启发。就是解析函数不一定要作为爬虫的方法。 解析这个步骤如果真的只写在一个函数里面真的非常非常乱，因为解析不只一个函数。比如解析表单的隐藏域，解析页面的url，解析页面内容等。 单独写一个解析类也可以。至于它和爬虫类的关系，我觉得组合关系更为合适（想象出了一只蜘蛛身上背着一个红外透视仪的样子），spider的解析器可以更换，这样子我觉着更符合逻辑一些。 关于更换解析器的方式，我打算先写一个通用的解析器类作为基类，而后派生出子解析器类，子解析器根据不同的网站采取不同的解析行为。 然后新建my_parser.py文件，写了一个MyParser类。解析方式是xpath和beautifulsoup。这里面的代码是我把已经用于爬取学校网站的特定代码通用化之后的示例代码，实际上并不会被调用，只是统一接口，用的时候会新写一个类继承它，并覆盖里面的函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class MyParser(object): def login_data_parser(self,login_url): ''' This parser is for chd :param url: the url you want to login :return (a dict with login data,cookies) ''' response=requests.get(login_url) html=response.text # parse the html soup=BeautifulSoup(html,'lxml') #insert parser,following is an example example_data=soup.find('input',&#123;'name': 'example_data'&#125;)['value'] login_data=&#123; 'example_data':example_data &#125; return login_data,response.cookies def uni_parser(self,url,xpath,**kwargs): response=requests.post(url,**kwargs) html=response.text tree=etree.HTML(html) result_list=tree.xpath(xpath) return result_list def get_urls(self,catalogue_url,**kwargs): ''' get all urls that needs to crawl. ''' #prepare base_url='http://example.cn/' cata_base_url=catalogue_url.split('?')[0] para = &#123; 'pageIndex': 1 &#125; #get the number of pages xpath='//*[@id="page_num"]/text()' page_num=int(self.uni_parser(cata_base_url,xpath,params=para,**kwargs)) #repeat get single catalogue's urls xpath='//a/@href'#link tag's xpath url_list=[] for i in range(1,page_num+1): para['pageIndex'] = i #get single catalogue's urls urls=self.uni_parser(cata_base_url,xpath,params=para,**kwargs) for url in urls: url_list.append(base_url+str(url)) return url_list def get_content(self,url,**kwargs): ''' get content from the parameter "url" ''' html=requests.post(url,**kwargs).text soup=BeautifulSoup(html,'lxml') content=soup.find('div',id='content') content=str(content) return content 我把构造登录信息的部分放在了解析器中。并在登录中调用。 登录之后得到的cookies就在参数中传递。 数据库类由于只打算存到数据库，所以并没有写一个“存档宝石类“，或许之后会写。 目前我只写了一个保存函数，以及自己封装的一个数据库类。 这个数据库类是my_database.py中的MyDatabase（应该不会撞名吧），目前只封装了insert函数，传入的参数有三个：数据库名，表名，装有记录的字典。代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import pymysqlclass MyDatabase(object): def __init__(self,*args,**kwargs): self.conn=pymysql.connect(*args,**kwargs) self.cursor=self.conn.cursor() def insert(self,db,table,record_dict): ''' :param db:name of database that you want to use :param table:name of table that you want to use :param record_dict:key for column,value for value ''' #1.use the database sql='use &#123;&#125;'.format(db) self.cursor.execute(sql) self.conn.commit() #2.connect the sql commend sql='insert into &#123;&#125;('.format(table) record_list=list(record_dict.items()) for r in record_list: sql += str(r[0]) if r != record_list[-1]: sql += ',' sql+=') values(' for r in record_list: sql += '"' sql += str(r[1]) sql += '"' if r != record_list[-1]: sql += ',' sql+=')' #3.commit self.cursor.execute(sql) self.conn.commit() def show(self): pass def __del__(self): self.cursor.close() self.conn.close()if __name__ == "__main__": db_data=&#123; 'host':'127.0.0.1', 'user':'root', 'passwd':'password', 'port':3306, 'charset':'utf8' &#125; test_record=&#123; 'idnew_table':'233' &#125; mydb=MyDatabase(**db_data) mydb.insert('news','new_table',test_record) 封装之后用起来比较方便。 save函数123456def save(content,**save_params): mydb=MyDatabase(**save_params) record=&#123; 'content':pymysql.escape_string(content) &#125; mydb.insert('dbase','bulletin',record) pymysql.escape_string()函数是用于将内容转义的，因为爬取的是html代码（就不解析那么细了，直接把那一块html代码全部存下来，打开的时候格式还不会乱），有些内容可能使组合成的sql语句无法执行。 爬虫类给构造函数传入特定的解析器和保存函数，然后调用crawl方法就可以让spider背着特制的parser去爬取网站内容啦~ 登录函数和上次不太一样，做了一些修改，不过主要功能仍然是获取登录之后的cookies的。 简单说一下修改：我们学校网站登录之后会从登陆页面开始，经过三四次跳转之后才到达首页，期间获取到的cookies都需要保留，这样才能利用这些cookies来进入新闻公告页面。于是禁止重定向，手动获取下一个url，得到这一站的cookies之后再手动跳转，直到跳转到首页。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import requestsclass MySpider(object): def __init__(self,parser,save,**save_params): self.parser=parser#parser is a object of class self.save=save#save is a function self.save_params=save_params self.cookies=None self.headers=&#123; "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36" &#125; def login(self,login_url,home_page_url): ''' login :param login_url: the url you want to login :param login_data_parser: a callback function to get the login_data you need when you login,return (login_data,response.cookies) :param target_url: Used to determine if you have logged in successfully :return: response of login ''' login_data=None #get the login data login_data,cookies=self.parser.login_data_parser(login_url) #login without redirecting response=requests.post(login_url,headers=self.headers,data=login_data,cookies=cookies,allow_redirects=False) cookies_num=1 while(home_page_url!=None and response.url!=home_page_url):#if spider is not reach the target page print('[spider]: I am at the "&#123;&#125;" now'.format(response.url)) print('[spider]: I have got a cookie!Its content is that \n"&#123;&#125;"'.format(response.cookies)) #merge the two cookies cookies=dict(cookies,**response.cookies) cookies=requests.utils.cookiejar_from_dict(cookies) cookies_num+=1 print('[spider]: Now I have &#123;&#125; cookies!'.format(cookies_num)) next_station=response.headers['Location'] print('[spider]: Then I will go to the page whose url is "&#123;&#125;"'.format(next_station)) response=requests.post(next_station,headers=self.headers,cookies=cookies,allow_redirects=False) cookies=dict(cookies,**response.cookies) cookies=requests.utils.cookiejar_from_dict(cookies) cookies_num+=1 if(home_page_url!=None and response.url==home_page_url): print("login successfully") self.cookies=cookies return response def crawl(self,login_url,home_page_url,catalogue_url): self.login(login_url,home_page_url) url_list=self.parser.get_urls(catalogue_url,cookies=self.cookies,headers=self.headers) for url in url_list: content=self.parser.get_content(url,cookies=self.cookies,headers=self.headers) self.save(content,**self.save_params) def __del__(self): pass 调用为了更好地展示结构，大部分内容都pass省略掉。想看具体代码可以去我github的spider库 这个文件内首先创建了一个特定解析类，继承自通用解析类，再写了一个保存函数，准备好参数，最后爬取。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from my_spider import MySpiderfrom my_parser import MyParserfrom my_database import MyDatabasefrom bs4 import BeautifulSoupimport requestsimport pymysqlclass chdParser(MyParser): def login_data_parser(self,login_url): ''' This parser is for chd :param url: the url you want to login :return (a dict with login data,cookies) ''' pass return login_data,response.cookies def get_urls(self,catalogue_url,**kwargs): ''' get all urls that needs to crawl. ''' #prepare pass #get page number pass #repeat get single catalogue's urls pass for i in range(1,page_num+1): para['pageIndex'] = i #get single catalogue's urls pass return url_list def save(content,**save_params): passif __name__ == '__main__': login_url="pass"#省略 home_page_url="pass" catalogue_url="pass" parser=chdParser() save_params=&#123; 'host':'127.0.0.1', 'user':'root', 'passwd':'password', 'port':3306, 'charset':'utf8' &#125; sp=MySpider(parser,save,**save_params) sp.crawl(login_url,home_page_url,catalogue_url)]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记4模拟登录函数的优化]]></title>
    <url>%2Fpost%2Fpython_spider_note4optimization_of_the_login_function%2F</url>
    <content type="text"><![CDATA[前面写的代码虽然完成了爬取的功能，但是过于凌乱，于是打算重构一遍。首先从登陆开始 改进前的代码面向过程这是第一次写的登录函数，获取登录信息和登录本身是放在一起的。 1234567891011121314151617181920212223242526272829303132333435363738def login(): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ #设置 login_url = 'http://ids.chd.edu.cn/authserver/login?service=http%3A%2F%2Fportal.chd.edu.cn%2F' headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36', &#125; #新建会话 session=requests.session() #获取登录校验码 html=session.post(login_url,headers=headers).text soup=BeautifulSoup(html,'lxml') lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; #登录 response=session.post(login_url,headers=headers,data=login_data) if response.url=='http://portal.chd.edu.cn/': print('登录成功！') return session 面向对象第二次是将全部函数封装到类中，这次将获取登录信息从其中分出来。但是两者关系仍然太过于紧密。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061class spider: ''' 爬虫类 ''' def __init__(self,headers): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 self.headers=headers#头信息 self.cookiejar=http.cookiejar.LWPCookieJar('cookie.txt') def get_login_data(self,login_url): ''' 获取登录需要的数据 :param login_url: 登录页面url :return: 一个存有登录数据的字典 ''' # 获取登录校验码 html = self.session.post(login_url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') lt = soup.find('input', &#123;'name': 'lt'&#125;)['value'] dllt = soup.find('input', &#123;'name': 'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data = &#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn': '', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ if self.load_cookie(): self.is_login = True else: #获取登录信息 login_data=self.get_login_data(login_url) # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url: print("登录成功") self.is_login=True self.save_cookie() else: print("登录失败") return self.session #省略后面的函数 改进这次改进，我打算让login()函数与获取登录信息用的函数关系没有那么紧密，让后者可以被替换或者不用。 所以使用了回调函数，也就是将函数指针作为参数传入，不过python变量本身就像指针一样，直接传变量即可。 函数头1def login(self,login_url,login_data_parser=None,target_url=None): 传入了三个参数， login_url : 显而易见，这是登录页面的url login_data_parser : 这是一个函数，用于解析页面中随机生成的隐藏域代码的函数，可以不传入 target_url : 用于判断是否登录成功，这是登录之后会跳转到的页面 获取登录信息接着判断参数是否为函数（是否可调用），如果可以调用，就调用它获取登录信息。在这里不需要关心函数内部具体如何获取，而只用关心它的接口。 这个函数的返回值是一个装有登录信息的dict，和一个cookies。 12345def login(self,login_url,login_data_parser=None,target_url=None): login_data=None #get the login data if(login_data_parser!=None and callable(login_data_parser)): login_data,cookies=login_data_parser(login_url) 登录然后就完成了 1234567891011121314151617181920212223def login(self,login_url,login_data_parser=None,target_url=None): ''' login :param login_url: the url you want to login :param login_data_parser: a callback function to get the login_data you need when you login,return (login_data,response.cookies) :param target_url: Used to determine if you have logged in successfully :return: response of login ''' login_data=None #get the login data if(login_data_parser!=None and callable(login_data_parser)): login_data,cookies=login_data_parser(login_url) #login response=requests.post(login_url,headers=self.headers,data=login_data,cookies=cookies) if(target_url!=None and response.url==target_url): print("login successfully") self.cookies=cookies return response 获取登录信息函数这个和前面就是一样的了。只要修改传给login函数的函数，就可以获取不同网站的登录信息。login函数变得更加通用了，不再过于依赖登录信息函数存在。 1234567891011121314151617181920212223242526def chd_login_data_parser(self,url): ''' This parser is for chd :param url: the url you want to login :return (a dict with login data,cookies) ''' response=requests.get(login_url) html=response.text # parse the html soup=BeautifulSoup(html,'lxml') lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': input('input account:'), 'password': input('input passwd:'), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data,response.cookies]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git恢复误提交的内容]]></title>
    <url>%2Fpost%2Fgit_reset_incorrect_commit%2F</url>
    <content type="text"><![CDATA[在图书馆敲下最后几行代码，然后就着手机热点把爬虫代码push上去之后，突然想起来，我好像忘了把账号密码部分改成手动输入，现在push上去的是明文啊！掀桌！早知道就回宿舍上传了，说不准还能想起来。 问题及其解决方案已经上传了，即便我再改回来上传，别人也可以从git log里面看到我的账号密码。 那就版本回退，重新更新再上传。但是在我使用GitHub Desktop的Revert this commit的时候它却让我解决一大堆冲突……等会儿，啥时候多出来那么多“changes”？？？刚刚还一个都没有啊，怎么我用了这个选项还没回退就出现一大堆冲突？ 我对git其实不熟练，用GUI界面也是，解决这些冲突比较麻烦。所以最后的解决方案比较粗暴： 删除本地库以解决那一大堆的冲突文件 从远程库clone回来 把库内文件全部打包复制在别的路径 在库里面打开git bash，使用git reset --hard 版本号回到没出事的版本 将前面备份的文件复制回来 修改之后重新提交，完成 教训 得多准备一条分支，别直接在主分支上边写 一定要注意代码中的隐私信息！]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[手工归档编程项目]]></title>
    <url>%2Fpost%2Farchive_project%2F</url>
    <content type="text"><![CDATA[以前写代码建立的工程到处堆放，导致不能很好的找到以前的代码。虽然以前简单地划分了一下文件夹，但并没有投入太多精力去想如何分类。所以打算养成归档编程项目的好习惯，记录一下过程。也给读者们一个参考。 不放图了，文件树结构就用无序列表来显示。 分类整理首先把所有项目文件夹全部放进一个专门的文件夹里面，最好不要中文名，也不要拼音，这是个好习惯，以后的命名也是。我将它起名为DEVELOP。 将它放置在F盘（我拿F盘当文件盘），并且设置一个快捷方式在桌面，嘿嘿我还给快捷方式选了一个很炫酷的图标让自己开心一下。 然后根据语言将其分为cpp_develop，py_develop，vb_develop，web_develop等（html，css，js等统一划分到web_develop里面，因为我个人觉得它们三个分不开） 在每一个语言文件夹里面再细分(用cpp举例) 文件夹名 内容 cpp_archive 用于归档已经完成的项目，方便以后查找 cpp_project 用于存放正在开发的项目 cpp_test 用于测试。这里面我建立了几个空项目用于在别人问我代码问题的时候测试 cpp_example 用于存放从各种渠道得到的源代码，用于研究学习，里面的代码是别人的 cpp_lessonwork 用于存放课设或者课程实验代码，可并入cpp_project cpp_pratice 用于存放一些不足以称为项目的代码 现在的目录大概是这样的： DEVELOP cpp_develop cpp_archive cpp_project cpp_test cpp_example cpp_lessonwork cpp_pratice py_develop vb_develop web_develop 归档规则项目名称+六位数日期(附加信息) 比如： 词法分析代码高亮190403(修复了xx) 日期是为了手动版本控制，利用肉眼就能知道哪些信息。以前做课设的时候就这样弄的（不过队友都不配合我这样搞，我发的版本是多少，改了之后发过来还是多少……） 总之归档时保证下次打开这个项目时能够唤醒当时编写时的记忆即可。]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[简易倒排索引]]></title>
    <url>%2Fpost%2Fsimple_inverted_index%2F</url>
    <content type="text"><![CDATA[智能信息检索这门课程有个上机作业，题目是“实现倒排索引”。 用到了以前没有学的STL中的vector。 经过两次课上写代码（3小时）加上课后修bug的时间（晚上十点到十二点）总共5个小时，终于完成了一个简易的倒排索引。因为十点时已经太困，喝了柠檬茶提神结果现在睡不着，所以继续熬夜把博客写完吧。 前言勿抄袭代码，代码仅供参考。转载注明出处 倒排索引简介为了从文档集（collection）中检索出想要的结果，首先要将文档集中的每个词项（term）建立索引，以确定词项所在的文档（document）的id，从而返回根据关键字查询的结果。 倒排索引的格式大概是下图这样（代码成果图）： 每一个词项后面跟着它在文档集中出现的次数，以及出现过的文档的id所组成的一个序列。 例如第一条： 词项 词频 倒排记录表 API 6 4，5，6 就代表API这个词在文档集（六个文件）中出现了六次，这六次分布在文档4、文档5和文档6。 搜索引擎大致就是这个原理，建立好了索引之后，只需要把你搜索的关键词对应的posting求交集然后把对应的文档显示出来就可以了。 数据结构设计文档（Document）文档其实在这里就是文件，对于每个文档，都有一个文档名，以及相对应的文档ID，它们得绑定好，否则会混乱。因此将它们放在一个结构体里面。 12345struct Document&#123; string docName;//文档名 int docID;//文档id&#125;; 索引项（IndexItem）同样的，每一个记录的词项、词频和记录表也是绑定的，所以也打包起来。文档id的数目不定，又不想自己写链表或者动态数组怕出错，因此采用了STL（标准模板库）里面的动态数组vector（向量容器）。 123456struct IndexItem&#123; string term;//词项 int frequence;//词频 vector&lt;int&gt; posting;//记录表&#125;; 索引类（CIndex）代码应该不难看懂。 12345678910111213141516171819202122class CIndex&#123; vector&lt;IndexItem&gt; indexList;//索引表 vector&lt;Document&gt; collection;//文档集public: CIndex(); //利用文件名数组初始化文档集 CIndex(string p_collection[], int n); //显示文档集内所有文档的文件名 void showCollection(); //显示当前倒排索引表 void showIndexList(); //索引单篇文档 int indexDocument(FILE*fp, int docID); //索引文档集 int indexCollection(); //排序索引表 int sortIndex(); //索引表合并同类项 int mergeIndex(); ~CIndex();&#125;; 大致思路 扫描一篇文档，将这篇文档对应的文档ID加入对应词项的posting 对文档集中每一篇文档重复第一步，获取所有词项及其对应的posting加入索引表，此时每个词项的posting中只有一个文档ID，并且有很多重复的词项记录； 排序索引表； 将重复的项的posting合并，并且增加词频，删除重复项。 2019-4-4补充：想到一个新思路——直接按照ID从小到大扫描一遍整个文档集，每扫描一个词项，就在词典中查找这个词项，增加词频，然后把现在正在处理的文档的ID加入到posting，最后再排个序即可。 代码实现有参构造函数初始化文档集 12345678910111213//@name &lt;CIndex::CIndex&gt;//@brief &lt;初始化文档集&gt;//@param &lt;string p_collection[]:文档文件名数组&gt;&lt;int n:数组长度&gt;CIndex::CIndex(string p_collection[], int n)&#123; Document nextDoc; for (int i = 0; i &lt; n; i++) &#123; nextDoc.docName = p_collection[i]; nextDoc.docID = i+1;//编号从1开始 collection.push_back(nextDoc); &#125;&#125; 索引单篇文档大致思路是，一个个字符读取进来，如果是字母就一直读完整个单词，并把这个单词作为词项加入表中。 12345678910111213141516171819202122232425262728293031323334353637//@name &lt;CIndex::indexDocument&gt;//@brief &lt;索引单篇文档&gt;//@param &lt;FILE * fp:已打开的文件指针&gt;&lt;int docID:此文件的编号&gt;//@return &lt;扫描到的词项数量&gt;int CIndex::indexDocument(FILE * fp, int docID)&#123; char ch;//扫描用的变量 IndexItem indexItem;//打包用的变量 int num = 0;//扫描到的词项数量 while (!feof(fp)) &#123;//一次循环获取一个单词 //找到第一个字母 do &#123; ch = fgetc(fp); if (feof(fp)) break;//防止空文件导致的无限循环 &#125; while (!isalpha(ch)); if (feof(fp)) break;//防止因文件后面的空行而索引空字符串 //读取单词，给索引项赋值 while (isalpha(ch)) &#123; indexItem.term += ch; ch = fgetc(fp); &#125; indexItem.frequence = 1; indexItem.posting.push_back(docID);//将本文件的文档ID加入posting //把索引项加入词典 indexList.push_back(indexItem); num++; //清空索引项，准备下一次 indexItem.term=""; indexItem.posting.clear(); &#125; return num;&#125; 索引文档集索引文档弄好之后，索引整个文档集不过是加个循环而已 123456789101112131415161718//@name &lt;CIndex::indexCollection&gt;//@brief &lt;索引文档集&gt;//@return &lt;词项总数目&gt;int CIndex::indexCollection()&#123; int num = 0; //打开对应的文件并索引 for (int i = 0; i &lt; collection.size(); i++) &#123; //打开文件 FILE* fp = fopen(collection[i].docName.c_str(), "r"); //索引单篇文档 num+=indexDocument(fp, collection[i].docID); //关闭文件 fclose(fp); &#125; return num;&#125; 排序索引表直接使用&lt;algorithm&gt;头文件里面的sort()函数进行排序，自定义比较函数cmp() 123456789bool cmp(IndexItem a, IndexItem b)&#123; return a.term&lt;b.term;//词项按照从小到大排序&#125;int CIndex::sortIndex()&#123; sort(indexList.begin(), indexList.end(), cmp); return 0;&#125; 去重12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697//@name &lt;CIndex::mergeIndex&gt;//@brief &lt;索引表合并同类项&gt;int CIndex::mergeIndex()&#123; IndexItem item1,item2; sortIndex(); vector&lt;IndexItem&gt;::iterator it_cur=indexList.begin();//创建迭代器 vector&lt;IndexItem&gt;::iterator it_next = it_cur + 1; vector&lt;int&gt; temp; vector&lt;int&gt;::iterator p1, p2;//用于合并posting的迭代器 while (it_cur != indexList.end()) &#123; if(it_cur+1!=indexList.end()) it_next = it_cur + 1; else break; while((*it_cur).term == (*it_next).term) &#123;//这个循环内处理掉所有与当前词项重复的词项 //将二者的posting排序 sort((*it_cur).posting.begin(), (*it_cur).posting.end()); sort((*it_next).posting.begin(), (*it_next).posting.end()); //有序合并两者的posting p1 = (*it_cur).posting.begin(); p2 = (*it_next).posting.begin(); while (p1 != (*it_cur).posting.end() &amp;&amp; p2 != (*it_next).posting.end()) &#123; if ((*p1) &lt; (*p2))//结果集中加入较小的元素 &#123; temp.push_back(*p1); //这个while用于跳过重复的元素 p1++; &#125; else if((*p1) &gt; (*p2)) &#123; temp.push_back(*p2); p2++; &#125; else &#123; temp.push_back(*p1); //遇到相同的则两个都后移，避免出现重复 p1++; p2++; &#125; &#125; while(p1 != (*it_cur).posting.end())//如果串1没有合并完则将串1后面部分直接复制 &#123; temp.push_back(*p1); p1++; &#125; while(p2 != (*it_next).posting.end()) &#123; temp.push_back(*p2); p2++; &#125; //删除结果集重复部分 temp.erase(unique(temp.begin(), temp.end()), temp.end()); (*it_cur).frequence++;//词频增加 (*it_cur).posting.assign(temp.begin(), temp.end());//将结果复制 indexList.erase(it_next);//删除重复项 temp.clear(); if (it_cur + 1 != indexList.end()) it_next = it_cur + 1; else break; &#125; it_cur++; &#125; /*失败代码 for (int i = 0; i &lt; indexList.size()-1; i++) &#123; item1 = indexList[i]; item2 = indexList[i + 1]; int j = 1;//j是相对于item1的偏移量 while (item1.term == item2.term) &#123; vector&lt;int&gt; temp(item1.posting.size()+item2.posting.size()); sort(item1.posting.begin(), item1.posting.end()); sort(item2.posting.begin(), item2.posting.end()); merge(item1.posting.begin(), item1.posting.end(), item2.posting.begin(), item2.posting.end(), temp.begin()); indexList[i].posting.assign(temp.begin(), temp.end()); indexList.erase(indexList.begin()+i+j); indexList[i].frequence++; item2 = indexList[i + j]; &#125; j = 1; &#125; */ return 0;&#125; 一开始使用的是普通的for循环，但是发现随着元素的删除，循环次数应该改变，因此改成了迭代器加while的方式。 迭代器还是个蛮有用的东西，就是一个封装得比较好的指针。 main测试1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;algorithm&gt;#include "CIndex.h"#include &lt;string&gt;using namespace std;string fileList[6] =&#123; "doc1.txt", "doc2.txt", "doc3.txt", "doc4.txt", "doc5.txt", "doc6.txt"&#125;;int main()&#123; CIndex in(fileList,6); in.showCollection(); in.indexCollection(); in.mergeIndex(); in.showIndexList(); return 0;&#125;]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[老鼠和毒药问题]]></title>
    <url>%2Fpost%2Frats_and_poison%2F</url>
    <content type="text"><![CDATA[昨天在上完课回宿舍的路上，楠哥提起了一道他在某个基础知识竞赛上遇到的题目，我觉得解法很巧妙，分享记录一下。 题目有1024瓶水，其中一瓶有毒，你有10只老鼠用于试毒（这里是题目假设，所以别下不了手让老鼠试毒OVO），老鼠如果喝到毒药，会在一星期后死亡。你有一周时间，如何找出这一瓶毒药？ 解法楠哥说他刚开始想用二分，可是时间上不允许。 也就是把瓶子分两组，每组的瓶子里都倒出一点混合在一起给一只老鼠喝，哪一组的老鼠中毒了，就再把这一组的瓶子分两组，以此类推。但是这样时间上来不及，第一周缩小范围到512瓶……第九周2瓶，第十周找到。耗时太长。 于是他想到了另一种解法： 给每个瓶子标号，给老鼠也标号0到9。 从逻辑上将10只老鼠当成10位的二进制数。 将瓶子的编号转换为二进制数，比如第5号瓶子转换为第101号瓶子，将编号第0位（即最右边一位）为1的水给0号老鼠喝，编号第1位（即从右边数第二位）为1的水给1号老鼠喝，以此类推。 也就是说，0号老鼠喝了1,11,101,111……这些瓶子的水，1号老鼠喝了10,11，110,111……这些瓶子的水，后面的老鼠也是如此。 如果一周时间到，0号老鼠嗝屁了，那么就说明有毒的水的编号的第0位（最右边的位）为1；如果1号老鼠嗝屁了，就说明有毒的水编号的第1位是1…… 最后根据10只老鼠中毒情况，得到一个10位的二进制数，这个数转换为十进制就是毒药的编号。 我觉得这个解法很巧妙。 这让我想起了在听我们学校ACM协会的某节课的时候提到的状态压缩，也是使用二进制的，不过我当时没听懂，也就没记下来。 老鼠有10只，它们的死活可以表示2^10种状态，恰好是1024种。]]></content>
      <categories>
        <category>算法模型</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记3封装爬虫类]]></title>
    <url>%2Fpost%2Fpython_spider_note3class_spider%2F</url>
    <content type="text"><![CDATA[在完成了基本的爬取任务之后，接到了将其封装为一个爬虫类的任务 传送门： python爬虫学习笔记1一个简单的爬虫 python爬虫学习笔记2模拟登录与数据库 前言转载注明出处。 任务介绍1、尝试不使用session去进行爬取，最好能将cookies保存下来可以供下次使用。2、第二个是尝试将这些封装成面向对象的方式，模拟登陆，爬取，解析，写入数据库这几个部分分离开来。 先做第二个任务 过程记录190310 周日创建爬虫类12345678class spider: ''' 爬虫类 ''' def __init__(self): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 获取登录所需信息获取登录信息（账号密码以及校验码）这部分与登录可以分开，单独写一个成员函数。 在输入密码这个地方，本来查到可以使用getpass这个库里面的getpass()函数来使用类似linux的密码不回显，用法如下： 123import getpasspasswd=getpass.getpass()print(passwd)#测试用输出 但是直接在pycharm里面运行是会卡在输入那里，并且也会回显。后来查到了，这个方法是在命令行当中才管用，我试了一下在python命令行中使用， 123456&gt;&gt;&gt;import getpass&gt;&gt;&gt;passwd=getpass.getpass()Warning: Password input may be echoed.Password: &gt;? 123&gt;&gt;&gt;print(passwd)123 虽然可以使用了，但是仍然会回显。所以这个命令行说的应该不是python命令行，而是cmd或者shell。 在虚拟环境的cmd里面，成功了，Password后面未回显我的输入，下面的数字是测试用的输出，将密码打印出来。 123(venv) F:\DEVELOP\py_develop\spider&gt;python test.pyPassword:123 不过为了方便调试代码，我还是使用了input()函数 参考链接： python3-password在输入密码时隐藏密码-博客园 Python之控制台输入密码的方法-博客园 12345678910111213141516171819202122232425def get_login_data(self,login_url): ''' 获取登录需要的数据 :param login_url: 登录页面url :return: 一个存有登录数据的字典 ''' # 获取登录校验码 html = self.session.post(login_url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') lt = soup.find('input', &#123;'name': 'lt'&#125;)['value'] dllt = soup.find('input', &#123;'name': 'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data = &#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn': '', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data 登录123456789101112131415def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ login_data=self.get_login_data(login_url)#获取登录信息 # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url:#如果没有跳转回登录页面，那么就是登录成功 print("登录成功") self.is_login=True else: print("登录失败") return self.session day8进度 了解了一下Python类与对象的语法，尝试将代码封装到类中（一些中间代码未保留），不过想要将它改的有通用性（能够爬取其他网站）有些困难，还是先固定只能爬取信息门户 接下来的计划：将类完成之后再慢慢优化，学习使用cookie代替session保持登录，以及数据库的更多知识 190311 周一day9进度 图书馆借了一本mysql的书籍，在mysql命令行上练习创建数据库，表以及字段的操作 在将代码封装成类的过程中，学习了如何将参数作为一个字典传入，以及将一个字典作为参数传入 190312 周二获取单页目录内的公告url目录网页的内容： 关于……的通知 关于……获奖 …… 2700条记录，分为138页显示，下一页 123456789101112131415161718192021222324252627def get_url_from_cata(self,url,params): ''' 返回当前页面的url组成的列表 :param url: 无参数的url#如：http://portal.xxx.edu.cn/detach.portal :param params:url的？后参数#如：?pageIndex=1 :return:以页面指向的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取url域名部分 #如：http://portal.xxx.edu.cn base=url.split('/') base=base[0]+'//'+base[2] #获取当前页所有链接 html = self.session.post(url,params=params).text#用params参数来拼接参数 soup = BeautifulSoup(html, 'lxml') rss_title = soup.find_all('a', class_='rss-title')#获取所有链接 result_list=[] for url in rss_title: title=url.get_text().strip() page_url=base+'/'+url['href']#将url拼接完整 l=(title,page_url) result_list.append(l) #print(result_list) return result_list 获取所有目录内的公告url1234567891011121314151617181920212223242526272829303132333435def get_url_from_cata_all(self, url): ''' 获取页面的底部跳转到其他页的链接并获取目录，给出一个目录页的url，获取相关的所有目录页的url并获取链接 :param url: 其中任何一个目录页的url#如：http://portal.xxx.edu.cn/detach.portal?pageIndex=1 :return:以所有页面的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取除去参数之后的url #如：http://portal.xxx.edu.cn/detach.portal base=url.split('?')[0] html = self.session.post(url).text soup = BeautifulSoup(html, 'lxml') # 获取页数 reg = '共.*?条记录 分(.*?)页显示' reg = re.compile(reg, re.S) num = int(re.findall(reg, html)[0]) #获取url para = &#123; 'pageIndex': 1, 'pageSize': '', '.pmn': 'view', '.ia': 'false', 'action': 'bulletinsMoreView', 'search': 'true', 'groupid': 'all', '.pen': 'pe65' &#125; ret=[] for i in range(1,num+1): ret.extend(self.get_url_from_cata(base,params=para)) para['pageIndex'] = i return ret day10进度实现了自动获取目录页数，并从每一页目录获取所有的url，返回当前所有公告的url的列表 190313 周三获取正文123456789101112131415def get_page(self,url): ''' 提取页面中的公告正文 :param url: 页面url :return: 正文 ''' html = self.session.post(url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') bulletin_content = soup.find('div', class_='bulletin-content') bulletin_content =bulletin_content.get_text() return bulletin_content 保存到txt123456789101112131415def save_by_txt(self,file_content,file_name): ''' 获取单个公告页面的公告并保存到txt :param file_content:文件内容(str) :param file_name:输出文件名(str) :return:无 ''' # 转换为可以作为文件名字的形式 reg = r'[\/:*?"&lt;&gt;|]' file_name = re.sub(reg, "", file_name) with open(file_name, 'w', encoding='utf8') as fout: fout.write(file_content) print('成功保存到&#123;&#125;'.format(file_name)) 保存到db1234567def save_by_db(self,content,title): #未改造完成 db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') cursor = db.cursor() cursor.execute("insert into spider(`title`,`content`) values('&#123;0&#125;','&#123;1&#125;')".format(title, content)) db.commit() print('已经成功保存公告到数据库：“&#123;&#125;”'.format(title)) day11进度尝试将保存到数据库的函数里面的数据库参数放到函数形参处，怎么弄都觉得不太合适，于是还是将原本的代码放入 190314 周四cookie保持登录参考链接： Python——Cookie保存到本地-知乎（解决了问题的主要链接） 爬虫保存cookies时重要的两个参数（ignore_discard和ignore_expires）的作用 首先是库 1import http.cookiejar 初始化12345def __init__(self,headers): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 self.headers=headers#头信息 self.cookiejar=http.cookiejar.LWPCookieJar('cookie.txt') 保存cookie的函数大概是将已登录的session对象的cookies转换为字典（用了一个类似列表生成式的东西，查了一下，是字典生成式，python还真是方便，这么多简写方式），然后保存到cookiejar对象中，调用save()函数来将cookie内容保存到第一个参数指定的文件中，即使cookie已经被抛弃和过期。 1234def save_cookie(self): requests.utils.cookiejar_from_dict(&#123;c.name: c.value for c in self.session.cookies&#125;, self.cookiejar) # 保存到本地文件 self.cookiejar.save('cookies', ignore_discard=True, ignore_expires=True) 加载cookie的函数首先初始化一个LWPCookieJar对象 1load_cookiejar = http.cookiejar.LWPCookieJar() 接着从文件中加载cookie 1load_cookiejar.load('cookies', ignore_discard=True, ignore_expires=True) 这里有个问题，这里如果加载失败了（没有这个文件，之前没有保存），需要知道已经失败了。所以使用一个try语句块测试一下。 然后把这个LWPCookieJar对象给转换成字典，再转换赋值给session.cookie，这样就加载成功了 1234567891011121314151617def load_cookie(self): ''' 加载cookie :return: 是否成功 ''' load_cookiejar = http.cookiejar.LWPCookieJar() # 从文件中加载cookies(LWP格式) try: load_cookiejar.load('cookies', ignore_discard=True, ignore_expires=True) except: return False # 转换成字典 load_cookies = requests.utils.dict_from_cookiejar(load_cookiejar) # 将字典转换成RequestsCookieJar，赋值给session的cookies. self.session.cookies = requests.utils.cookiejar_from_dict(load_cookies) return True 修改后的login()1234567891011121314151617181920def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ if self.load_cookie(): self.is_login = True else: #获取登录信息 login_data=self.get_login_data(login_url) # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url: print("登录成功") self.is_login=True self.save_cookie() else: print("登录失败") return self.session day12进度 完成了爬虫类的封装 使用http.cookiejar库实现了登录一次，在cookie有效期内不必再次登录的功能 代码总览import12345import requestsfrom bs4 import BeautifulSoupimport pymysqlimport reimport http.cookiejar 构造函数12345678910class spider: ''' 爬虫类 ''' def __init__(self,headers): self.session=requests.session()#初始化登录session self.is_login=False#登录状态 self.headers=headers#头信息 self.cookiejar=http.cookiejar.LWPCookieJar('cookie.txt') 获取登录信息12345678910111213141516171819202122232425def get_login_data(self,login_url): ''' 获取登录需要的数据 :param login_url: 登录页面url :return: 一个存有登录数据的字典 ''' # 获取登录校验码 html = self.session.post(login_url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') lt = soup.find('input', &#123;'name': 'lt'&#125;)['value'] dllt = soup.find('input', &#123;'name': 'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data = &#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn': '', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; return login_data 登录12345678910111213141516171819202122def login(self,login_url): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ if self.load_cookie(): self.is_login = True else: #获取登录信息 login_data=self.get_login_data(login_url) # 登录 response = self.session.post(login_url, headers=self.headers, data=login_data) if response.url!=login_url: print("登录成功") self.is_login=True self.save_cookie() else: print("登录失败") return self.session 获取单页目录1234567891011121314151617181920212223242526def get_url_from_cata(self,url,params): ''' 返回当前页面的url组成的列表 :param url: 无参数的url :param params:url的？后参数 :return:以页面指向的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取url域名部分 base=url.split('/') base=base[0]+'//'+base[2] #获取当前页所有链接 html = self.session.post(url,params=params).text#用params参数来拼接参数 soup = BeautifulSoup(html, 'lxml') rss_title = soup.find_all('a', class_='rss-title')#获取所有链接 result_list=[] for url in rss_title: title=url.get_text().strip() page_url=base+'/'+url['href']#将url拼接完整 l=(title,page_url) result_list.append(l) #print(result_list) return result_list 获取全部目录123456789101112131415161718192021222324252627282930313233def get_url_from_cata_all(self, url): ''' 获取页面的底部跳转到其他页的链接并获取目录，给出一个目录页的url，获取相关的所有目录页的url并获取链接 :param url: 其中任何一个目录页的url :return:以所有页面的标题和url组成的元组为元素的列表，即[(title,content),(title,content)]的形式 ''' #获取除去参数之后的url base=url.split('?')[0] html = self.session.post(url).text soup = BeautifulSoup(html, 'lxml') # 获取页数 reg = '共.*?条记录 分(.*?)页显示' num = int(re.findall(reg, html)[0]) #获取url para = &#123; 'pageIndex': 1, 'pageSize': '', '.pmn': 'view', '.ia': 'false', 'action': 'bulletinsMoreView', 'search': 'true', 'groupid': 'all', '.pen': 'pe65' &#125; ret=[] for i in range(1,num+1): ret.extend(self.get_url_from_cata(base,params=para)) para['pageIndex'] = i return ret 获取正文123456789101112131415def get_page(self,url): ''' 提取页面中的公告正文 :param url: 页面url :return: 正文 ''' html = self.session.post(url, headers=self.headers).text soup = BeautifulSoup(html, 'lxml') bulletin_content = soup.find('div', class_='bulletin-content') bulletin_content =bulletin_content.get_text() return bulletin_content 保存到txt123456789101112131415def save_by_txt(self,file_content,file_name): ''' 获取单个公告页面的公告并保存到txt :param file_content:文件内容(str) :param file_name:输出文件名(str) :return:无 ''' # 转换为可以作为文件名字的形式 reg = r'[\/:*?"&lt;&gt;|]' file_name = re.sub(reg, "", file_name) with open(file_name, 'w', encoding='utf8') as fout: fout.write(file_content) print('成功保存到&#123;&#125;'.format(file_name)) 保存到数据库123456def save_by_db(self,content,title): db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') cursor = db.cursor() cursor.execute("insert into spider(`title`,`content`) values('&#123;0&#125;','&#123;1&#125;')".format(title, content)) db.commit() print('已经成功保存公告到数据库：“&#123;&#125;”'.format(title)) 保存cookie1234def save_cookie(self): requests.utils.cookiejar_from_dict(&#123;c.name: c.value for c in self.session.cookies&#125;, self.cookiejar) # 保存到本地文件 self.cookiejar.save('cookies', ignore_discard=True, ignore_expires=True) 加载cookie123456789101112131415161718def load_cookie(self): ''' 加载cookie :return: 是否成功 ''' load_cookiejar = http.cookiejar.LWPCookieJar() # 从文件中加载cookies(LWP格式) try: load_cookiejar.load('cookies', ignore_discard=True, ignore_expires=True) except: print('cookie加载失败') return False # 转换成字典 load_cookies = requests.utils.dict_from_cookiejar(load_cookiejar) # 将字典转换成RequestsCookieJar，赋值给session的cookies. self.session.cookies = requests.utils.cookiejar_from_dict(load_cookies) return True 爬取12345678def crawl(self,login_url,cata_url): self.login(login_url)#登陆 item_list=self.get_url_from_cata_all(cata_url)#获取所有标题以及对应链接 for i in item_list: title,url=i#解包 text=self.get_page(url)#获取内容 self.save_by_txt(text,title+'.txt')#保存 #self.save_by_db(text,title) 调用123456789headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36',&#125;login_url='http://xxx.xxx.xxx.cn/authserver/login?service=http%3A%2F%2Fportal.chd.edu.cn%2F'cata_url='http://xxxxxx.xxx.xxx.cn/detach.portal?pageIndex=1&amp;pageSize=&amp;.pmn=view&amp;.ia=false&amp;action=bulletinsMoreView&amp;search=true&amp;groupid=all&amp;.pen=pe65'#调用spiderman=spider(headers)spiderman.crawl(login_url, cata_url)]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记2模拟登录与数据库]]></title>
    <url>%2Fpost%2Fpython_spider_note2login_and_database%2F</url>
    <content type="text"><![CDATA[为了加入学校里面一个技术小组，我接受了写一个爬取学校网站通知公告的任务。这个任务比以前写的爬虫更难的地方在于，需要模拟登录才能获得页面，以及将得到的数据存入数据库。 本文按照日期来记录我完成任务的过程，然后再整理一遍全部代码。读者可以通过侧栏目录跳转阅读。不介绍库的安装。 传送门：爬虫学习笔记1 转载声明关于参考链接：本文用到的其他博客的链接都以（我自己对内容的概括或者文章原标题-来源网站-作者名）的格式给出，关于作者名，只有博客作者自己明确声明为“原创”，我才会加上作者名。引用的文章内容我会放在来源链接的下方。 关于本文：我发一下链接都注明出处了，如果想转载，也请这样做。作者憧憬少，链接的话看浏览器地址栏。 任务介绍爬取信息门户新闻并且存入数据库。 首先分解任务： 实现爬取综合新闻页面的公开新闻存入markdown文件中(190303完成) 将数据存到数据库（190304完成） 学习模拟登录（190305到190307完成） 爬取信息门户新闻（190308完成） （进阶）将代码进行封装、优化（目前未封装） （进阶）动态更新（目前未着手） 过程记录190303 周日练习爬取公开页面我的第一个爬虫是在2月多的时候在家写的，那个只是个简单的爬虫，目标是公开的页面，不需要模拟登录，也不需要存储到数据库，直接存到txt文件中。 先爬取学校官网的综合新闻页面复习一下。 首先讲一下我的思路： 由于新闻和公告页面通常是有一个目录页面的，也就是包含子页面的链接，在目录的子页面内才是正文内容。 假设这一页目录有三个新闻，就像是下面： 新闻目录 新闻一 新闻二 新闻三 点击查看下一页 这样的结构。 如果要写一个爬虫函数来爬取所有新闻页面，那么就要从目录着手。目录中含有前往别的新闻页面的链接，所以可以在目录页获取本页所有新闻的链接，遍历所有链接并提取新闻内容。 至于翻页也可以这样做到，“下一页”按钮也是一个链接，可以通过这个链接获取到下一页的内容。翻页部分原理比较简单，我是先攻克其他难关，把它留到最后写的。 提取单页面新闻首先是提取单个页面的新闻。向目标url发出访问请求： 1234567891011import requestsdef getNews(url): ''' 提取页面的新闻与图片并存储为markdown文件 :param url: 要爬取的目标网页url :return: 无 ''' #发送请求 r=requests.get(url)#r为response对象 html=r.text#r.text是请求的网页的内容 print(html) 编码问题这里遇到了第一个问题，提取到的页面有乱码。 解决方法：先获取响应对象的二进制响应内容，然后将其编码为utf8 参考链接： python中response.text与response.content的区别-CSDN requests.content返回的是二进制响应内容 而requests.text则是根据网页的响应来猜测编码 UNICODE,GBK,UTF-8区别（一个比较好的编码的教程，便于理解编码的概念）-博客园 Python解决抓取内容乱码问题（decode和encode解码）-CSDN-浅然_ 字符串在Python内部的表示是unicode编码，在做编码转换时，通常需要以unicode作为中间编码，即先将其他编码的字符串解码（decode）成unicode，再从unicode编码（encode）成另一种编码。 decode的作用是将其他编码的字符串转换成unicode编码，如str1.decode(‘gb2312’)，表示将gb2312编码的字符串str1转换成unicode编码。 encode的作用是将unicode编码转换成其他编码的字符串，如str2.encode(‘utf-8’)，表示将unicode编码的字符串str2转换成utf-8编码。 修改代码为： 1234#发送请求r=requests.get(url)html=r.content#获取二进制字节流html=html.decode('utf-8')#转换为utf8编码（该网页使用的是utf8编码） 解析网页（bs4）一开始我和之前一样使用正则表达式来提取，但是不够熟悉，总是写不出匹配的上的正则表达式。还是使用另一个东西——BeautifulSoup库 具体如何使用请查看其他教程，本文只说我自己用到的部分。 参考链接： Python爬虫常用的几种数据提取方式-CSDN-凯里潇 零基础入门python3爬虫-bilibili（里面的视频p11） beautifulsoup（基本选择器，标准选择器，css选择器）-CSDN-Halosec_Wei（基本上是上面一个b站链接的文字版，不知道是不是同一个人） beautifulsoup详细教程-脚本之家 beautifulsoup基本用法总结-CSDN-kikay BeautifulSoup是Python的一个库，最主要的功能就是从网页爬取我们需要的数据。BeautifulSoup将html解析为对象进行处理，全部页面转变为字典或者数组，相对于正则表达式的方式，可以大大简化处理过程。 我目前的理解是，这个BeautifulSoup库需要用到其他html解析库，可以使用python自带的，也可以安装第三方库，其他的库就像功能扩展插件一样，没有的话它自己也能解析。我安装了名为lxml的解析库。 查看源代码，找到网页中有关新闻的代码，手动将其格式化之后如下（内容不重要，省略）： 12345678910111213141516171819202122232425&lt;h1 class="arti-title"&gt;标题省略&lt;/h1&gt;&lt;p class="arti-metas"&gt; &lt;span class="arti-update"&gt;发布时间：2019-01-23&lt;/span&gt; &lt;span class="arti-update1"&gt;作者：xx&lt;/span&gt; &lt;span class="arti-update2"&gt;来源：xxx&lt;/span&gt;&lt;/p&gt;&lt;div class="entry"&gt; &lt;article class="read"&gt; &lt;div id="content"&gt; &lt;div class='wp_articlecontent'&gt; &lt;p&gt;新闻前言省略 &lt;/p&gt; &lt;p&gt;&lt;br /&gt;&lt;/p&gt; &lt;p&gt;新闻内容省略 &lt;/p&gt; &lt;p&gt; &lt;img width="556" height="320" align="bottom" src="url省略" border="0"&gt; &lt;/p&gt; &lt;p style="text-align:right;"&gt;（审稿：xx &amp;nbsp;网络编辑：xx） &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; 接着上面的代码： 123456789#解析htmlsoup=BeautifulSoup(html,"lxml")#返回已解析的对象#获取标题title=soup.find('h1',class_='arti-title').string#获取时间update=soup.find('span',class_='arti-update').string#获取正文标签content=soup.find('div',class_='wp_articlecontent') 提取图片我打算将新闻保存到markdown文件中，提取新闻中的图片的链接的地址，这样在md文件中就能显示出图片了。 1234567#获取图片链接base='学校官网url，用于和img标签中的相对地址拼接成绝对地址'imgsTag=content.find_all('img')imgsUrl=[]for img in imgsTag: imgsUrl.append(base+img['src'])#拼接成完整的url img.extract()#删除图片标签 删除多余标签123456#删除多余标签for p in content.find_all('p',&#123;'style':"text-align:center;"&#125;): p.extract()p=content.find('p', &#123;'style': "text-align:right;"&#125;)if(p!=None): p.extract() 保存到文件123456789101112# 拼接成字符串#后来知道这样的提取方式其实不能完全提取到所有内容fileContent=''for i in content.contents:#遍历正文内容的所有子标签 if(i.string!=None):#如果子标签里面有内容 #print(i.string)#调试 fileContent+=i.string#基本只剩下p标签了 fileContent+='\n\n' #保存到md文件with open('data.md','w') as fout: fout.write(fileContent) 代码总览12345678910111213141516171819202122232425262728293031323334353637383940414243444546import requestsfrom bs4 import BeautifulSoup#第4个版本改名bs4而不是全名那么长了def getNews(url): ''' 提取页面的新闻与图片并存储为markdown文件 :param url: 要爬取的目标网页url :return: 无 ''' #发出请求 r=requests.get(url) html=r.content html=html.decode('utf-8')#转换编码 #解析html soup=BeautifulSoup(html,"lxml") content=soup.article #获取标题 title=soup.find('h1',class_='arti-title').string #获取时间 update=soup.find('span',class_='arti-update').string #获取正文 content=soup.find('div',class_='wp_articlecontent') #获取图片链接 base='http://xxxxx.xxx'#学校官网url，用于和img标签中的相对地址拼接成绝对地址 imgsTag=content.find_all('img') imgsUrl=[] for img in imgsTag: imgsUrl.append(base+img['src'])#拼接成完整的url img.extract()#删除图片标签 #删除多余标签 for p in content.find_all('p',&#123;'style':"text-align:center;"&#125;): p.extract() p=content.find('p', &#123;'style': "text-align:right;"&#125;) if(p!=None): p.extract() # 拼接成字符串 fileContent='' for i in content.contents: if(i.string!=None): #print(i.string)#调试 fileContent+=i.string fileContent+='\n\n' with open('data.md','w') as fout: fout.write(fileContent) 提取多页面新闻原理在上面说了，提取完单页基本上就完成了。 12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupdef getNewsContents(url): ''' 爬取目录页面链接到的页面 :param url: 新闻目录页面的url :return: 无 ''' #获取网页内容 r=requests.get(url)#以get方式访问 html=r.content html=html.decode('utf-8') #获取每篇新闻的链接 base='http://xxxxx.xxx'#学校官网url，用于和相对地址拼接成绝对地址 soup=BeautifulSoup(html,'lxml') for page_url in soup.find_all('a',class_='column-news-item'): page_url=base+'/'+page_url['href'] print(page_url) getNews(page_url)#调用提取单页函数 day1进度 实现爬取长安大学综合新闻页面的公开新闻存入markdown文件中 复习了requests库的使用 学习了BeautifulSoup4库的基本使用 190304 周一这一天主要是将前一天爬取的数据存入数据库。 将数据存入数据库安装MySQL数据库参考链接： 零基础入门python3爬虫-bilibili（里面的视频p4） 使用MySQL WorkbenchMySQL Workbench是一个可视化工具，安装MySQL的时候自带（我安装的是最新版的），在安装目录找到它的exe然后加个快捷方式在桌面，可以方便地查看数据和执行SQL查询指令，具体使用方法可以问度娘。我现在也不是很会。 我创建的数据库名为news，里面创建了一个数据表chdnews。 连接数据库和大多数数据库一样，MySQL是C/S模式的，也就是客户端（client）/服务端（server）模式的。数据库有可能在远程服务器上。想要使用数据库，就需要连接到数据库。 python中要使用数据库需要一个pymysql库。 下面是连接的代码： 123import pymysql#连接数据库db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') 这个连接函数看参数名就可以看出含义了。 host：主机ip，127.0.0.1是回传地址，指本机。也就是连接本电脑的MySQL的意思 port：端口号，用来和ip一起指定需要使用数据库的软件。在安装的时候会让你设置，默认3306 user&amp;passwd：用户名和密码，在安装的时候已经设置好了 db：你要连接的数据库的名字。一台电脑上可以有很多数据库，数据库里面可以有很多数据表。 charset：字符编码 插入数据接着可以准备一个游标，游标大概是一个用于存储结果集开头地址的指针吧，我是这么理解的。在我学了更多数据库知识后可能会更新这一部分。 12#创建游标cursor = db.cursor() 接着执行SQL的插入语句： 12#插入cursor.execute("insert into chdnews(`title`,`article`) values('&#123;0&#125;','&#123;1&#125;')".format(title,fileContent))#此处变量为上文代码中的变量 这里的SQL语句是这样的： 1insert into 数据表名(字段名1，字段名2) values(值1，值2) 后面的format函数是python的格式化函数，将变量的值加入到字符串中对应位置。 最后提交： 12#提交更改db.commit() 接着打开workbench，就会发现已经存入数据库了。（你得把代码放在上面提取单页新闻的函数那里，放在保存到文件的那部分代码那儿） day2进度 下载并安装MySQL以及MySQL Workbench 使用pymysql库进行数据库的连接，实现了把第一天得到的数据存入数据库 190305 周二初步了解模拟登录最后的任务需要爬取登录后才能查看的页面，于是我去搜索了很多博客，只放一部分对我有帮助的链接。 参考链接： 模拟登录CSDN-博客园 模拟登录github-博客园 首先查看一下需要的登录数据： 打开登录网页，用F12打开开发者工具，选择network（网络）选项卡 登录你的账号，此时控制台会显示一大堆请求与响应，找到以post方式发送的请求，一般排在第一个 那里会显示几个栏目，找到Form Data（表单数据），这个里面是你填写登录表单之后使用POST方式发送给服务端的内容。这里面除了自己填写的账号密码之外还有一些东西，比如下图的lt,dllt,execution,_eventId,rmShown这些都是在表单的隐藏域中，查看登录页面的源代码是可以看的到的。这些隐藏起来的东西是为了检验你是否是从浏览器进来的，只要获取到这些东西，再加上头部信息，就能伪装成浏览器了 至于头部信息，在下图也可以看到我折叠起来的几个栏目，有一个是Request Headers，这是我们在点击登录按钮时发送的POST请求信息的信息头。将里面的User-Agent给复制到你代码里面存在一个字典里面等会用 把头部信息和表单数据都看一下，准备一下 12345678910111213141516171819202122#登录前的准备login_url = 'http://xxxx.xxx'#登录页面的url#头部信息headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' #加上后面这些会后悔的，别加。 'Host':'xx.xx.xx.xx', 'Referer':'http://xxx.xxx?xxx=http://xxx.xx', 'Origin':'http://xxx.xxx.xx'&#125;#登录用的数据login_data=&#123; 'username': '你的账号', 'password': '你的密码', 'btn':'', 'lt': LT-790162-J9kW2aEFsK3ihu4AzXcovdsJy6cYBM1552123884047-D1Nx-cas， #实际上lt并不能这样写上去，下文会解释。这里记录我自己的错误 'dllt': 'userNamePasswordLogin', 'execution': 'e1s1', '_eventId': 'submit', 'rmShown': 1 &#125; 数据准备好之后就开始登录，使用的是requests的另一个方法——post。 向服务器发出请求（request）的方式有get和post，查看html源代码的时候在表单标签处可以看到表单提交的方法。如： 1&lt;form id="casLoginForm" method="post"&gt; 像这样写html代码会让浏览器在你按下登录按钮的时候以post的方式提交表单，也就是以post的方式向服务器发起request，将form data发送过去。 post方法的好处是在发送过程中会隐藏你的表单数据，不会被直接看到； 而前面使用过的get方法，会把你的表单数据加在url后面，网址后边以问号开头，以&amp;连接的就是发送过去的参数。 涉及登录用post比较好，以免轻易泄露密码。 12#以post方式发出登录请求r=requests.post(login_url,headers=headers,data=login_data) 按理来说应该可以了呀，为什么不行？仍然得到登录页面。在这一天我折腾了很久，没有得到答案。 不过在找资料时却学到了其他的一些知识，关于cookie和session。 cookie和session我目前的理解（如果不对欢迎留言）： http是无状态协议，两次访问都是独立的，不会保存状态信息。也就是你来过一次，下次再来的时候网站还是当你第一次来。那么怎么知道你来过，从而给你还原之前的数据呢？就有人想出cookie和session两种方式。 cookie（直译：小甜饼）是服务端（网站服务器）收到客户端（你电脑）的request（请求）的时候和response（响应）一起发给客户端的数据。客户端把它存在文件里面，并在下一次访问这个网站时将cookie随着request一起发送过去，这样服务端就会知道你就是之前来过的那个人了。cookie存储在客户端。 客户端发送request 服务端发送response附带一个cookie（一串数据） 客户端第二次访问时把cookie复制一份一起发过去 服务端看到你的cookie就知道你是谁了 session（会话）是在服务端内存中保存的一个数据结构，一旦有客户端来访问，那么就给这个客户端创建一个新的session在服务端的内存，并将它的session ID随着response发回给客户端。客户端第二次访问时，会将被分配的SID随着request一起发过来，服务端在这边验证SID之后就会知道你来过。session存储在服务端。 客户端发送request 服务端发送response并在自己这边创建一个session（一堆数据）并发送一个session ID给客户端 客户端第二次访问时把session ID一起发过去 服务端看到你的session ID就知道你是谁了 不过这俩是用来保持登录的，我还没登录成功想这个干啥？请看下一天。 day3进度 初步了解cookie和session的概念 了解如何使用chrome浏览器的控制台查看post表单信息 尝试使用requests的post方法模拟登录，失败，返回登录页面 190306 周三表单校验码（非验证码）怎么弄都不成功，都跳回登录页面。我只好去询问组长这是为什么。 原来我没发现表单校验码会变的！ 一直没注意啊啊啊啊啊啊！ 我没有认真比对过两次打开的乱码不一样，看结尾一样就以为一样了。其中的lt这个域每次打开网页都是不一样的，随机出的！ 既然知道了问题，就好解决了。 123456789101112131415161718#获取登录校验码html=requests.post(login_url,headers=headers).textsoup=BeautifulSoup(html,'lxml')lt=soup.find('input',&#123;'name':'lt'&#125;)['value']dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value']execution = soup.find('input', &#123;'name': 'execution'&#125;)['value']_eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value']rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value']login_data=&#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown&#125; 为了保险，我把其他的表单域也给解析赋值给变量了。 不过仍然无法登陆成功，而是进入了一个诡异的页面: 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;style&gt; body &#123; width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;&lt;p&gt;If you see this page, the nginx web server is successfully installed andworking. Further configuration is required.&lt;/p&gt;&lt;p&gt;For online documentation and support please refer to&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;Commercial support is available at&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 确实有进展，但是这是啥？nginx？查了一下是一个高性能的HTTP和反向代理服务器，但是和我现在登录有什么关系呢？（黑人问号.jpg） 利用session保持校验码即使登录成功，还有一个问题无法解决，那就是我获取校验码的request和登录用的request是两次不同的访问请求呀，这样校验码又会变化。 我想起了前一天看到的session，这玩意不就能让服务端记住我？（cookie试了一下，保存下来的是空的文件不知道怎么回事） 于是新建一个会话： 12#新建会话session=requests.session() 在获取校验码的时候改成使用session变量来发起请求： 12#获取登录校验码html=session.post(login_url,headers=headers).text 这里的session是在客户端创建的，并不是服务端那个，我想它可能存储的是服务端发送过来的session ID吧。 同理在正式发送请求时这样： 12#登录r=session.post(login_url,headers=headers,data=login_data) 这样就能让服务端知道我是刚刚获取校验码的那个小伙汁：D 在这一天我没有办法验证是否有效，不过在之后我验证了这个方法的成功性。 day4进度 知道了原来有个每次会变化的校验码“lt”，找到了跳转回登录页面的原因。使用Beautifulsoup来获取每次的校验码，不过仍然没有解决无法登录的问题 使用session对象来保证获取校验码和登录时是同一个会话，未验证 190307 周四多余的头部信息我终于发现了问题所在！！！！！ 1234567headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36', #'Host':'xxx.xxx.xxx.xxx', #'Referer':'http://xxx.xxx.xxx.xxx...',#不详细打码了 #'Origin':'http://xxx.xxx.xxxx' #去掉多余的头信息才成功登录！！！！！卡了很久没想到是因为这个&#125; 头部信息写多了，我只保留了User-Agent之后成功登录了，你们能体会到我当时有多开心吗！ 我将成为新世界的卡密小组里面最快完成的人！ 解决了这个问题，剩下的就特别简单了。 当时我有一个下午的时间，于是我将进度迅速推进。 爬取通知公告设登录页面为pageA，登录之后的页面跳转到pageB，而pageB有一个按钮跳转到pageC，这个pageC就是day1的时候的目录页面，里面有着pageC1、pageC2、pageC3……等页面的链接，而这个pageC最后面还有个按钮用于跳转到目录的下一页，也就是pageC?pageIndex=2，还有137页公告栏目录。 没有什么新的东西，和day1说的爬取方式差不多，只是页面正文的格式和day1的新闻不太一样。核心结构如下，我省略了很多： 12345678910111213141516&lt;html&gt; &lt;body&gt; &lt;div class="bulletin-content" id="bulletin-contentpe65"&gt; &lt;p style=";background: white"&gt; &lt;a name="_GoBack"&gt; &lt;/a&gt; &lt;span style="font-size: 20px;font-family: 仿宋"&gt; 校属各单位： &lt;/span&gt; &lt;/p&gt; &lt;p&gt; &lt;br/&gt; &lt;/p&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 大概就是一个&lt;p&gt;标签里面放一个或多个&lt;span&gt;标签，而这里面可能还会嵌套几个&lt;span&gt;标签，里面才有内容，而两个内部的&lt;span&gt;之间还可能有内容。 这要怎么解析？ 在尝试了很多方案之后，我终于百度到一个函数： 1tag.get_text()#提取名为tag的bs4标签的内部的所有文字 参考链接： BeautifulSoup获取标签中包含的文字-CSDN-niewzh（正是这个博客解决了我的问题） BeautifulSoup中的.text方法和get_text()方法的区别-CSDN 解决方案： 12345678910#获取正文内容html=session.post(url,headers=headers).textsoup=BeautifulSoup(html,'lxml')article=soup.find('div',class_='bulletin-content')news_content=''for p in article.find_all('p'): if p.span!=None:#如果p含有一层span text=str(p.get_text()).strip()#获取内容并去除多余空格 news_content+=text+'\n' 接着我就把爬下来的东西存到数据库里面去了。弄完之后得去赶作业了，这一天的时间用完了。 day5进度1.找到无法登录且跳转到未知页面的原因是头部信息加了多余的值，解决之后成功登录到信息门户，实现模拟登陆2.利用之前爬取单个页面到文件的方法，用beautifulsoup解析并保存内容到文件3.存入MySQL数据库中4.还差爬取多页目录的功能，预计明天完成。整理代码后可提交 190308 周五更多的目录页开了一个新文件准备整理一下代码，并完成最后一个功能——爬取完目录页第一页之后爬取后面更多的页。 查看源代码的时候，找“第二页”这个按钮对应的链接，发现了规律： 12345678910111213&lt;div class="pagination-info clearFix"&gt; &lt;span title='共2740条记录 分137页显示'&gt; 2740/137 &lt;/span&gt; &lt;a href='detach.portal?pageIndex=1&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第1页'&gt;&amp;lt;&amp;lt;&lt;/a&gt; &lt;div title="当前页"&gt;1&lt;/div&gt; &lt;a href='detach.portal?pageIndex=2&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第2页'&gt;2&lt;/a&gt; &lt;a href='detach.portal?pageIndex=3&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第3页'&gt;3&lt;/a&gt; &lt;a href='detach.portal?pageIndex=4&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第4页'&gt;4&lt;/a&gt; &lt;a href='detach.portal?pageIndex=5&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到第5页'&gt;5&lt;/a&gt; &lt;a href='detach.portal?pageIndex=6&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65'&gt;&amp;gt;&lt;/a&gt; &lt;a href='detach.portal?pageIndex=137&amp;amp;pageSize=&amp;amp;.pmn=view&amp;amp;.ia=false&amp;amp;action=bulletinsMoreView&amp;amp;search=true&amp;amp;groupid=all&amp;amp;.pen=pe65' title='点击跳转到最后页'&gt;&amp;gt;&amp;gt;&lt;/a&gt;&lt;/div&gt; 可以看出，指向其他目录页的相对链接，只是参数略有不同，参数中只有pageIndex发生了变化。至于给url加参数，我记得前几天看到过。 123456789101112131415161718#作为参数的字典para=&#123; 'pageIndex':1,#这里需要修改，先爬第一页 'pageSize':'', '.pmn':'view', '.ia':'false', 'action':'bulletinsMoreView', 'search':'true', 'groupid':'all', '.pen':'pe65'&#125;catalogue_url='http://xxx.xx.xx.cn/detach.portal'#未加参数的新闻目录页url session = login() # 获取已登录的session,这个自定义函数会在下面列出 for i in range(1,page_count+1):#page_count是要获取的页数 para['pageIndex']=i#设置新闻当前页的索引 # 从目录页获取新闻页面链接 html = session.post(catalogue_url,params=para).text 整理代码要用到的库 1234import requestsimport refrom bs4 import BeautifulSoupimport pymysql get_bulletin123456789101112131415161718192021222324252627282930313233343536def get_bulletin(page_count): ''' 目录有多页，从第一页开始获取，往后获取page_count页的目录，并读取目录指向的所有公告 :param page_count: 要爬取的目录页面的数量 :return: 无 ''' para=&#123; 'pageIndex':1, 'pageSize':'', '.pmn':'view', '.ia':'false', 'action':'bulletinsMoreView', 'search':'true', 'groupid':'all', '.pen':'pe65' &#125; catalogue_url='http://xxx.xxx.xxx.cn/detach.portal'#未加参数的公告目录页url session = login() # 获取已登录的session for i in range(1,page_count+1): para['pageIndex']=i#设置公告当前页的索引 # 从目录页获取公告页面链接 html = session.post(catalogue_url,params=para).text soup = BeautifulSoup(html, 'lxml') rss_title = soup.find_all('a', class_='rss-title') #将得到的链接与标题组装成字典 bulletin_dict = &#123;&#125; for url in rss_title: bulletin_title = str(url.span.string).strip() bulletin_url = 'http://xxx.xx.xx.cn/' + url['href'] bulletin_dict.setdefault(bulletin_title, bulletin_url)#添加一条公告记录 #保存公告到数据库 for bulletin_title, bulletin_url in bulletin_dict.items(): #saveInTXT(bulletin_url, session, bulletin_title)#这个是保存到txt文件的函数，用于测试 saveInDB(news_url, session, news_title) login1234567891011121314151617181920212223242526272829303132333435363738def login(): """ 登录并返回已经登录的会话 :return: 已经登录的会话（session） """ #设置 login_url = 'http://xxx.xx.xx.cn/authserver/login?service=http%3A%2F%2F%2F' headers=&#123; 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' &#125; #新建会话 session=requests.session() #获取登录校验码 html=session.post(login_url,headers=headers).text soup=BeautifulSoup(html,'lxml') lt=soup.find('input',&#123;'name':'lt'&#125;)['value'] dllt=soup.find('input',&#123;'name':'dllt'&#125;)['value'] execution = soup.find('input', &#123;'name': 'execution'&#125;)['value'] _eventId = soup.find('input', &#123;'name': '_eventId'&#125;)['value'] rmShown = soup.find('input', &#123;'name': 'rmShown'&#125;)['value'] login_data=&#123; 'username': input("请输入学号："), 'password': input("请输入密码："), 'btn':'', 'lt': lt, 'dllt': dllt, 'execution': execution, '_eventId': _eventId, 'rmShown': rmShown &#125; #登录 response=session.post(login_url,headers=headers,data=login_data) if response.url=='http://xxx.xx.xx.cn/': print('登录成功！') return session saveInTXT12345678910111213141516171819202122232425262728293031323334353637def saveInTXT(url, session, title): ''' 获取单个公告页面的公告并保存到txt :param url: 要获取的页面的url :param session:已经登录的会话 :param title:公告标题 :return:无 ''' #将标题转换为可以作为文件名字的形式 reg = r'[\/:*?"&lt;&gt;|]' title = re.sub(reg, "", title) path='bullet\\' + title+'.txt'#保存在py文件目录下的bulletin文件夹内，以txt格式保存 headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' &#125; ''' #测试代码，从文件读取手动获取的公告html页面，单机测试 with open('new.txt','r',encoding='utf8') as fin: html=fin.read() ''' html=session.post(url,headers=headers).text soup=BeautifulSoup(html,'lxml') #print(soup.prettify()) bulletin_content=soup.find('div', class_='bulletin-content') bulletin_content= '' for p in bulletin_content.find_all('p'): if p.span!=None:#如果p含有一层span text=str(p.get_text()).strip() bulletin_content+= text + '\n' with open(path,'w',encoding='utf8') as fout: fout.write(bulletin_content) print('“&#123;&#125;”成功保存到&#123;&#125;'.format(title,path)) saveInDB1234567891011121314151617181920212223242526272829def saveInDB(url, session, title): ''' 获取单个公告页面的公告并保存到txt :param url: 要获取的页面的url :param session:已经登录的会话 :param title:公告标题 :return:无 ''' headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36' &#125; html=session.post(url,headers=headers).text soup=BeautifulSoup(html,'lxml') bulletin_content=soup.find('div', class_='bulletin-content') bulletin_content= '' for p in bulletin_content.find_all('p'): if p.span!=None:#如果p含有一层span text=str(p.get_text()).strip() bulletin_content+= text + '\n' #保存到数据库 db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='news', charset='utf8') cursor = db.cursor() cursor.execute("insert into chdnews(`title`,`content`) values('&#123;0&#125;','&#123;1&#125;')".format(title, bulletin_content)) db.commit() print('已经成功保存公告到数据库：“&#123;&#125;”'.format(title)) 调用12#调用get_bulletin(10)#爬取10页公告 暂时没有将其通用化，直接将网址写死在函数里面了。 day6进度 通过调整服务门户的url中的参数来获取通知公告的每一个目录页的url，从而爬取所有公告 将学习中写的测试代码重新构造整理，添加函数注释，提交任务 190309 周六day7进度写了本篇博客进行总结]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[换了一个主题]]></title>
    <url>%2Fpost%2Fhexo_change_theme%2F</url>
    <content type="text"><![CDATA[把主题从shana（夏娜）换成了NexT，记录一下这个过程，以及遇到的一些有用的博客链接。流水账，主要保存链接。 缘起刚弄了hexo博客很兴奋，于是去鼓捣各种东西，首先选了一个二次元的主题shana，虽然这个主题我很喜欢，但是想要加目录或者是其他的一些东西，网上根本就搜不到相关的内容，在解决各种问题的过程中，我发现搜索到的几乎都是关于NexT这个主题的解决方法，应为这个主题很多人用。 在又一次发现主题的文件被我“弄坏了”（背景图片的幻灯片播放只显示一次）之后，我想还是换成NexT吧，这样就能专注于写博客，而不是为设置博客而烦恼。 安装NexT主题NexT主题安装和其他主题一样，clone下来再改一下站点配置文件_config.yml就好了。 然后再设置这个主题的配置文件。这个主题的配置文件与shana相比起来不知道详细了多少，各种设置都准备齐全了。没费多少功夫就配置好很好看的站点了。接着就是把站长统计之类的东西设置一下。 cnzz站长统计，统计访问 leanCloud数据统计，统计文章阅读数，参考链接：Hexo-Next搭建个人博客（添加统计访客量以及文章阅读量） 当然我也试着弄了一下gitment评论，仍然不行，那么只能继续采用“直接在菜单中给出issue页面链接”的方式了。参考链接：Hexo-Next 添加 Gitment 评论系统]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++学生信息管理系统（一）]]></title>
    <url>%2Fpost%2Fcpp_student_info_management_system1%2F</url>
    <content type="text"><![CDATA[尝试重新设计与编写大一第一学期的c++课设——学生信息管理系统。本文作简单思路分析与代码分享。B站视频内录制了从头开始写的整个过程：课程设计|c++控制台简易学生信息管理系统 思路要求：能够录入，显示，查找，删除，文件存取学生信息 以当时的知识是以链表来实现的，这次也是使用链表。 首先，创建一个链表结点类用于存放学生的信息，每个对象都是一个学生。 其次，创建一个链表类用于将结点连接起来。 最后，利用链表类已经创建好的各种接口，在main函数中进行装配，实现所需要的各种功能。 链表结点类 类名：CStudent 属性：姓名、性别、成绩、其余本质相同的属性（如班级号，学号）省略。 方法：以不同方式显示该学生所有信息、手动录入学生信息 类声明12345678910111213141516171819202122232425262728#pragma once//链表结点类//学生类//属性：姓名、性别、成绩//方法：录入、显示class CStudent&#123; char name[20]; bool sex;//true为男，false为女 int score;public: //链表需要的指针域 CStudent* next;//=======================================public: //构造函数 CStudent(const char p_name[], bool p_sex, int p_score); CStudent(); //录入与显示 void input(); void show(int method); //get char* getName() &#123; return name; &#125; bool getSex() &#123; return sex; &#125; int getScore() &#123; return score; &#125; //析构函数 ~CStudent();&#125;; 类实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//@fileName &lt;CStudent.cpp&gt;#include "CStudent.h"#include &lt;cstring&gt;#include &lt;iostream&gt;using namespace std;CStudent::CStudent(const char p_name[], bool p_sex, int p_score)&#123;//有参构造则自动录入信息 strcpy(name, p_name); sex = p_sex; score = p_score;&#125;CStudent::CStudent()&#123; input();//如果无参构造，则手动录入信息&#125;void CStudent::input()&#123; cout &lt;&lt; "请输入学生姓名：" &lt;&lt; endl; cin &gt;&gt; name; //如果遇到cin连续输入出错的问题，可以在每次输入后加个cin.get() cout &lt;&lt; "请输入学生性别（1为男，0为女）：" &lt;&lt; endl; int isex; cin &gt;&gt; isex; sex = isex ? true : false; cout &lt;&lt; "请输入学生成绩：" &lt;&lt; endl; cin &gt;&gt; score;&#125;void CStudent::show(int method)&#123; switch (method) &#123; case 0://横向显示，一行一条记录 cout &lt;&lt; name&lt;&lt;"\t" &lt;&lt; (sex ? "男" : "女")&lt;&lt;"\t" &lt;&lt; score&lt;&lt;"\t" &lt;&lt; endl; break; case 1://纵向显示，每行一个属性 cout &lt;&lt; "姓名：" &lt;&lt; name &lt;&lt; endl &lt;&lt; "性别：" &lt;&lt; (sex ? "男" : "女") &lt;&lt; endl &lt;&lt; "成绩：" &lt;&lt; score &lt;&lt; endl &lt;&lt; endl; break; default: break; &#125; &#125;CStudent::~CStudent()&#123;&#125; 链表类类声明12345678910111213141516171819202122232425262728//@fileName &lt;CStudentList.h&gt;#pragma once#include "CStudent.h"//链表类（带头结点的单向链表）//属性：指向头结点的头指针//方法：构造函数（手动输入）、构造函数（传入结点对象数组）、析构函数//方法：显示表、查找、删除、把数据存入文件、从文件中读取数据class CStudentList&#123; CStudent* head;//头指针public: //构造函数 CStudentList(int n);//手动录入n个学生的信息 CStudentList(CStudent s[],int n);//通过对象数组自动录入n个学生的信息 CStudentList(const char fileName[]);//读取文件中的数据信息来初始化 CStudentList(); //功能 void showList();//显示整个链表的信息 int search(const char name[]);//按名字查找并返回找到的个数 void deleteNode(CStudent*p);//删除指针p指向的结点 int deleteByName(const char name[]);//删除表中第一个匹配的记录，同时返回是否删除成功 //文件读写 void save(const char fileName[]); void open(const char fileName[]); //析构函数 ~CStudentList();&#125;; 类实现构造函数我设计了四个构造函数。 1.如果没有参数，那么就只建立一个空链表，即只有一个头结点的链表。 1234567//@funcName &lt;CStudentList::CStudentList&gt;//@brief &lt;创建空链表&gt;CStudentList::CStudentList()&#123; head = new CStudent("HEAD", 1, 100);//头结点本身的数据并不重要，所以随意填写。 head-&gt;next = NULL;&#125; 2.手动录入信息的构造函数 1234567891011121314//@funcName &lt;CStudentList::CStudentList(int n)&gt;//@brief &lt;创建n个结点的链表，并手动录入信息&gt;//@parameter &lt;n:初始链表结点数目（不计头结点）&gt;CStudentList::CStudentList(int n)&#123; head = new CStudent("HEAD",1,100); head-&gt;next = NULL; for (int i = 0; i &lt; n; i++) &#123; CStudent *newNode = new CStudent(); newNode-&gt;next = head-&gt;next; head-&gt;next = newNode; &#125;&#125; 3.通过数组自动录入信息的构造函数 和上一个差不多。 1234567891011121314//@funcName &lt;CStudentList::CStudentList(CStudent s[],int n)&gt;//@brief &lt;创建n个结点的链表，并自动从数组中获取信息&gt;//@parameter &lt;s:结点类对象数组&gt;&lt;n:数组s的长度&gt;CStudentList::CStudentList(CStudent s[], int n)&#123; head = new CStudent("HEAD", 1, 100); head-&gt;next = NULL; for (int i = 0; i &lt; n; i++) &#123; CStudent *newNode = new CStudent(s[i].getName(),s[i].getSex(),s[i].getScore()); newNode-&gt;next = head-&gt;next; head-&gt;next = newNode; &#125;&#125; 4.通过文件自动录入信息的构造函数 使用到了另一个成员函数open() 123456789//@funcName &lt;CStudentList::CStudentList(const char fileName[])&gt;//@brief &lt;自动从文件中读取信息&gt;//@parameter &lt;fileName:数据来源文件的名字&gt;CStudentList::CStudentList(const char fileName[])&#123; head = new CStudent("HEAD", 1, 100); head-&gt;next = NULL; open(fileName);&#125; 显示链表123456789101112//@funcName &lt;CStudentList::showList()&gt;//@brief &lt;显示整个链表&gt;void CStudentList::showList()&#123; CStudent*p = head-&gt;next; cout &lt;&lt; "姓名\t性别\t成绩" &lt;&lt; endl; while (p != NULL) &#123; p-&gt;show(0);//以一行一记录的形式显示 p = p-&gt;next;//工作指针向后移动 &#125;&#125; 查询结点1234567891011121314151617181920//@funcName &lt;CStudentList::search&gt;//@brief &lt;按照名字查找数据并显示&gt;//@parameter &lt;name:要查找的学生的名字&gt;//@return &lt;找到的记录数目&gt;int CStudentList::search(const char name[])&#123; int num = 0; CStudent*p = head-&gt;next; cout &lt;&lt; "---------查找结果---------" &lt;&lt; endl; while (p != NULL)//遍历链表 &#123; if (strcmp(p-&gt;getName(),name)==0)//找到了需要的信息 &#123; num++; p-&gt;show(0); &#125; p = p-&gt;next; &#125; return num;&#125; 查找删除由于删除结点与查找要删除的结点相对独立，因此将删除结点独立出来一个函数，以便查找删除不同属性的数据。 1234567891011121314//@funcName &lt;CStudentList::deleteNode&gt;//@brief &lt;删除指针指向的链表节点&gt;//@parameter &lt;p:要删除的结点的指针&gt;void CStudentList::deleteNode(CStudent*p)&#123; CStudent*p1 = head, *p2 = head-&gt;next; while (p2 != p) &#123; p1 = p1-&gt;next; p2 = p2-&gt;next; &#125; p1-&gt;next = p-&gt;next; delete p;&#125; 以查找姓名的删除函数为例子： 123456789101112131415161718//@funcName &lt;CStudentList::deleteByName&gt;//@brief &lt;按名字查找并删除第一个符合条件的结点&gt;//@parameter &lt;name:要删除的结点的名字&gt;//@return &lt;是否删除成功(成功返回0，失败返回-1)&gt;int CStudentList::deleteByName(const char name[])&#123; CStudent*p = head-&gt;next; while (p != NULL) &#123; if (strcmp(p-&gt;getName(), name) == 0)//找到了需要的信息 &#123; deleteNode(p); return 0; &#125; p = p-&gt;next; &#125; return -1;&#125; 读取数据123456789101112131415161718192021222324252627282930//@funcName &lt;CStudentList::open&gt;//@brief &lt;从文件中读取数据并以覆盖形式写入链表&gt;//@parameter &lt;fileName:数据文件名&gt;void CStudentList::open(const char fileName[])&#123; //清空链表 CStudent* p = head-&gt;next; while (p != NULL) &#123; delete head; head = p; p = p-&gt;next; &#125; //未删除head //从文件读取数据 ifstream fin(fileName); while (!fin.eof())//end of file &#123; char name[20]; bool sex=0; int score=0; fin &gt;&gt; name &gt;&gt; sex &gt;&gt; score; //利用头插法把数据插入到链表中 CStudent *newNode = new CStudent(name,sex,score); newNode-&gt;next = head-&gt;next; head-&gt;next = newNode; &#125;&#125; 保存数据123456789101112131415161718//@funcName &lt;CStudentList::save&gt;//@brief &lt;将链表存入文件&gt;//@parameter &lt;fileName:保存到的数据文件名&gt;void CStudentList::save(const char fileName[])&#123; ofstream fout(fileName);//打开文件，创建文件流对象 //遍历链表 CStudent *p = head-&gt;next; while (p != NULL) &#123; //将数据存入 fout &lt;&lt; p-&gt;getName() &lt;&lt; " " &lt;&lt; p-&gt;getSex() &lt;&lt; " " &lt;&lt; p-&gt;getScore() &lt;&lt;endl; p = p-&gt;next; &#125;&#125; 析构函数1234567891011CStudentList::~CStudentList()&#123; CStudent* p = head-&gt;next; while (p != NULL) &#123; delete head; head = p; p = p-&gt;next; &#125; delete head;&#125; 菜单菜单比较简单，整个程序主要流程： 显示菜单选项，等待输入选项编号 分支语句，按照不同选项调用链表提供的函数 如果没有选择退出选项就循环 菜单函数示例123456789void menu()&#123; cout &lt;&lt; "========学生信息管理系统========" &lt;&lt; endl; cout &lt;&lt; "1.显示学生信息表" &lt;&lt; endl; cout &lt;&lt; "2.查找学生信息" &lt;&lt; endl; cout &lt;&lt; "3.从文件读取" &lt;&lt; endl; cout &lt;&lt; "4.将数据存入文件" &lt;&lt; endl; cout &lt;&lt; "0.退出" &lt;&lt; endl;&#125; 选项分支示例12345678910111213141516171819202122int main()&#123; int opt = -1; CStudentList list("data.txt"); while (opt != 0) &#123; menu(); cin &gt;&gt; opt; switch (opt) &#123; case 1: system("cls"); list.showList(); break; default: break; &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[notepad++添加文件关联]]></title>
    <url>%2Fpost%2Fnotepadpp_file_association%2F</url>
    <content type="text"><![CDATA[将一些常用notepad++打开的文件设置为默认notepad++打开的方法 在研究hexo博客的各种功能的时候，总是需要打开配置文件.yml，一开始我是用右键菜单里面的【用notepad++打开】的方式来打开，后来又遇到了各种.ejs,.styl之类的文件也需要用notepad++来编辑，就将这些文件类型的默认打开方式设置为notepad++。 方法一 菜单栏【设置】-&gt;【首选项】 如图选择【文件关联】，找到需要添加的文件类型，如果没有就选择【customize】（自定义）自己输入，然后添加。 方法二 【ctrl+r】打开【运行】输入control（也就是打开控制面板） 小图标查看方式，找到【默认程序】，选择【将文件类型或协议与程序关联】，找到需要的后缀名，选择它的默认程序即可。]]></content>
      <categories>
        <category>工具使用</category>
      </categories>
      <tags>
        <tag>IDE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫学习笔记1简易爬虫]]></title>
    <url>%2Fpost%2Fpython_spider_note1simple_spider%2F</url>
    <content type="text"><![CDATA[学了python语法之后在b站搜索练手的小项目，发现了这个视频：Python实用练手小项目（超简单） 视频里面讲解了一个爬取图片网站图片的小爬虫。后面用到了我还没学的数据库，不过前面的部分是已经学了的，于是我就打算写一个不用数据库的，爬取某个盗版小说内容的爬虫。 声明：本人不会将得到的小说内容作任何商业用途，也请阅读此文章的各位读者遵纪守法，此文章只用作学习交流，原创内容，转载请注明出处。 项目描述爬虫，在我理解中就是模拟人的浏览行为来获取网站上的信息的脚本，爬虫能得到的信息，一般情况下人也有权限可以得到。 盗版小说网站，不需要登录就可以看到小说内容，内容是写死在html文件里面的，通过右键菜单的查看源代码就能够查看到小说内容，很适合拿来练手。 再次声明：本人不会将得到的小说内容作任何商业用途，也请阅读此文章的各位读者遵纪守法，此文章只用作学习交流，原创内容，转载请注明出处。 思路爬虫的思路是向服务器发出请求，并收到服务器回复的数据，接着从获取的数据中取得想要的信息，保存在数据库中。 由于是小说，就直接保存在文本文件当中。 所以分为以下几步： 发出请求 接收数据 提取信息 保存数据 编程原理发出请求和接收数据发出请求需要一个库，名字叫做requests，它是基于python自带的urllib库写的第三方库，差不多就是升级版的意思吧。 要注意是requests，不是request，结尾有个s，确实存在一个不带s的库，注意区分。 可以使用下面的命令进行安装： 1pip install requests pip 是 Python 包管理工具，总之有了这个玩意，你不用管它从哪里下载，在哪里安装，总之就告诉它要安装啥，它就帮你安排得明明白白的。以后会遇到很多这样的东西，比如npm啥的。 命令在cmd里面输就行了，如果电脑上没有这东西就百度一下怎么下载，一般来说安装了python应该就有了。 如果使用的是pyCharm这种IDE，那就可以直接在代码import这个库，等库的名字变红再在旁边找安装按钮，很方便的。 这个库里面有个get函数，是采用get的方式（除此之外还有post方式，学html表单的时候应该有学到）来向服务器发出访问请求，并将获得的数据作为返回值。 1234import requests#省略代码r = requests.get(url)#url是你要访问的网址print(r)#如果输出是&lt;Response [200]&gt;，那么就是访问成功了 此时返回变量是请求对象，要从中获取数据，就需要使用它的两个属性text和content r.text是数据的html形式，r.content是字节流的形式。二者的区别 前者返回文本格式（即二进制格式经过编码后），后者返回二进制格式。后者一般用于图片的保存。 我们需要获取的是文本内容，因此需要前者。 1html=r.text 提取信息我们打开笔趣阁（一个盗版小说网站）的一个小说页面，随便选一章点进去，查看源代码，发现小说的内容是放在一个&lt;div&gt;里面的： 1&lt;div class="content" id="booktext"&gt;小说内容&lt;center&gt;翻页信息&lt;/center&gt;&lt;/div&gt; 其他章节也是如此，所以就可以利用这个规律将其提取出来，用的就是正则表达式。 正则表达式使用正则表达式需要使用一个内置的库re，根据上面的规律可以写出下面的正则表达式： 123456789import rereg = r'&lt;div class="content" id="booktext"&gt;(.*?)&lt;center&gt;'#正则表达式reg = re.compile(reg)#将字符串转换为正则表达式对象，加快匹配速度content= re.findall(reg, html)#返回一个列表，列表项为匹配到的内容if content==[]:#未匹配到小说内容 print("获取失败！")else: content=str(content[0])#将列表转换为字符串 re.compile()函数 编码转换但是我写到这里的时候遇到了一个问题，就是获取到的内容是乱码。一看到乱码就应该想到是编码出了问题。 右键菜单查看网页编码，是GBK编码，需要转换编码。现在的情况是，网页利用GBK的编码来“加密”了小说文本，而我们需要用同样的方式来“解码”。需要用到decode函数 1html=r.content.decode("GBK", "ignore")#转换编码 将获得的二进制数据按照网页原本的编码GBK来解码，就能获取到正确的内容了。 去除分隔字符此时提取到的内容还有这很多HTML实体，比如&amp;nbsp;和&lt;br /&gt;，注意到它们的分布也有规律： 123&lt;div class="content" id="booktext"&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;小说内容&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;小说内容&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;……省略……&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;大雪落下，悄然覆盖着这一切。&lt;br /&gt;&lt;center&gt; 除了开头和结尾之外，都是以&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;进行分隔的。 可以利用split()函数将其分割之后重新组合， 也可以使用字符串的替换函数replace() 1content=content.replace("&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;","\n\n ") 保存数据保存在文本文件中就ok了： 123with open(fileName,'w') as fout:#fileName为保存路径加文件名 fout.write('\n\n=====================\n\n' + fileName + '\n\n=====================\n\n') fout.write(content) 获取单章节内容代码12345678910111213141516171819202122232425262728293031323334353637import requestsimport reimport osdef getNovelByURL(url,fileName): ''' :param url: 网页的url :param fileName: 保存数据的文件的名字 :return: -1为失败，0为成功 ''' #筛选文件名内非法字符 #调试的时候前面几百章都行突然一章不行，发现是因为章节名字里面有非法字符 reg=r'[\/:*?"&lt;&gt;|]' fileName=re.sub(reg,"",fileName)#利用正则表达式去除非法字符 # 获取网页 r = requests.get(url) html = r.content html = html.decode("GBK", "ignore") # 获取网页中小说内容 reg = '&lt;div class="content" id="booktext"&gt;\n&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;(.*?)&lt;br /&gt;\n&lt;center&gt;' reg = re.compile(reg)#预编译 content = re.findall(reg, html) #保存到文件 if content==[]: print("获取失败！") return -1 else: content=str(content[0])#转换为字符串 content=content.replace("&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;","\n\n ") with open(fileName,'w') as fout: fout.write('\n\n=====================\n\n' + fileName + '\n\n=====================\n\n') fout.write(content) print("成功爬取（&#123;&#125;），存储在&#123;&#125;".format(url,os.path.dirname(__file__)+'/'+fileName)) return 0 获取全部章节内容的思路盗版小说网站章节的url有个规律，就是url的最后一串数字是连续的，照这个规律，知道第一章的url，就可以获得后续章节的url。于是我着手写这么个函数： 123456789def getNovelByIndexInc(url, number=1): ''' 此函数用于通过已知的起始url来获取仅有尾部索引不同且连续的一系列网页内的小说， 不连续时会跳过获取失败的网址，不过有可能连续几千个网址都是无效网址，所以慎用此函数 或改用getNovelByContentPage函数 :param url:起始章节的url :param number: 要获取的章节数 :return:无 ''' 从我写的注释里面也可以看出，我失败了。 一开始的一百多章还是没什么问题的，只有偶尔几个网址是无效网址，但是后面爬取的时候等了十分钟还没爬取到下一章，一直输出“无效网址”，我查看了那断片的两个连续章节之后才发现，最后的一串数字差了几万。不会是因为作者断更吧！ 这种方式不可靠，还是换一种方式。 那么要如何可以改进呢？ 我写了另一个函数： 1234567def getNovelByContentPage(url,path='novel'): ''' 通过获取目录页面链接与标题，进一步调用获取已知链接页面的函数来保存页面内容 :param url: 书籍目录页面 :param path:保存路径，默认为同目录下的novel文件夹 :return:-1为失败，0为成功 ''' 网站的书籍页面会有一个目录，而目录下隐藏的就是我需要的全部章节的链接呀！ 这个函数用到的内容上面也都讲到了，就直接放代码吧。 获取全部章节内容的代码1234567891011121314151617181920212223242526272829303132333435import requestsimport reimport osdef getNovelByContentPage(url,path='novel'): ''' 通过获取目录页面链接与标题，进一步调用获取已知链接页面的函数来保存页面内容 :param url: 书籍目录页面 :param path:保存路径，默认为同目录下的novel文件夹 :return:-1为失败，0为成功 ''' # 获取网页 r = requests.get(url) html = r.content#获取网页二进制内容 html = html.decode("GBK", "ignore")#转换编码 # 获取网页中小说内容 reg = '&lt;dd&gt;&lt;a href="(.*?)" title="(.*?)"&gt;.*?&lt;/a&gt;&lt;/dd&gt;'#获取链接和标题 reg = re.compile(reg, re.S) info= re.findall(reg, html) #由于是分组匹配，得到的列表中每个元素的[0]是链接，[1]是标题 #保存到文件 if info==[]: print("获取章节目录失败") return -1 else: if not os.path.exists(path):#检查目录是否已经存在 os.makedirs(path) for i in info: realpath=path+"\\"+i[1]+".txt" if os.path.exists(realpath):#避免重复爬取 continue else: getNovelByURL(i[0],realpath)#调用获取单页面内容的函数 return 0]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为博客增加访问统计]]></title>
    <url>%2Fpost%2Fhexo_visit_count%2F</url>
    <content type="text"><![CDATA[用CNZZ统计网站访问量 我用的主题是shana，网站统计的配置部分是这样的： 12345# 网站统计# 站长统计 填写id# eg: # CNZZ: 123456789CNZZ: 百度了一下发现CNZZ和百度统计都可以统计网站访问量。当然想统计呀，这样就更有动力来写了。 尝试了百度统计一个多小时之后还没弄好，我就开始试CNZZ，毕竟主题的作者直接写在配置里面了，还是按照规矩来吧。 CNZZ不是中国站长（cnzz.cn）那个，而是友盟（cnzz.com），我一开始进的是中国站长……找了老半天统计功能才发现进错网站了。 步骤 注册一个cnzz账号 填写网站信息 复制统计代码 粘贴统计id到配置文件 粘贴统计代码到需要统计的页面开头 粘贴代码到哪里又是个问题，根据前面尝试弄百度统计的经验，在主题文件夹下的\layout\_partial内是用于生成页面的代码，摸索一阵后发现应该粘贴到head.ejs里面以达到生成在页面前面的效果。 一开始没显示“站长统计”的字样我以为是无效，甚至还去issue里面问shana的作者怎么弄。 后来发现，原来是shana主题在切换背景图片的时候会掩盖字样……好吧是我太心急了。 今日收获hexo的页面生成方式theme\&lt;themeName&gt;\layout\_partial下的文件都是.ejs文件，应该是“扩展的js”文件，用于生成相应的页面。 例如head.ejs中专门存储生成html文件的&lt;head&gt;部分 more文章一开始是全部展开的，浏览起来比较难受，查了之后发现其实只需要在文章中加上一个标记就可以折叠。 .md文件里面是下面这种结构： 12345显示出来的文章提要&lt;!--more--&gt;正文]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo部署博客的过程记录]]></title>
    <url>%2Fpost%2Fhexo_deploy_log%2F</url>
    <content type="text"><![CDATA[建立Hexo博客的相关知识整理成的笔记，不太全面。 缘起这部分算是年终总结一样的东西吧。 在2018年8月底的时候，我还是对域名、服务器等名词没有了解的一个web小白，那时一个朋友在群里发了一个非常好看的个人博客，我一下子就被吸引了，羡慕但是又没有能力自己弄，感觉太难了。当时我只学过一点HTML和CSS，javaScript还未怎么学，而且已经很久没有练习过，已经忘得差不多。 那个朋友东给我发了一个《基于CentOS搭建WordPress个人博客》的页面，东说他想弄，已经租了个腾讯云服务器，问我有没有兴趣。我当时还是蛮犹豫的，对于我来说难度还是很大的，那个网页上并不是个教程，说的内容我大部分看不懂。不过我还是想整一个的，于是尝试去学。 我属于那种“如果不能基本上理解一个概念，那么就会完全拒绝相关的知识输入，即便已经记住了也不会长久”的学习类型，而且以前又比较自闭，不想问别人，只在网上查找已有的问题答案，所以学习起来特别困难。 买了域名，备了案，租了学生价服务器，照着教程《新手如何用腾讯云服务器搭建一个wordPress博客-简书》鼓捣了好久终于弄出来一个wordPress博客。但我当时仅仅是“知其然而不知其所以然”，并不认为自己学到了什么，弄好了主页就一直搁置在那里，感觉心疼租服务器的钱但是又没办法。开学了又有很多事情要忙，大学并不像高中的时候想象的那么悠闲。 直到一个学期结束我才考虑起开始重新弄个博客。经过了一个学期，我学会了更多的东西，把上个暑假的建博客的流程给理解了应该没什么问题。 我开始整理以前编程留下的笔记。以前使用的是vnote，但是我觉得界面不太好看，而且功能大多用不上，遇到问题百度也搜不到，碰巧它这时又不知道出了什么毛病，于是换成了Typora，把笔记重新筛选了一遍。分类尽可能少，渐渐地开始“一元化”笔记。 随后又想起了以前只学了一点的git。没有一次性学完它，导致我没有去用它，顶多只是使用网页版github上传一下代码点亮小绿块让自己爽一下，也搁置了很久。说起来我真是喜欢半途而废。 不如把这整理好的笔记传到github上面备份吧，感觉比网盘备份b格高。刚好前几天学Python的时候找到了廖雪峰的git教程，这让我有些后悔没有听另一个朋友朱的推荐。 整github的时候又发现了github page的功能，想起来github也可以搭博客，所以今天（2019-2-6）就研究了一整天搭好了这个博客。挑了个和我以前羡慕的个人博客相同的主题，美滋滋，成就感爆棚。 好了，接下来我来分享一下我是如何搭建这样一个博客的。不保证零基础能看懂。 Hexo——一个博客框架和WordPress差不多，都是用来搭建博客的一个框架。但是问题来了—— 框架，又是啥？ 自学计算机类的知识最大的问题就在于百度到的东西需要各种各样的前置知识，很难一下子理解那是什么意思，越听越迷糊。不管在看这篇博客的你知不知道框架的意思，反正上个暑假的我是不明白的。而且这是简称，光百度一个“框架”好像又搜不到明确的定义。 这个障碍一直妨碍着我对bootstrap、vue、MFC、QT等框架的准确理解，后来我才知道，软件框架到底是个啥 简而言之，在我的理解里面，框架，就是可以复用的代码，就是“不要重复造轮子”中的“轮子”，就是别人已经写好的封装了各种复杂API的库。框架可以帮你完成一些基础语法本身也可以完成的事情，让你不必在建房子的时候从烧砖开始，而是可以解放思维直接开始画楼房设计图。 Hexo，就是一个可以帮助你生成静态网页的一个工具，所有的核心功能比如打标签归档加时间，以及界面美化工作都帮你做好了，你可以专注于博客内容的创作，而不必学习如何“烧砖”（写前端代码）。网上搜索“hexo”，可以找到它的官网。hexo的官网文档做得非常好，不仅提供准确的中文版文档，还附有视频，让我学得非常之愉快。 不过作为一个“楼房设计工程师”，你还是需要一些其他的帮手来帮助你“建房子”。 在Hexo官网的文档里面有详细的教程教你如何安装必须的东西，我在这里只讲一些理解性的东西，详细的指令不多讲。 安装Hexo的时候你需要俩工具： node.js git node.jsnode.js-百度百科 Node.js 是一个让 JavaScript 运行在服务端的开发平台，实质是对Chrome V8引擎进行了封装。 引用自node.js和JavaScript的关系-博客园 JavaScript是一门语言 node.js不是一门语言，也不是一种特殊的JavaScript方言 - 它仅仅就是用于运行普通JavaScript代码的东西 所有浏览器都有运行网页上JavaScript的JavaScript引擎。Firefox有叫做Spidermonkey的引擎，Safari有JavaScriptCore，Chrome有V8 node.js就是带有能操作I/O和网络库的V8引擎，因此你能够在浏览器之外使用JavaScript创建shell脚本和后台服务或者运行在硬件上 个人理解：node.js是javaScript的解释器 为啥要安装它呢？应该是为了使用node.js的npm（Node Package Manager，是一个node.jS包管理和分发工具）,可以理解为一个安装程序，可以给你安装官方已经整合好的包。当然其他作用我也不知道。 如果已经有git和node.js，直接使用下面指令进行安装： 1`$ npm install -g hexo-cli` 虽然前面有个linux系统的shell的命令提示符，但是安装好node.js之后用windows系统的cmd里面也是可以用的。至于打开cmd，win+R打开运行窗口输入“cmd”，回车就出现了。记得输入时不要输入前面的$符号，那是命令提示符。 git——版本控制系统git的安装和使用就不多说了。用于将Hexo生成好的页面给推送到github这个远程库里。至少要知道git的一些基本概念。 Hexo的使用这Hexo安装好之后你可以在cmd使用它的指令。 初始化初始化Hexo的指令（命令提示符不写了，下同）： 1hexo init 指定目录（省略则初始化当前目录） 初始化hexo之后会在你指定的目录生成一大堆文件，这些文件和文件夹是从它的官方github库里面clone下来的，这也是一开始要下好git的原因。 比较重要的几个文件是： _config.yml 配置文件，使用YAML来写的数据文件 scaffolds 模板文件夹，存放新文章的模板 source 文章，图片，草稿等资源都放在这里 themes 主题文件夹，Hexo根据主题生成静态页面 新建文章1hexo new [layout] &lt;title&gt; 生成静态页面1hexo generate 或者简写成 1hexo g 前文说过，Hexo是用来帮助你生成静态网页的一个工具，就是用这个指令。这个指令将目前编写好的文章以及主题等东西给包装好，生成用于上传到你的网站上（这里我们用github page）的网页。至于此命令的详细说明，请看Hexo文档。 部署网站说实话我在看文档的时候没看懂“部署网站”是啥意思，后来知道了，这就是将hexo generate生成的静态页面推送到你的github库里面去的意思。 指令是： 1hexo deploy 也可以简写成： 1hexo d deploy可以与generate共同简写成： 1hexo d -g 或者 1hexo g -d 一个意思，都是先生成静态页面，再部署网站。 本地测试1`$ hexo server` 在自己电脑上运行服务器来查看博客的效果，默认情况下，访问网址为： http://localhost:4000/ 其余指令我目前还没用到，详情请见Hexo文档 github page在github仓库的setting里面，有一栏叫做github page，在其中的source选项内选择作为数据源的分支，一般将博客部署在master分支，所以选择master作为数据源。 你可以选择两种方式来给你用来存放博客数据的仓库起名字，第一种就是你百度经常看到的：你的用户名.github.io的形式，这种形式会让你在选择好数据源之后提示： 1Your site is published at https://你的用户名.github.io/ 然后你可以使用给出的链接来打开你的博客，点击链接，会默认打开你数据源分支内的index.html文件作为主页，如果没有这个文件就会404：找不到页面，当然，Hexo会帮你生成好index.html，只要你把生成好的页面给push上github就可以。如果你知道包含哪些文件的话，自己手动上传应该也ok。 有了这种方式，其实你甚至可以不需要Hexo，自己写html页面也能做一个博客，不过这样就像前文说的从“烧砖”开始建楼了。 第二种起名方式就是不按照第一种来，随便起，比如用户名是HaneChiri，创建的仓库名叫blog,那么选择完数据源分支之后呢，得到的提示可能是： 1Your site is published at https://hanechiri.github.io/blog/ 这样需要写的网址就会长一些，要加上仓库名。试了一下这样是可以的。我的两个仓库就分别使用了以上两种方式。 markdown的图片这里的文章使用的是markdown语法，一个比较容易学习的标记语言，可以让你手不离键盘地完成排版，我现在就是在用markdown来写.md文件，然后放进source文件夹的_post子文件夹里面，之后再上传。 markdown可以方便的插入图片和超链接。但是图片一般来说是利用相对路径放在.md文件的附近的，生成静态页面的时候图片的路径又会被打乱，导致图片显示失败。 Hexo文档里面提供了几种方式来插入图片，比如插件。但是那种方式无法实时预览，而且难弄。 所以干脆使用外部图片链接，在github上面再建立一个仓库用来存放图片，提供链接给博客使用。 要这样使用的前提是去开启github page这个设置。 比如用户名是HaneChiri，创建的仓库名叫blog_images，那么在这个仓库根目录下的图片avatar.jpg的链接就是 1https://hanechiri.github.io/blog_images/avatar.jpg 而不是 1https://github.com/HaneChiri/blog_images/avatar.jpg 后者是浏览编辑这个图片的链接，而不是图片本身。 上传之后无法访问这个链接也不要急，等几分钟就可以了。 参考教程 【持续更新】最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析-遇见西门 步骤总结由于网上教程很多，我在这里只是简单把我部署博客的步骤总结一下： 开一个github空仓库（注册和新建仓库应该不用多说） 在一个本地空文件夹内初始化hexo 此文件夹内，与远程库建立关联（其实这一步可以不必，不过以后可能用得到，先弄着吧） 给_config.yml文件内deploy属性设置好type（: git，记得冒号后面有个空格）、url（github仓库的链接）和branch（推送到的分支，一般用master） 修改其他配置比如title、author、new_post_name、language、post_asset_folder 安装一个git部署的东西npm install --save hexo-deployer-git 生成并在本地测试页面效果 生成并部署网站hexo d -g 新建，编辑文章然后重复上一步]]></content>
      <categories>
        <category>过程记录</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello-world]]></title>
    <url>%2Fpost%2Fhello_world%2F</url>
    <content type="text"><![CDATA[Hello World这是利用Hexo和github建立的个人博客。使用了hexo的二次元主题夏娜 我是憧憬少，初めまして，よろしくお願いします！ 1234567//代码高亮测试#include &lt;iostream&gt;using namespace std;int main()&#123; cout&lt;&lt;"hello world"&lt;&lt;endl;&#125; $$\Sigma$$]]></content>
  </entry>
</search>
